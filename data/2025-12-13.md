<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 该论文首次系统研究强化学习在文本到3D自回归生成中的应用，提出了奖励设计、算法优化、新基准和分层强化学习范式，并开发了首个RL增强的3D生成模型AR3D-R1。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在2D图像生成中已证明有效，但应用于3D生成仍面临挑战，因为3D对象具有更高的空间复杂性，需要全局一致的几何结构和细粒度局部纹理，对奖励设计和RL算法高度敏感。

Method: 1) 评估奖励维度和模型选择；2) 研究GRPO变体，探索令牌级优化；3) 引入MME-3DR基准；4) 提出Hi-GRPO分层强化学习范式，通过专用奖励集成优化全局到局部的3D生成。

Result: 开发了AR3D-R1，这是首个RL增强的文本到3D模型，能够从粗形状到纹理细化进行专家级生成。研究表明与人类偏好对齐的奖励和多模态模型能提供稳健的3D属性信号。

Conclusion: 该研究为RL驱动的3D生成推理提供了重要见解，证明了强化学习在复杂3D生成任务中的有效性，并为该领域建立了系统研究框架。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Video-Bench：首个全面评估多模态大语言模型视频空间智能的基准，包含1,106个问题，覆盖感知、规划、预测和跨视频推理四个层次，揭示当前模型与人类存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估MLLMs在连续视觉输入中空间理解能力的基准，这对于MLLMs成为物理环境中的通用助手至关重要。需要建立系统性的评估框架来衡量模型在视频空间智能方面的进展。

Method: 构建了MMSI-Video-Bench基准，采用四层框架：感知、规划、预测和跨视频推理。包含1,106个问题，基于25个数据集和内部视频的1,278个片段。每个问题由3DV专家精心设计和评审，提供解释性原理。还支持三个领域导向的子基准：室内场景感知、机器人和接地基准。

Result: 评估了25个开源和专有MLLMs，发现显著的人机差距：许多模型表现接近随机水平，最佳推理模型落后人类近60%。空间微调模型在基准上泛化能力仍然有限。细粒度错误分析揭示了在几何推理、运动接地、长时程预测和跨视频对应方面的系统性失败。

Conclusion: MMSI-Video-Bench为推进视频空间智能研究提供了坚实的测试平台，揭示了当前MLLMs在空间理解方面的严重不足，需要新的方法来解决几何推理、运动理解和跨视频推理等核心挑战。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2是一个基于儿童发展轨迹的视觉语言模型框架，通过婴儿中心的多模态预训练数据和认知评估工具箱，实现了高效的小模型预训练，在部分任务上超越了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 早期儿童发展轨迹为视觉基础模型的高效预训练提供了自然目标，但现有模型缺乏发展心理学基础。需要构建符合婴儿认知发展规律的多模态学习框架。

Method: 1) 构建纵向、多方面的婴儿中心视听语料库预训练集；2) 开发DevCV工具箱，将NIH Baby Toolbox的视觉相关测量转化为10个多模态任务基准；3) 使用紧凑模型从头开始预训练。

Result: 紧凑模型在DevCV工具箱上表现出色，在空间推理、记忆和词汇理解等任务上取得竞争性性能，部分任务甚至超越了GPT-4o。

Conclusion: BabyVLM-V2提供了一个原则性、统一的框架，有望加速基于发展心理学原理的视觉基础模型预训练研究。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [4] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一个可扩展的多视角Transformer，用于度量尺度、密集前馈4D重建，能够直接生成多帧的像素级运动和几何预测，支持多种传感器模态，在精度和计算效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注2视角密集场景流或稀疏3D点跟踪，缺乏对多帧4D重建的直接处理能力；同时，现有4D重建方法通常仅支持单目RGB视频，无法充分利用其他可用传感器数据。

Method: 采用模块化的4D场景表示：将每视角4D预测编码为以局部相机坐标系表示的自我中心因子（深度图和相机内参）和以全局世界坐标系表示的他者中心因子（相机外参和场景流）。使用可扩展的多视角Transformer架构，支持RGB、RGB-D、IMU、雷达等多种模态输入。

Result: 在多种设置下均取得优越性能：精度提升2-3倍（误差降低2-3倍），计算效率提升15倍，为下游应用开辟了新途径。

Conclusion: Any4D提供了一个灵活、高效的4D重建框架，通过模块化场景表示和多模态支持，在精度和速度上均显著超越现有方法，具有广泛的应用前景。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [5] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView是一个统一的4D一致性框架，将空间、时间和视角条件分离表示，能够处理多种4D任务，包括新颖视角合成、文本/图像到视频生成等，在多个基准测试中表现优于任务专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于特定的4D一致性任务子集（如新颖视角合成、文本到视频等），导致训练数据分散且方法碎片化。需要一个统一框架来泛化处理广泛的4D任务。

Method: 将空间、时间和视角条件分别表示，允许这些输入的灵活组合。通过统一框架处理静态、动态和多视角输入，支持时间前后外推和完整相机控制。

Result: 在多个基准测试中与任务专用模型竞争：在LLFF多视角NVS数据集上图像质量提升33%，在Neural 3D Video动态NVS基准上提升60%，在RE-10K静态相机控制上提升20%，在文本条件视频生成中相机轨迹误差减少4倍。

Conclusion: OmniView展示了通用4D视频模型的可行性，通过单一模型实现强大的泛化能力，为统一的4D理解与生成提供了有效框架。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [6] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens是一种模态无关的潜在令牌，允许模型在文本和图像模态之间自由思考，以解决多模态推理问题，在空间推理基准上相比基线提升3%平均性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在空间、时间、功能等真实世界推理方面存在局限性，它们依赖专业工具、昂贵的图像生成或手工制作的数据来切换文本和图像思维。需要一种更简单的方法来实现跨模态推理。

Method: 提出Mull-Tokens（模态无关潜在令牌），通过预训练学习在文本或图像模态中保存中间信息。首先使用交错文本-图像轨迹进行监督训练，然后仅使用最终答案进行无监督微调。

Result: 在四个具有挑战性的空间推理基准测试中（包括解谜和视角转换任务），Mull-Tokens相比仅使用文本推理或交错图像-文本推理的基线方法，平均提升3%，在推理密集的谜题解决任务上最高提升16%。

Conclusion: Mull-Tokens为文本和视觉推理的落地挑战提供了一个简单解决方案，允许模型在多个模态中进行抽象思考，超越了现有方法的局限性。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [7] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT：一种通过时间戳条件控制实现细粒度时序控制的主题驱动视频生成框架


<details>
  <summary>Details</summary>
Motivation: 现有主题驱动视频生成方法缺乏对主题出现和消失的细粒度时序控制，这在合成视频、故事板制作和可控动画等应用中至关重要。

Method: 提出AlcheMinT统一框架，引入显式时间戳条件控制，采用新颖的位置编码机制编码时间间隔（与主题身份关联），同时无缝集成预训练视频生成模型的位置嵌入。加入主题描述性文本标记以增强视觉身份与视频描述之间的绑定，通过标记级联避免额外的交叉注意力模块。

Result: AlcheMinT在视觉质量上与最先进的视频个性化方法相当，同时首次实现了视频中多主题生成的精确时序控制，在主题身份保持、视频保真度和时序遵循方面表现出色。

Conclusion: AlcheMinT通过时间戳条件控制解决了主题驱动视频生成中的时序控制问题，实现了多主题的精确时序控制，为合成视频、故事板制作等应用提供了重要工具。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: SceneMaker提出解耦的3D场景生成框架，通过分离去遮挡模型与3D物体生成，并构建统一姿态估计模型，解决现有方法在严重遮挡和开放集场景下几何质量与姿态精度难以兼顾的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集场景下，由于缺乏足够的开放集去遮挡和姿态估计先验，难以同时生成高质量的几何结构和准确的姿态。

Method: 1. 将去遮挡模型与3D物体生成解耦，利用图像数据集和收集的去遮挡数据集增强对多样化开放集遮挡模式的处理能力；2. 提出统一姿态估计模型，整合全局和局部机制的自注意力和交叉注意力以提高精度；3. 构建开放集3D场景数据集以扩展姿态估计模型的泛化能力。

Result: 综合实验表明，该解耦框架在室内和开放集场景上均表现出优越性，代码和数据集已开源。

Conclusion: SceneMaker通过解耦去遮挡与生成、改进姿态估计模型并构建专用数据集，有效解决了开放集场景下3D生成的质量与精度问题，为复杂场景的3D重建提供了有效方案。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 使用NRC-VAD词典对《霍比特人》对话进行情感计算分析，发现整体保持积极平静基调，随着故事发展角色能动性逐渐增强，情感节奏在紧张与舒适间循环。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算文本分析方法揭示文学作品中的微妙情感结构，将数字工具与文学阐释相结合，分析《霍比特人》对话的情感基调。

Method: 使用正则表达式提取对话文本，预处理后采用NRC-VAD词典对情感维度进行量化评分，包括效价、唤醒度和支配度三个维度。

Result: 对话整体保持积极（高效价）平静（低唤醒）基调，角色能动性（支配度）随故事进展逐渐增强；情感节奏呈现危险兴奋与幽默友情之间的平衡循环。

Conclusion: 计算工具与文学解读相结合能有效揭示文学作品中的情感结构，展示了《霍比特人》通过情感调节塑造的稳定叙事节奏。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [10] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 评估多模态大语言模型在政治视频情感分析中的表现，发现在理想条件下表现良好，但在真实议会辩论场景中效果不佳


<details>
  <summary>Details</summary>
Motivation: 情感在政治沟通中至关重要，多模态生成式AI为情感分析带来新机遇，但缺乏对其有效性的实证证据

Method: 使用两个互补的人工标注视频数据集，评估当前多模态大语言模型在视频情感唤醒度分析中的表现

Result: 理想条件下mLLMs的情感唤醒度评分高度可靠且无明显人口统计偏差，但在真实议会辩论录音中表现不佳，可能影响下游统计推断

Conclusion: 需要持续深入评估新兴生成式AI方法在政治分析中的应用，并提供了一个可复制的评估框架

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可用于大型用户设施的提案评审，通过成对偏好排序替代传统人工评分，相关性高且成本低两个数量级


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在评分相关性弱、评审者偏见和不一致性问题，而成对偏好方法虽然逻辑更严谨但工作量呈二次方增长，不适用于人工评审

Method: 利用LLMs进行成对偏好排序，基于橡树岭国家实验室SNS三个束线的提案和发表记录，使用嵌入模型进行提案相似性定量评估

Result: LLM排序与人工排序强相关（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5），识别高发表潜力提案的能力不亚于人工评审，成本降低两个数量级以上

Conclusion: LLMs为大型设施提案评审提供了可扩展、一致且经济高效的替代方案，并能进行传统方法难以实现的定量分析

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [12] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出节点级剪枝框架用于电路发现，解决现有方法计算成本高、粒度粗的问题，通过多粒度可学习掩码和稀疏惩罚实现单次微调中的全面压缩。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了神经元等更细粒度的结构，需要解决可扩展性和粒度限制问题。

Method: 提出节点级剪枝框架，引入跨多个粒度（从整个块到单个神经元）的可学习掩码，在统一优化目标中使用粒度特定的稀疏惩罚指导剪枝过程，实现单次微调中的全面压缩。

Result: 方法发现的电路节点数少于先前方法，证明许多粗粒度方法认为重要的神经元实际上无关紧要，同时保持任务性能；内存占用显著降低5-10倍，无需在内存中保存中间激活。

Conclusion: 提出的节点级剪枝框架在电路发现中同时解决了可扩展性和粒度限制问题，能够识别更细粒度的电路结构，计算效率更高，为理解LLM内部机制提供了更好的工具。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [13] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 建立了POMDP决策代理与单输入过程函数之间的精确对应关系，揭示了AI与量子物理框架的深层联系


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策代理与量子物理中高阶量子操作经典极限之间的数学对应关系，为跨学科理解提供新视角

Method: 通过将POMDP中代理的策略和记忆更新组合成过程函数w，使用链积与POMDP环境交互，建立形式化对应关系

Result: 成功建立了决策代理与过程函数之间的精确对应，揭示了物理视角（过程函数作为环境）与AI视角（过程函数编码代理）的对偶解释

Conclusion: 该对应关系可扩展到多智能体系统，将观察独立的分散POMDP识别为多输入过程函数的自然领域，为跨学科研究开辟新途径

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


### [14] [Unified Smart Factory Model: A model-based Approach for Integrating Industry 4.0 and Sustainability for Manufacturing Systems](https://arxiv.org/abs/2512.10631)
*Ishaan Kaushal,Amaresh Chakrabarti*

Main category: cs.AI

TL;DR: 提出统一智能工厂模型（USFM），将高层可持续目标转化为可测量的工厂级指标，通过系统信息映射制造活动，特别关注PCB组装工厂案例。


<details>
  <summary>Details</summary>
Motivation: 解决可持续目标与实际工厂实施之间的差距，特别是中小企业实现可持续目标时面临的挑战，需要系统化方法将高层目标转化为可操作的指标和数据需求。

Method: 使用基于模型的系统工程语言（对象过程方法论）建模制造活动，整合制造过程与系统、数据处理、KPI选择与评估三个模块，通过PCB组装工厂案例验证。

Result: 成功展示了如何选择、建模和映射环境可持续性KPI到必要数据，重点关注能耗和环境影响指标，证明模型能减少冗余、避免关键信息遗漏并增强数据收集。

Conclusion: USFM有效连接了可持续目标与实际实施，为特别是中小企业实现可持续目标提供了重要工具，通过系统化方法填补了目标与实施之间的鸿沟。

Abstract: This paper presents the Unified Smart Factory Model (USFM), a comprehensive framework designed to translate high-level sustainability goals into measurable factory-level indicators with a systematic information map of manufacturing activities. The manufacturing activities were modelled as set of manufacturing, assembly and auxiliary processes using Object Process Methodology, a Model Based Systems Engineering (MBSE) language. USFM integrates Manufacturing Process and System, Data Process, and Key Performance Indicator (KPI) Selection and Assessment in a single framework. Through a detailed case study of Printed Circuit Board (PCB) assembly factory, the paper demonstrates how environmental sustainability KPIs can be selected, modelled, and mapped to the necessary data, highlighting energy consumption and environmental impact metrics. The model's systematic approach can reduce redundancy, minimize the risk of missing critical information, and enhance data collection. The paper concluded that the USFM bridges the gap between sustainability goals and practical implementation, providing significant benefits for industries specifically SMEs aiming to achieve sustainability targets.

</details>
