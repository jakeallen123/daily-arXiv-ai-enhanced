<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 评估不同参数规模的SmolVLM2模型（500M和2.2B）在BLV用户可访问性视频描述任务中的表现，引入两个专门评估框架并测试不同提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对依赖详细上下文感知描述的盲人和低视力用户。

Method: 使用500M和2.2B参数的SmolVLM2变体，在两个多样化数据集（AVCaps户外和Charades室内）上进行评估，引入多上下文BLV框架和导航辅助框架，系统评估四种提示设计策略，并在智能手机上部署FP32和INT8精度变体。

Result: 论文评估了不同规模模型在可访问性描述质量方面的表现，但具体结果数据未在摘要中提供。

Conclusion: 通过系统评估不同参数规模、提示策略和部署方案，为BLV用户提供更实用的视频描述解决方案，平衡描述质量与资源限制。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出了Latent Upscaler Adapter (LUA)，一个轻量级模块，在VAE解码前直接在生成器的潜在代码上执行超分辨率，避免了传统图像超分辨率的伪影和额外延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以扩展到训练分辨率之外，直接高分辨率采样缓慢且昂贵，而事后图像超分辨率会在解码后引入伪影和额外延迟。

Method: LUA作为即插即用组件集成，无需修改基础模型或添加额外的扩散阶段，通过潜在空间中的单次前向传递实现高分辨率合成。使用共享的Swin风格骨干网络和特定尺度的像素重排头支持2倍和4倍放大。

Result: 与像素空间超分辨率相比，解码和上采样时间降低近3倍（从512px生成1024px仅增加+0.42秒，而相同SwinIR架构的像素空间SR需要1.87秒），同时保持可比较的感知质量。

Conclusion: LUA在保持原生高分辨率生成保真度的同时，为现代扩散管道提供了实用且高效的可扩展高保真图像合成路径，并在不同VAE的潜在空间中表现出强大的泛化能力。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: Depth Anything 3 (DA3) 是一个从任意数量视觉输入预测空间一致几何的模型，无需相机位姿信息，使用单一Transformer骨干网和深度射线预测目标，在几何精度和相机位姿估计方面超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 追求最小化建模，证明单一普通Transformer骨干网无需架构专业化，单一深度射线预测目标可避免复杂的多任务学习。

Method: 采用师生训练范式，使用单一Transformer骨干网（如DINO编码器）和深度射线预测目标，无需相机位姿信息。

Result: 在新建的视觉几何基准测试中，DA3在所有任务上达到新SOTA，相机位姿精度平均提升44.3%，几何精度提升25.1%，在单目深度估计上超越DA2。

Conclusion: DA3证明了简单架构的有效性，仅使用公开学术数据集训练，在几何预测和相机位姿估计方面实现了最先进的性能。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [4] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 提出Self-Consistency Sampling (SCS)方法，通过视觉扰动和轨迹重采样来解决多模态大语言模型中结果奖励强化学习的不忠实推理问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试中，结果奖励强化学习面临一个被忽视的障碍：不忠实的推理轨迹（即通过错误推理链但猜对选项）与真实推理获得相同奖励，这是一个不可忽视的缺陷。

Method: SCS方法对每个问题：(i)引入小的视觉扰动，(ii)对初始轨迹进行重复截断和重采样；通过结果轨迹之间的一致性产生可微的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率最高提升7.7个百分点，且额外计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也取得了显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单、通用的解决方案，有效解决了不忠实推理轨迹的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 论文提出了一种编码器增强的因果解码器模型架构，通过更高效的训练方法接近语言熵的下界，从而获得更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于因果语言模型的熵估计计算成本过高，需要开发更高效的方法来估计语言熵并改善模型泛化。

Method: 引入编码器增强的因果解码器架构，在有限硬件条件下实现更高压缩比，并基于每个token估计熵值。

Result: 训练接近但不超出估计每token熵值的因果模型比不考虑熵的模型表现出更好的泛化性能。

Conclusion: 通过考虑语言熵约束来训练模型，可以显著提高模型的泛化能力，这为高效语言模型训练提供了新方向。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [6] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR是一个新颖的LLM推理框架，通过将模型响应分解为可验证的子问题-子答案对，进行细粒度评估和精确精炼，在多个推理基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗粒度的自我验证和自我纠正，限制了在复杂任务上的有效性，需要更精细的评估和精炼方法。

Method: 将模型响应分解为可验证的子问题-子答案对，通过受控重解和自一致性检查进行步骤级置信度估计，精确定位不可靠步骤并进行迭代精炼。

Result: 在五个推理基准测试和三个LLM上的实证结果显示，SSR始终优于最先进的迭代自我精炼基线方法。

Conclusion: SSR不仅提供性能提升，还为评估和理解LLM内部推理过程提供了原则性的黑盒方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [7] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开放的30亿参数语言模型家族，使用公开数据和代码库训练，在完全开源模型中达到最先进水平，并发布了支持128K上下文长度的Instella-Long和专注于数学推理的Instella-Math两个变体。


<details>
  <summary>Details</summary>
Motivation: 当前大多数高性能语言模型都是闭源或部分开源，限制了透明度和可复现性，需要开发完全开源的替代方案。

Method: 使用AMD Instinct MI300X GPU进行大规模预训练、通用指令调优和人类偏好对齐，尽管预训练token数量较少，但通过专门优化实现高性能。

Result: Instella在完全开源模型中达到最先进水平，与同规模的开源权重模型具有竞争力，并开发了支持长上下文和数学推理的专门变体。

Conclusion: Instella为社区提供了透明、高性能且多功能的替代方案，推动了开放和可复现的语言建模研究。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [8] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: GAD是一种黑盒蒸馏方法，通过将学生LLM作为生成器、训练判别器来区分学生和教师模型的响应，形成极小极大博弈，实现无需访问教师模型内部参数的有效蒸馏。


<details>
  <summary>Details</summary>
Motivation: 黑盒蒸馏仅能学习专有教师模型的文本输出，无法访问其内部logits或参数，需要开发更有效的蒸馏方法。

Method: 将学生LLM作为生成器，训练判别器区分学生和教师模型的响应，形成极小极大博弈，判别器作为在线奖励模型与学生共同进化。

Result: GAD在实验中持续优于常用的序列级知识蒸馏方法，Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中与教师模型GPT-5-Chat表现相当。

Conclusion: GAD是黑盒LLM蒸馏的一个有前景且有效的范式。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [9] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种仅权重的后训练量化方法，通过成对旋转量化和通道缩放来减少异常值影响，在推理任务上比AWQ平均提升2.4%准确率，且开销低于10%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的权重和激活值存在异常值，导致量化误差大和精度严重下降，特别是在推理任务中误差会沿思维链累积。现有方法要么无法充分抑制异常值，要么在推理时引入显著开销。

Method: 结合硬件高效且可优化的独立Givens旋转与通道级缩放，均衡通道间幅值并缩小每个量化组内的动态范围；同时协同设计推理内核以充分利用GPU并行性，保持旋转和缩放操作在运行时轻量。

Result: 在推理任务上比AWQ平均提升2.4%准确率，且开销低于10%。

Conclusion: 为推理型大型语言模型更高效和准确的部署铺平了道路。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [10] [Towards an Agentic Workflow for Internet Measurement Research](https://arxiv.org/abs/2511.10611)
*Alagappan Ramanathan,Eunju Kang,Dongsu Han,Sangeetha Abdu Jyothi*

Main category: cs.NI

TL;DR: ArachNet是一个基于LLM代理的系统，能够自动生成网络测量工作流，模仿专家推理过程，解决网络测量研究中的可访问性危机。


<details>
  <summary>Details</summary>
Motivation: 网络测量研究面临可访问性危机：复杂分析需要集成多个专业工具，需要专业领域知识。当网络中断发生时，操作员需要快速诊断工作流，但开发这些工作流需要专业知识和大量手动工作。

Method: ArachNet通过四个专门代理运行，模仿专家工作流程，从问题分解到解决方案实施。系统基于LLM代理自动生成测量工作流，利用可预测的组合模式进行系统化自动化。

Result: 在逐步挑战性的互联网弹性场景中验证，系统独立生成的工作流与专家级推理匹配，产生与专业解决方案相似的分析输出。生成的工作流处理传统需要数天手动协调的复杂多框架集成。

Conclusion: ArachNet通过自动化专家使用的系统推理过程，降低了测量工作流组合的门槛，使更广泛的用户能够访问复杂的测量能力，同时保持研究质量分析所需的技术严谨性。

Abstract: Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.
  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 提出了一种验证模拟故障场景在真实世界中重现性的方法，通过定义时间序列传感器数据与Scenic场景程序的匹配关系，并开发了高效的查询算法来在真实数据集中定位匹配场景。


<details>
  <summary>Details</summary>
Motivation: 解决仿真测试中发现的自动驾驶故障场景是否能在真实世界中重现的问题，弥合仿真与现实之间的差距，避免将仿真数据产生的伪故障误判为真实问题。

Method: 使用Scenic概率编程语言表示抽象场景，定义时间序列传感器数据与场景程序的匹配标准，开发查询算法在标记数据集中高效识别匹配场景。

Result: 实验表明，该算法比最先进的商业视觉大语言模型更准确，查询速度快几个数量级，且能随查询时间序列数据时长扩展。

Conclusion: 该方法能有效验证仿真发现的故障场景在真实世界中的重现性，为仿真测试结果提供可靠验证，缩小仿真与现实差距。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
