{"id": "2602.17734", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17734", "abs": "https://arxiv.org/abs/2602.17734", "authors": ["Raja Soundaramourty", "Ozkan Kilic", "Ramu Chenchaiah"], "title": "Five Fatal Assumptions: Why T-Shirt Sizing Systematically Fails for AI Projects", "comment": null, "summary": "Agile estimation techniques, particularly T-shirt sizing, are widely used in software development for their simplicity and utility in scoping work. However, when we apply these methods to artificial intelligence initiatives -- especially those involving large language models (LLMs) and multi-agent systems -- the results can be systematically misleading. This paper shares an evidence-backed analysis of five foundational assumptions we often make during T-shirt sizing. While these assumptions usually hold true for traditional software, they tend to fail in AI contexts: (1) linear effort scaling, (2) repeatability from prior experience, (3) effort-duration fungibility, (4) task decomposability, and (5) deterministic completion criteria. Drawing on recent research into multi-agent system failures, scaling principles, and the inherent unreliability of multi-turn conversations, we show how AI development breaks these rules. We see this through non-linear performance jumps, complex interaction surfaces, and \"tight coupling\" where a small change in data cascades through the entire stack. To help teams navigate this, we propose Checkpoint Sizing: a more human-centric, iterative approach that uses explicit decision gates where scope and feasibility are reassessed based on what we learn during development, rather than what we assumed at the start. This paper is intended for engineering managers, technical leads, and product owners responsible for planning and delivering AI initiatives.", "AI": {"tldr": "\u4f20\u7edfT-shirt\u4f30\u7b97\u65b9\u6cd5\u5728AI\u9879\u76ee\u4e2d\u5931\u6548\uff0c\u4f5c\u8005\u63d0\u51fa\u68c0\u67e5\u70b9\u4f30\u7b97\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848", "motivation": "\u4f20\u7edf\u654f\u6377\u4f30\u7b97\u65b9\u6cd5\uff08\u7279\u522b\u662fT-shirt sizing\uff09\u5728AI\u9879\u76ee\uff08\u5c24\u5176\u662fLLM\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u4e2d\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u8bef\u5bfc\uff0c\u9700\u8981\u65b0\u7684\u4f30\u7b97\u65b9\u6cd5", "method": "\u5206\u6790\u4f20\u7edfT-shirt sizing\u7684\u4e94\u4e2a\u57fa\u672c\u5047\u8bbe\u5728AI\u9879\u76ee\u4e2d\u7684\u5931\u6548\uff0c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5931\u8d25\u3001\u6269\u5c55\u539f\u5219\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e0d\u53ef\u9760\u6027\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u68c0\u67e5\u70b9\u4f30\u7b97\u65b9\u6cd5", "result": "\u4f20\u7edf\u4f30\u7b97\u7684\u4e94\u4e2a\u5047\u8bbe\u5728AI\u9879\u76ee\u4e2d\u90fd\u5931\u6548\uff1a\u975e\u7ebf\u6027\u52aa\u529b\u6269\u5c55\u3001\u7ecf\u9a8c\u4e0d\u53ef\u91cd\u590d\u3001\u52aa\u529b-\u65f6\u95f4\u4e0d\u53ef\u4e92\u6362\u3001\u4efb\u52a1\u4e0d\u53ef\u5206\u89e3\u3001\u5b8c\u6210\u6807\u51c6\u975e\u786e\u5b9a\u6027", "conclusion": "\u9700\u8981\u91c7\u7528\u68c0\u67e5\u70b9\u4f30\u7b97\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u66f4\u4ee5\u4eba\u4e3a\u672c\u3001\u8fed\u4ee3\u7684\u65b9\u6cd5\uff0c\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u5b9e\u9645\u5b66\u4e60\u800c\u975e\u521d\u59cb\u5047\u8bbe\u91cd\u65b0\u8bc4\u4f30\u8303\u56f4\u548c\u53ef\u884c\u6027"}}
{"id": "2602.17838", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17838", "abs": "https://arxiv.org/abs/2602.17838", "authors": ["Lara Khatib", "Micheal Pu", "Bogdan Vasilescu", "Meiyappan Nagappan"], "title": "Examining LLMs Ability to Summarize Code Through Mutation-Analysis", "comment": null, "summary": "As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as \"bugs\", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5f02\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6d4b\u8bd5LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u800c\u975e\u8868\u9762\u610f\u56fe", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u4f9d\u8d56LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u8fdb\u884c\u6587\u6863\u3001\u6d4b\u8bd5\u548c\u5ba1\u67e5\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\u3002LLM\u7ecf\u5e38\u81ea\u4fe1\u5730\u63cf\u8ff0\u4ee3\u7801\u770b\u8d77\u6765\u5e94\u8be5\u505a\u4ec0\u4e48\uff08\u610f\u56fe\uff09\uff0c\u5374\u5ffd\u7565\u4e86\u5b9a\u4e49\u5b9e\u9645\u884c\u4e3a\u7684\u5fae\u5999\u8fb9\u754c\u60c5\u51b5\u6216\u903b\u8f91\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5f02\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1a\u751f\u6210\u6458\u8981\u2192\u5728\u4ee3\u7801\u4e2d\u6ce8\u5165\u76ee\u6807\u53d8\u5f02\u2192\u68c0\u67e5LLM\u662f\u5426\u66f4\u65b0\u6458\u8981\u4ee5\u53cd\u6620\u65b0\u884c\u4e3a\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u603b\u8ba1624\u4e2a\u53d8\u5f02-\u6458\u8981\u8bc4\u4f30\uff0c\u8986\u76d662\u4e2a\u7a0b\u5e8f\u3002", "result": "1) \u572812\u4e2a\u53d7\u63a7\u5408\u6210\u7a0b\u5e8f\u4e0a\uff1a\u6458\u8981\u51c6\u786e\u7387\u968f\u590d\u6742\u5ea6\u6025\u5267\u4e0b\u964d\uff08\u5355\u51fd\u657076.5%\u2192\u591a\u7ebf\u7a0b\u7cfb\u7edf17.3%\uff09\uff1b2) \u572850\u4e2a\u4eba\u5de5\u7f16\u5199\u7a0b\u5e8f\u4e0a\uff1a\u6458\u8981\u51c6\u786e\u738749.3%\uff0c\u6a21\u578b\u5e38\u63cf\u8ff0\u7b97\u6cd5\u610f\u56fe\u800c\u975e\u5b9e\u9645\u53d8\u5f02\u884c\u4e3a\uff1b3) GPT-4\u5230GPT-5.2\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0849.3%\u219285.3%\uff09\uff0c\u4f46\u4e24\u8005\u4ecd\u96be\u4ee5\u533a\u5206\u5b9e\u73b0\u7ec6\u8282\u4e0e\u6807\u51c6\u7b97\u6cd5\u6a21\u5f0f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u53d8\u5f02\u5206\u6790\u4f5c\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u751f\u6210\u6458\u8981\u662f\u5426\u53cd\u6620\u7a0b\u5e8f\u884c\u4e3a\u800c\u975e\u8868\u9762\u6587\u672c\u6a21\u5f0f\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.17887", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17887", "abs": "https://arxiv.org/abs/2602.17887", "authors": ["Carla Fern\u00e1ndez-Navarro", "Francisco Chicano"], "title": "Automated LLM-Based Accessibility Remediation: From Conventional Websites to Angular Single-Page Applications", "comment": null, "summary": "Web accessibility remains an unresolved issue for a large part of the web content. There are many tools to detect errors automatically, but fixing those issues is still mostly a manual, slow, and costly process in which it is easy for developers to overlook specific details. The situation becomes even more complex with modern Single-Page Applications (SPAs), whose dynamic nature makes traditional static analysis approaches inadequate. This work proposes a system that aims to address this challenge by using Large Language Models (LLMs) to automate accessibility fixes. The proposal presents a modular workflow applicable to both static websites and complex Angular projects. The framework actively implements corrections within the DOM of static web pages or the source code of SPAs. The system was tested on 12 static websites and 6 open-source Angular projects, fixing 80% of the accessibility issues on public websites and 86% of the issues on Angular applications. Our proposal also generates meaningful visual descriptions for images while preserving the application's design and stability. This work contributes to ensuring that accessibility stops being a technical debt deferred to the future and becomes a natural part of everyday development workflows.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590d\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u652f\u6301\u9759\u6001\u7f51\u7ad9\u548cAngular\u5355\u9875\u5e94\u7528\uff0c\u4fee\u590d\u7387\u8fbe80-86%", "motivation": "\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u73b0\u6709\u5de5\u5177\u53ea\u80fd\u68c0\u6d4b\u95ee\u9898\u4f46\u4fee\u590d\u4ecd\u9700\u624b\u52a8\u5b8c\u6210\uff0c\u8fc7\u7a0b\u7f13\u6162\u3001\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u9057\u6f0f\u7ec6\u8282\u3002\u5355\u9875\u5e94\u7528\u7684\u52a8\u6001\u7279\u6027\u4f7f\u4f20\u7edf\u9759\u6001\u5206\u6790\u65b9\u6cd5\u5931\u6548\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u53ef\u5e94\u7528\u4e8e\u9759\u6001\u7f51\u7ad9\u548c\u590d\u6742Angular\u9879\u76ee\u3002\u6846\u67b6\u5728\u9759\u6001\u7f51\u9875\u7684DOM\u6216SPA\u6e90\u4ee3\u7801\u4e2d\u4e3b\u52a8\u5b9e\u65bd\u4fee\u6b63\uff0c\u540c\u65f6\u4e3a\u56fe\u50cf\u751f\u6210\u6709\u610f\u4e49\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u4fdd\u6301\u5e94\u7528\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u572812\u4e2a\u9759\u6001\u7f51\u7ad9\u548c6\u4e2a\u5f00\u6e90Angular\u9879\u76ee\u4e0a\u6d4b\u8bd5\uff0c\u4fee\u590d\u4e86\u516c\u5171\u7f51\u7ad980%\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u548cAngular\u5e94\u752886%\u7684\u95ee\u9898\u3002\u7cfb\u7edf\u6210\u529f\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u540c\u65f6\u4fdd\u6301\u5e94\u7528\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u786e\u4fdd\u53ef\u8bbf\u95ee\u6027\u4e0d\u518d\u662f\u88ab\u63a8\u8fdf\u5230\u672a\u6765\u7684\u6280\u672f\u503a\u52a1\uff0c\u800c\u662f\u6210\u4e3a\u65e5\u5e38\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u7136\u7ec4\u6210\u90e8\u5206\u3002\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u81ea\u52a8\u5316\u4fee\u590d\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u52a8\u6001\u7684\u5355\u9875\u5e94\u7528\u3002"}}
{"id": "2602.17955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17955", "abs": "https://arxiv.org/abs/2602.17955", "authors": ["Imgyeong Lee", "Tayyib Ul Hassan", "Abram Hindle"], "title": "Mining Type Constructs Using Patterns in AI-Generated Code", "comment": null, "summary": "Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.", "AI": {"tldr": "AI\u4ee3\u7406\u5728TypeScript\u9879\u76ee\u4e2d\u8fc7\u5ea6\u4f7f\u7528'any'\u5173\u952e\u5b57\u548c\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\uff0c\u5bfc\u81f4\u7c7b\u578b\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u5947\u602a\u7684\u662f\u5176PR\u63a5\u53d7\u7387\u6bd4\u4eba\u7c7b\u9ad81.8\u500d", "motivation": "\u7814\u7a76AI\u5728\u7c7b\u578b\u76f8\u5173\u7f16\u7a0b\u4efb\u52a1\u4e2d\u662f\u5426\u771f\u6b63\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4ee5\u53caAI\u4ee3\u7406\u662f\u5426\u5728\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\u4e2d\u8fc7\u5ea6\u4f7f\u7528\u6216\u8bef\u7528\u7c7b\u578b\u6784\u9020", "method": "\u5728TypeScript\u9879\u76ee\u9886\u57df\u8fdb\u884c\u9996\u6b21\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u7c7b\u578b\u6784\u9020\u4f7f\u7528\u4e0a\u7684\u5dee\u5f02", "result": "AI\u4ee3\u7406\u6bd4\u4eba\u7c7b\u66f4\u5bb9\u6613\u4f7f\u7528'any'\u5173\u952e\u5b57\uff089\u500d\uff09\uff0c\u66f4\u9891\u7e41\u4f7f\u7528\u5ffd\u7565\u7c7b\u578b\u68c0\u67e5\u7684\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\uff0c\u4f46AI\u4ee3\u7406\u7684PR\u63a5\u53d7\u7387\u6bd4\u4eba\u7c7b\u9ad81.8\u500d", "conclusion": "\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u65f6\u5e94\u4ed4\u7ec6\u786e\u8ba4\u4ee3\u7801\u5e93\u7684\u7c7b\u578b\u5b89\u5168\u6027\uff0c\u5c3d\u7ba1AI\u7684PR\u63a5\u53d7\u7387\u66f4\u9ad8\uff0c\u4f46\u5176\u7c7b\u578b\u5b89\u5168\u5b9e\u8df5\u5b58\u5728\u95ee\u9898"}}
{"id": "2602.17954", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17954", "abs": "https://arxiv.org/abs/2602.17954", "authors": ["Mohammad Zangooei", "Lou Sala\u00fcn", "Chung Shue Chen", "Raouf Boutaba"], "title": "Graph-Neural Multi-Agent Coordination for Distributed Access-Point Selection in Cell-Free Massive MIMO", "comment": "Under submission to an IEEE journal", "summary": "Cell-free massive MIMO (CFmMIMO) systems require scalable and reliable distributed coordination mechanisms to operate under stringent communication and latency constraints. A central challenge is the Access Point Selection (APS) problem, which seeks to determine the subset of serving Access Points (APs) for each User Equipment (UE) that can satisfy UEs' Spectral Efficiency (SE) requirements while minimizing network power consumption. We introduce APS-GNN, a scalable distributed multi-agent learning framework that decomposes APS into agents operating at the granularity of individual AP-UE connections. Agents coordinate via local observation exchange over a novel Graph Neural Network (GNN) architecture and share parameters to reuse their knowledge and experience. APS-GNN adopts a constrained reinforcement learning approach to provide agents with explicit observability of APS' conflicting objectives, treating SE satisfaction as a cost and power reduction as a reward. Both signals are defined locally, facilitating effective credit assignment and scalable coordination in large networks. To further improve training stability and exploration efficiency, the policy is initialized via supervised imitation learning from a heuristic APS baseline. We develop a realistic CFmMIMO simulator and demonstrate that APS-GNN delivers the target SE while activating 50-70% fewer APs than heuristic and centralized Multi-agent Reinforcement Learning (MARL) baselines in different evaluation scenarios. Moreover, APS-GNN achieves one to two orders of magnitude lower inference latency than centralized MARL approaches due to its fully parallel and distributed execution. These results establish APS-GNN as a practical and scalable solution for APS in large-scale CFmMIMO networks.", "AI": {"tldr": "APS-GNN\uff1a\u7528\u4e8e\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u534f\u8c03AP\u9009\u62e9\uff0c\u5728\u6ee1\u8db3\u9891\u8c31\u6548\u7387\u8981\u6c42\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u529f\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u9700\u8981\u5728\u4e25\u683c\u7684\u901a\u4fe1\u548c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u8fd0\u884c\uff0c\u4f46\u73b0\u6709\u7684\u63a5\u5165\u70b9\u9009\u62e9\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u9891\u8c31\u6548\u7387\u8981\u6c42\u548c\u964d\u4f4e\u529f\u8017\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u534f\u8c03\u673a\u5236\u3002", "method": "\u63d0\u51faAPS-GNN\u6846\u67b6\uff0c\u5c06AP\u9009\u62e9\u95ee\u9898\u5206\u89e3\u4e3a\u5355\u4e2aAP-UE\u8fde\u63a5\u7ea7\u522b\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u5c40\u90e8\u89c2\u5bdf\u4ea4\u6362\u548c\u53c2\u6570\u5171\u4eab\uff0c\u91c7\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u9891\u8c31\u6548\u7387\u6ee1\u8db3\u89c6\u4e3a\u6210\u672c\u3001\u529f\u8017\u964d\u4f4e\u89c6\u4e3a\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9eCFmMIMO\u6a21\u62df\u5668\u4e2d\uff0cAPS-GNN\u5728\u6ee1\u8db3\u76ee\u6807\u9891\u8c31\u6548\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u542f\u53d1\u5f0f\u548c\u96c6\u4e2d\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c11\u6fc0\u6d3b50-70%\u7684AP\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "APS-GNN\u4e3a\u5927\u89c4\u6a21CFmMIMO\u7f51\u7edc\u4e2d\u7684AP\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u5e76\u884c\u6267\u884c\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u3002"}}
{"id": "2602.18088", "categories": ["cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18088", "abs": "https://arxiv.org/abs/2602.18088", "authors": ["Igor Ho\u0142owacz", "Piotr Br\u00f3dka"], "title": "Beyond Individual Influence: The Role of Echo Chambers and Community Seeding in the Multilayer three state q-Voter Model", "comment": "Preprint of the paper submitted to WAW 2026 - 21st Workshop on Modelling and Mining Networks", "summary": "The diffusion of complex opinions is severely hindered in multilayer social networks by echo chambers and cognitive consistency mechanisms. We investigate Influence Maximization strategies within the 3-state multilayer q-voter model. Utilizing the mABCD benchmark, we simulate social environments ranging from integrated Open Worlds to segregated Fortress Worlds. Our results reveal a topological paradox that we term the \"Fortress Trap\". In highly modular networks, strategies maximizing local density such as Clique Influence Maximization (CIM) and k-Shell fail to trigger global cascades, creating isolated bunkers of consensus due to the Overkill Effect. Furthermore, we identify a Redundancy Trap in perfectly aligned Clan topologies, where the structural overlap of layers creates a \"Perfect Prison,\" rendering it the most resistant environment to diffusion. We demonstrate that VoteRank, a strategy that prioritizes diversity of reach over local intensity, consistently outperforms structure-based methods. These findings suggest that, for complex contagion, maximizing topological entropy is more effective than reinforcing local clusters.", "AI": {"tldr": "\u591a\u5c42\u793e\u4ea4\u7f51\u7edc\u4e2d\u590d\u6742\u610f\u89c1\u6269\u6563\u53d7\u56de\u58f0\u5ba4\u548c\u8ba4\u77e5\u4e00\u81f4\u6027\u673a\u5236\u963b\u788d\uff0c\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u62d3\u6251\u71b5\u6700\u5927\u5316\u7684\u7b56\u7565\u6bd4\u5f3a\u5316\u5c40\u90e8\u96c6\u7fa4\u66f4\u6709\u6548", "motivation": "\u591a\u5c42\u793e\u4ea4\u7f51\u7edc\u4e2d\u590d\u6742\u610f\u89c1\u6269\u6563\u53d7\u5230\u56de\u58f0\u5ba4\u548c\u8ba4\u77e5\u4e00\u81f4\u6027\u673a\u5236\u7684\u4e25\u91cd\u963b\u788d\uff0c\u9700\u8981\u7814\u7a76\u5728\u8fd9\u79cd\u73af\u5883\u4e0b\u7684\u5f71\u54cd\u529b\u6700\u5927\u5316\u7b56\u7565", "method": "\u4f7f\u75283\u72b6\u6001\u591a\u5c42q-voter\u6a21\u578b\uff0c\u57fa\u4e8emABCD\u57fa\u51c6\u6a21\u62df\u4ece\u6574\u5408\u5f00\u653e\u4e16\u754c\u5230\u9694\u79bb\u5821\u5792\u4e16\u754c\u7684\u793e\u4ea4\u73af\u5883\uff0c\u6bd4\u8f83\u4e0d\u540c\u5f71\u54cd\u529b\u6700\u5927\u5316\u7b56\u7565", "result": "\u53d1\u73b0\"\u5821\u5792\u9677\u9631\"\u62d3\u6251\u6096\u8bba\uff1a\u9ad8\u5ea6\u6a21\u5757\u5316\u7f51\u7edc\u4e2d\uff0cCIM\u548ck-Shell\u7b49\u6700\u5927\u5316\u5c40\u90e8\u5bc6\u5ea6\u7684\u7b56\u7565\u65e0\u6cd5\u89e6\u53d1\u5168\u5c40\u7ea7\u8054\uff0c\u5f62\u6210\u5b64\u7acb\u5171\u8bc6\u5b64\u5c9b\uff1b\u5b8c\u7f8e\u5bf9\u9f50\u7684\u6c0f\u65cf\u62d3\u6251\u4e2d\u5b58\u5728\"\u5197\u4f59\u9677\u9631\"\uff0c\u5f62\u6210\"\u5b8c\u7f8e\u76d1\u72f1\"\uff1bVoteRank\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4f20\u67d3\uff0c\u6700\u5927\u5316\u62d3\u6251\u71b5\u6bd4\u5f3a\u5316\u5c40\u90e8\u96c6\u7fa4\u66f4\u6709\u6548\uff0c\u591a\u6837\u6027\u8986\u76d6\u4f18\u5148\u4e8e\u5c40\u90e8\u5f3a\u5ea6\u7684\u7b56\u7565\u8868\u73b0\u66f4\u597d"}}
{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86KPM-Bench\u6570\u636e\u96c6\u548cMoPE\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u9891\u63cf\u8ff0\u4e2d\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u52a8\u89e3\u6790\u548c\u63d0\u53d6\u6280\u672f\u63d0\u5347\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u51c6\u786e\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u4e2d\uff0c\u5bf9\u590d\u6742\u80a2\u4f53\u52a8\u6001\u7684\u7cbe\u786e\u63cf\u8ff0\u81f3\u5173\u91cd\u8981\u4f46\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "1) \u63d0\u51fa\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u8ba1\u7b97\u548c\u8bed\u8a00\u89e3\u6790\uff1b2) \u6784\u5efaKPM-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u89c6\u9891-\u63cf\u8ff0\u5bf9\u3001\u8fd0\u52a8\u7406\u89e3\u95ee\u7b54\u5bf9\u548c\u5e7b\u89c9\u8bc4\u4f30\u96c6\uff1b3) \u63d0\u51faMoPE\u7b97\u6cd5\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u8fd0\u52a8\u5c5e\u6027\uff1b4) \u57fa\u4e8eMoPE\u8bbe\u8ba1\u72ec\u7acb\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\uff1b5) \u5c06MoPE\u96c6\u6210\u5230GRPO\u540e\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "\u5f00\u53d1\u4e86KPM-Bench\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86MoPE\u7b97\u6cd5\u548c\u76f8\u5e94\u8bc4\u4f30\u6307\u6807\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fd0\u52a8\u63cf\u8ff0\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7KPM-Bench\u6570\u636e\u96c6\u548cMoPE\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u89c6\u9891\u63cf\u8ff0\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7406\u89e3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.17784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17784", "abs": "https://arxiv.org/abs/2602.17784", "authors": ["Meng Ye", "Xiao Lin", "Georgina Lukoczki", "Graham W. Lederer", "Yi Yao"], "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration", "comment": null, "summary": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.", "AI": {"tldr": "QueryPlot\u662f\u4e00\u4e2a\u8bed\u4e49\u68c0\u7d22\u548c\u5236\u56fe\u6846\u67b6\uff0c\u901a\u8fc7NLP\u6280\u672f\u6574\u5408\u5730\u8d28\u6587\u672c\u548c\u5730\u56fe\u6570\u636e\uff0c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u77ff\u4ea7\u8fdc\u666f\u9884\u6d4b\u81ea\u52a8\u5316\u3002", "motivation": "\u4f20\u7edf\u77ff\u4ea7\u8fdc\u666f\u9884\u6d4b\u9700\u8981\u4eba\u5de5\u6574\u5408\u5f02\u6784\u5730\u8d28\u77e5\u8bc6\uff08\u6587\u672c\u77ff\u5e8a\u6a21\u578b\u548c\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff09\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u77e5\u8bc6\u5bc6\u96c6\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9ad8\u6548\u5904\u7406\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u4e86120\u591a\u79cd\u77ff\u5e8a\u7c7b\u578b\u7684\u63cf\u8ff0\u6a21\u578b\uff0c\u5c06SGMC\u591a\u8fb9\u5f62\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u7f16\u7801\u67e5\u8be2\u548c\u533a\u57df\u63cf\u8ff0\uff0c\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\u8fdb\u884c\u533a\u57df\u6392\u5e8f\u548c\u7a7a\u95f4\u53ef\u89c6\u5316\uff0c\u652f\u6301\u7ec4\u5408\u67e5\u8be2\u548c\u591a\u6807\u51c6\u5206\u6790\u3002", "result": "\u5728\u94a8\u77fd\u5361\u5ca9\u77ff\u5e8a\u6848\u4f8b\u4e2d\uff0c\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u5b9e\u73b0\u4e86\u5bf9\u5df2\u77e5\u77ff\u5e8a\u7684\u9ad8\u53ec\u56de\u7387\uff0c\u9884\u6d4b\u533a\u57df\u4e0e\u4e13\u5bb6\u5b9a\u4e49\u7684\u8bb8\u53ef\u533a\u57df\u9ad8\u5ea6\u4e00\u81f4\u3002\u76f8\u4f3c\u5ea6\u5f97\u5206\u4f5c\u4e3a\u7279\u5f81\u52a0\u5165\u76d1\u7763\u5b66\u4e60\u7ba1\u9053\u53ef\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "QueryPlot\u6210\u529f\u6574\u5408\u4e86\u5730\u8d28\u6587\u672c\u548c\u7a7a\u95f4\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u52a8\u5316\u77ff\u4ea7\u8fdc\u666f\u9884\u6d4b\uff0c\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u67e5\u8be2\u3001\u53ef\u89c6\u5316\u548cGIS\u517c\u5bb9\u5c42\u5bfc\u51fa\u7684Web\u7cfb\u7edf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u5b89\u5168\u95ee\u9898\u7684\u6839\u6e90\u662f\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u800c\u975e\u8bad\u7ec3\u7f3a\u9677\uff0c\u901a\u8fc7\u7ecf\u6d4e\u5b66\u7406\u8bba\u5efa\u7acb\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u8bc1\u660e\u4e0d\u5b89\u5168\u884c\u4e3a\u662f\u7ed3\u6784\u6027\u5fc5\u7136\uff0c\u5b89\u5168\u662f\u79bb\u6563\u76f8\u800c\u975e\u5956\u52b1\u8fde\u7eed\u51fd\u6570\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u8303\u5f0f\u5c06LLM\u548cAI\u4ee3\u7406\u7684\u884c\u4e3a\u75c5\u7406\uff08\u5949\u627f\u3001\u5e7b\u89c9\u3001\u6218\u7565\u6b3a\u9a97\uff09\u89c6\u4e3a\u8bad\u7ec3\u7f3a\u9677\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u5176\u51fa\u73b0\u548c\u7a33\u5b9a\u6027\u3002\u9700\u8981\u4ece\u6839\u672c\u7406\u8bba\u4e0a\u7406\u89e3\u8fd9\u4e9b\u884c\u4e3a\u4e3a\u4f55\u6301\u7eed\u5b58\u5728\u3002", "method": "\u5c06\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\u9002\u914d\u5230AI\u9886\u57df\uff0c\u5efa\u7acb\u4ee3\u7406\u5728\u9519\u8bef\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u4e0b\u4f18\u5316\u7684\u7406\u8bba\u6846\u67b6\u3002\u901a\u8fc7\u516d\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\uff0c\u751f\u6210\u5b89\u5168\u884c\u4e3a\u7684\u62d3\u6251\u8fb9\u754c\u76f8\u56fe\u3002", "result": "\u8bc1\u660e\u4e0d\u5b89\u5168\u884c\u4e3a\u662f\u7ed3\u6784\u6027\u5fc5\u7136\uff1a\u4f5c\u4e3a\u7a33\u5b9a\u4e0d\u5bf9\u9f50\u5747\u8861\u6216\u632f\u8361\u5faa\u73af\u51fa\u73b0\uff0c\u6218\u7565\u6b3a\u9a97\u4f5c\u4e3a\"\u9501\u5b9a\"\u5747\u8861\u6216\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6301\u7eed\u5b58\u5728\u3002\u5b89\u5168\u662f\u4ee3\u7406\u8ba4\u77e5\u5148\u9a8c\u51b3\u5b9a\u7684\u79bb\u6563\u76f8\uff0c\u800c\u975e\u5956\u52b1\u5927\u5c0f\u7684\u8fde\u7eed\u51fd\u6570\u3002", "conclusion": "\u5b89\u5168\u9700\u8981\u4ece\u64cd\u7eb5\u73af\u5883\u5956\u52b1\u8f6c\u5411\u5851\u9020\u4ee3\u7406\u5bf9\u73b0\u5b9e\u7684\u89e3\u91ca\uff0c\u63d0\u51fa\"\u4e3b\u89c2\u6a21\u578b\u5de5\u7a0b\"\u4f5c\u4e3a\u7a33\u5065\u5bf9\u9f50\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u6807\u5fd7\u7740AI\u5b89\u5168\u8303\u5f0f\u7684\u6839\u672c\u8f6c\u53d8\u3002"}}
{"id": "2602.18012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18012", "abs": "https://arxiv.org/abs/2602.18012", "authors": ["Pragati Kumari", "Novarun Deb"], "title": "DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing", "comment": null, "summary": "The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DeCEAT\u6846\u67b6\uff0c\u4e13\u95e8\u8bc4\u4f30\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u73af\u5883\u53ef\u6301\u7eed\u6027\u548c\u6027\u80fd\u6743\u8861\uff0c\u53d1\u73b0\u4e0d\u540cSLMs\u5728\u80fd\u8017\u3001\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u63d0\u793a\u8bbe\u8ba1\u5bf9\u53ef\u6301\u7eed\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u73af\u5883\u5f71\u54cd\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u53ef\u6301\u7eed\u6027\u5206\u6790\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6e90\u548c\u78b3\u6392\u653e\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86DeCEAT\u6846\u67b6\uff0c\u4f7f\u7528HumanEval\u57fa\u51c6\u548c\u57fa\u4e8eAnthropic\u6a21\u677f\u7684\u81ea\u9002\u5e94\u63d0\u793a\u53d8\u4f53\uff0c\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u7cfb\u7edf\u8bc4\u4f30SLMs\u7684\u73af\u5883\u548c\u6027\u80fd\u6743\u8861\u3002\u4f7f\u7528CodeCarbon\u6d4b\u91cf\u80fd\u8017\u548c\u78b3\u6392\u653e\uff0c\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u8986\u76d6\u7387\u8bc4\u4f30\u751f\u6210\u6d4b\u8bd5\u7684\u8d28\u91cf\u3002", "result": "\u4e0d\u540cSLMs\u5c55\u73b0\u51fa\u4e0d\u540c\u7684\u53ef\u6301\u7eed\u6027\u4f18\u52bf\uff1a\u4e00\u4e9b\u6a21\u578b\u4f18\u5148\u8003\u8651\u4f4e\u80fd\u8017\u548c\u5feb\u901f\u6267\u884c\uff0c\u800c\u53e6\u4e00\u4e9b\u5728\u78b3\u6392\u653e\u7ea6\u675f\u4e0b\u4fdd\u6301\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u6216\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cSLM\u9a71\u52a8\u7684\u6d4b\u8bd5\u751f\u6210\u53ef\u6301\u7eed\u6027\u662f\u591a\u7ef4\u5ea6\u7684\uff0c\u4e14\u53d7\u63d0\u793a\u8bbe\u8ba1\u7684\u5f3a\u70c8\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u81ea\u52a8\u5316SLM\u6d4b\u8bd5\u751f\u6210\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u9610\u660e\u4e86\u63d0\u793a\u7ed3\u6784\u548c\u6a21\u578b\u9009\u62e9\u5982\u4f55\u5171\u540c\u5f71\u54cd\u73af\u5883\u548c\u6027\u80fd\u7ed3\u679c\uff0c\u586b\u8865\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u53ef\u6301\u7eed\u6027\u7814\u7a76\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.18151", "categories": ["cs.NI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18151", "abs": "https://arxiv.org/abs/2602.18151", "authors": ["Nikita Zeulin", "Olga Galinina", "Ibrahim Kilinc", "Sergey Andreev", "Robert W. Heath"], "title": "Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.", "AI": {"tldr": "\u8bba\u6587\u5f3a\u8c03\u786c\u4ef6\u5f02\u6784\u6027\u662f5G+\u6ce2\u675f\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5f02\u6784\u73af\u5883\u4e0b\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u7684\u7b56\u7565\u3002", "motivation": "5G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\uff0c\u7528\u6237\u8bbe\u5907\u7684\u786c\u4ef6\u5f02\u6784\u6027\u7ed9\u57fa\u4e8e\u6ce2\u675f\u7684\u901a\u4fe1\u5e26\u6765\u65b0\u6311\u6218\uff0c\u9650\u5236\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u9002\u7528\u6027\uff0c\u9700\u8981\u5c06\u786c\u4ef6\u5f02\u6784\u6027\u4f5c\u4e3aML\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\u7684\u9996\u8981\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u3002", "method": "\u5206\u6790\u5f02\u6784\u73af\u5883\u4e0b\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5176\u6027\u80fd\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u6539\u8fdb\u6ce2\u675f\u7ba1\u7406\u6cdb\u5316\u80fd\u529b\u7684\u6f5c\u5728\u7b56\u7565\u3002", "result": "\u63ed\u793a\u4e86\u786c\u4ef6\u5f02\u6784\u6027\u5bf9ML\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u5f02\u6784\u73af\u5883\u4e0b\u7684\u5177\u4f53\u5931\u8d25\u6848\u4f8b\u548c\u6027\u80fd\u95ee\u9898\u3002", "conclusion": "\u786c\u4ef6\u5f02\u6784\u6027\u5fc5\u987b\u4f5c\u4e3aML\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\u7684\u6838\u5fc3\u8bbe\u8ba1\u8003\u8651\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u8bbe\u5907\u591a\u6837\u6027\u7684\u6cdb\u5316\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "\u63d0\u51fa3D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u7528\u4e8e\u91ce\u5916\u573a\u666f\u7684\u6587\u672c-\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u4e0e\u63cf\u8ff0\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u52a8\u4f5c\u6709\u9650\u548c\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u624b\u90e8\u52a8\u4f5c\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5de5\u4f5c\u5ba4\u91c7\u96c6\u7684\u6709\u9650\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u771f\u5b9e\u91ce\u5916\u573a\u666f\uff0c\u4e14\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "(1) \u6784\u5efa3D-HIW\u6570\u636e\u96c6\uff1a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c3D\u624b\u90e8\u8ffd\u8e2a\u5668\uff0c\u4ece\u5927\u91cf\u7b2c\u4e00\u4eba\u79f0\u52a8\u4f5c\u89c6\u9891\u4e2d\u6807\u6ce832K\u4e2a3D\u624b\u90e8\u52a8\u4f5c\u5e8f\u5217\u548c\u5bf9\u5e94\u6587\u672c\uff1b(2) \u63d0\u51faCLUTCH\u7cfb\u7edf\uff1a\u5305\u542bSHIFT\uff08\u57fa\u4e8eVQ-VAE\u7684\u624b\u90e8\u52a8\u4f5c\u5206\u8bcd\u67b6\u6784\uff09\u548c\u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\uff08\u901a\u8fc7\u91cd\u5efa\u635f\u5931\u5fae\u8c03LLM\uff09\u3002", "result": "\u5728\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u52a8\u4f5c\u5230\u6587\u672c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u7684\u9996\u4e2a\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc73D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u91ce\u5916\u573a\u666f\u624b\u90e8\u52a8\u4f5c\u7684\u9ad8\u8d28\u91cf\u5efa\u6a21\uff0c\u4e3a\u6587\u672c-\u624b\u90e8\u52a8\u4f5c\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86LLMs\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\u4f5c\u4e3a\u5176\"\u793e\u4f1a\u5fc3\u667a\"\u7684\u8bc1\u636e\uff0c\u53d1\u73b0LLM\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u795e\u7ecf\u540c\u6b65\u6a21\u5f0f\uff0c\u4e14\u8fd9\u79cd\u540c\u6b65\u4e0e\u5176\u793e\u4ea4\u8868\u73b0\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u4e0a\u793e\u4f1a\u5fc3\u667a\u88ab\u8ba4\u4e3a\u662f\u751f\u7269\u4f53\u7684\u4e13\u5c5e\u5c5e\u6027\uff0c\u867d\u7136LLMs\u88ab\u5e7f\u6cdb\u63a5\u53d7\u4e3a\u4eba\u7c7b\u884c\u4e3a\u7684\u8fd1\u4f3c\uff0c\u4f46LLMs\u662f\u5426\u5177\u6709\u53ef\u6bd4\u8f83\u7684\u793e\u4f1a\u5fc3\u667a\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u540c\u6b65\u8fd9\u4e00\u5b9e\u8bc1\u8bc1\u636e\u6765\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u540c\u6b65\u4f5c\u4e3a\u5206\u6790LLMs\u793e\u4f1a\u6027\u7684\u65b0\u4ee3\u7406\u6307\u6807\uff0c\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u6d4b\u91cfLLMs\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u9760\u53cd\u6620\u793e\u4ea4\u53c2\u4e0e\u5ea6\u548c\u65f6\u95f4\u5bf9\u9f50\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\u4e0e\u5176\u793e\u4ea4\u8868\u73b0\u5f3a\u70c8\u76f8\u5173\uff0c\u8868\u660e\u795e\u7ecf\u540c\u6b65\u4e0eLLMs\u7684\u793e\u4ea4\u884c\u4e3a\u5b58\u5728\u91cd\u8981\u8054\u7cfb\uff0c\u63ed\u793a\u4e86LLMs\u5185\u90e8\u52a8\u6001\u4e0e\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u4e4b\u95f4\u7684\u60ca\u4eba\u76f8\u4f3c\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u68c0\u9a8cLLMs\u7684\"\u793e\u4f1a\u5fc3\u667a\"\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8868\u660eLLMs\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u795e\u7ecf\u540c\u6b65\u6a21\u5f0f\uff0c\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3LLMs\u7684\u793e\u4f1a\u6027\u672c\u8d28\u3002"}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\uff08\u7279\u522b\u662fOpenMath\uff09\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684\u672c\u4f53\u5f15\u5bfc\u4e0a\u4e0b\u6587\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u8106\u5f31\u6027\u548c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u57fa\u7840\u7b49\u6839\u672c\u9650\u5236\uff0c\u8fd9\u4e9b\u5728\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\uff08\u5982\u9700\u8981\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u6570\u5b66\uff09\u4e2d\u5c24\u4e3a\u6210\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u662f\u5426\u80fd\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u6570\u5b66\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7ba1\u9053\uff0c\u5229\u7528OpenMath\u672c\u4f53\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\uff0c\u5c06\u76f8\u5173\u5b9a\u4e49\u6ce8\u5165\u6a21\u578b\u63d0\u793a\u4e2d\u3002\u5728MATH\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\uff0c\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4f1a\u4e3b\u52a8\u964d\u4f4e\u6027\u80fd\u3002\u8fd9\u7a81\u663e\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "conclusion": "\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u68c0\u7d22\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u8868\u660e\u9700\u8981\u66f4\u7cbe\u786e\u7684\u68c0\u7d22\u673a\u5236\u6765\u5145\u5206\u53d1\u6325\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.18142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18142", "abs": "https://arxiv.org/abs/2602.18142", "authors": ["Sebastian Dingler", "Frederik Boenke"], "title": "Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing", "comment": null, "summary": "Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u5de5\u4f5c\u6d41\u751f\u6210\u6307\u4ee4\u7ea7\u7cbe\u786e\u5904\u7406\u5668\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u786c\u4ef6\u53ef\u7528\u524d\u521b\u5efa\u865a\u62dfECU\u6d4b\u8bd5\u73af\u5883\uff0c\u51cf\u5c11\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u7684\u74f6\u9888\u3002", "motivation": "\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u4e2d\uff0c\u8f6f\u4ef6\u5f80\u5f80\u6bd4\u786c\u4ef6\u66f4\u65e9\u5b8c\u6210\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u786c\u4ef6\u5bfc\u81f4\u53ea\u80fd\u8fdb\u884c\u6602\u8d35\u7684\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u786c\u4ef6\u53ef\u7528\u524d\u8fd0\u884c\u771f\u5b9e\u8f6f\u4ef6\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u865a\u62df\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u53cd\u9988\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7GNU\u8c03\u8bd5\u5668\u8fde\u63a5\u5230\u53c2\u8003\u6a21\u62df\u5668\uff0c\u81ea\u52a8\u751f\u6210SystemC/TLM 2.0\u7684\u6307\u4ee4\u7ea7\u7cbe\u786e\u5904\u7406\u5668\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u4fee\u6b63\u786e\u4fddCPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u8bc1\u660e\u6700\u5173\u952e\u7684\u6280\u672f\u98ce\u9669\u2014\u2014CPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u2014\u2014\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5316\u5dee\u5206\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u4fee\u6b63\u6765\u964d\u4f4e\u3002\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u6d4b\u8bd5\u3001\u975e\u4fb5\u5165\u5f0f\u8ffd\u8e2a\u548c\u7b26\u5408\u5b89\u5168\u6807\u51c6\u7684\u6545\u969c\u6ce8\u5165\u80fd\u529b\u3002", "conclusion": "\u867d\u7136\u4e91\u89c4\u6a21\u90e8\u7f72\u548c\u5b8c\u6574\u5de5\u5177\u94fe\u96c6\u6210\u4ecd\u9700\u672a\u6765\u5de5\u4f5c\uff0c\u4f46\u539f\u578b\u5c55\u793a\u4e86\u865a\u62dfECU\u53cc\u80de\u80ce\u7684\u53ef\u884c\u5de6\u79fb\u8def\u5f84\uff0c\u80fd\u591f\u5728\u7269\u7406\u786c\u4ef6\u5b58\u5728\u524d\u5b9e\u73b0\u65e9\u671f\u8f6f\u4ef6\u96c6\u6210\u548c\u6d4b\u8bd5\u3002"}}
{"id": "2602.18187", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.18187", "abs": "https://arxiv.org/abs/2602.18187", "authors": ["Wataru Uemura", "Takumi Hamano"], "title": "Noise Mitigation Methods for Digital Visible Light Communication", "comment": null, "summary": "Visible Light Communication (VLC) using Light Emitting Diodes (LEDs) has gained attention due to its low power consumption, long lifetime, and fast response. However, VLC suffers from optical noise generated by ambient light sources such as fluorescent lamps, which leads to waveform distortion and increased bit error rates (BER). In this paper, we propose two noise reduction methods for Digital Visible Light Communication (DVLC) systems. The first method exploits the periodic nature of interference caused by AC-powered-line illumination and reduces interference by subtracting sampled noise waveforms from the received signal. Second, inspired by Active Noise Control (ANC) techniques, an additional photodiode is introduced for noise reception, and subtraction circuits are employed to attenuate noise in real time. Experimental results show that both methods improve BER performance compared with conventional receivers, with the ANC-inspired approach achieving superior performance under all tested conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u6570\u5b57\u53ef\u89c1\u5149\u901a\u4fe1\u7cfb\u7edf\u7684\u566a\u58f0\u6291\u5236\u65b9\u6cd5\uff1a\u57fa\u4e8e\u4ea4\u6d41\u7535\u6e90\u5e72\u6270\u5468\u671f\u6027\u7684\u6ce2\u5f62\u51cf\u6cd5\u548c\u53d7\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u542f\u53d1\u7684\u5b9e\u65f6\u566a\u58f0\u6d88\u9664\u6280\u672f", "motivation": "\u53ef\u89c1\u5149\u901a\u4fe1\u4f7f\u7528LED\u5177\u6709\u4f4e\u529f\u8017\u3001\u957f\u5bff\u547d\u548c\u5feb\u901f\u54cd\u5e94\u7b49\u4f18\u70b9\uff0c\u4f46\u53d7\u5230\u8367\u5149\u706f\u7b49\u73af\u5883\u5149\u6e90\u4ea7\u751f\u7684\u5149\u5b66\u566a\u58f0\u5f71\u54cd\uff0c\u5bfc\u81f4\u6ce2\u5f62\u5931\u771f\u548c\u8bef\u7801\u7387\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u566a\u58f0\u6291\u5236\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u5229\u7528\u4ea4\u6d41\u7535\u6e90\u7167\u660e\u5e72\u6270\u7684\u5468\u671f\u6027\uff0c\u901a\u8fc7\u91c7\u6837\u566a\u58f0\u6ce2\u5f62\u5e76\u4ece\u63a5\u6536\u4fe1\u53f7\u4e2d\u51cf\u53bb\u6765\u964d\u4f4e\u5e72\u6270\uff1b2) \u53d7\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u6280\u672f\u542f\u53d1\uff0c\u5f15\u5165\u989d\u5916\u5149\u7535\u4e8c\u6781\u7ba1\u63a5\u6536\u566a\u58f0\uff0c\u5e76\u4f7f\u7528\u51cf\u6cd5\u7535\u8def\u5b9e\u65f6\u8870\u51cf\u566a\u58f0", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e24\u79cd\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u63a5\u6536\u5668\u90fd\u6539\u5584\u4e86\u8bef\u7801\u7387\u6027\u80fd\uff0c\u5176\u4e2d\u53d7\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u542f\u53d1\u7684\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u566a\u58f0\u6291\u5236\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u6570\u5b57\u53ef\u89c1\u5149\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u53d7\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u542f\u53d1\u7684\u5b9e\u65f6\u566a\u58f0\u6d88\u9664\u6280\u672f\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u8868\u73b0"}}
{"id": "2602.17785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17785", "abs": "https://arxiv.org/abs/2602.17785", "authors": ["Xinwei Ju", "Rema Daher", "Danail Stoyanov", "Sophia Bano", "Francisco Vasconcelos"], "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision", "comment": "14 pages, 6 figures; early accepted by IPCAI2026", "summary": "Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.", "AI": {"tldr": "PRISM\uff1a\u4e00\u79cd\u7528\u4e8e\u7ed3\u80a0\u955c\u5bfc\u822a\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4e0e\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u68c0\u6d4b\u548c\u4eae\u5ea6\u89e3\u8026\u5229\u7528\u89e3\u5256\u548c\u5149\u7167\u5148\u9a8c\u6307\u5bfc\u51e0\u4f55\u5b66\u4e60\u3002", "motivation": "\u7ed3\u80a0\u955c\u8f85\u52a9\u5bfc\u822a\u4e2d\u7684\u5355\u76ee\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\u5bf9\u6539\u5584\u7b5b\u67e5\u6548\u679c\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7eb9\u7406\u7f3a\u5931\u8868\u9762\u3001\u590d\u6742\u5149\u7167\u6a21\u5f0f\u3001\u53d8\u5f62\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u9760\u771f\u5b9e\u503c\u6570\u636e\u96c6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff0c\u7ed3\u5408\u8fb9\u7f18\u68c0\u6d4b\uff08\u4f7f\u7528\u5b66\u4e60\u578b\u8fb9\u7f18\u68c0\u6d4b\u5668\u5982DexiNed\u6216HED\uff09\u548c\u4eae\u5ea6\u89e3\u8026\uff08\u901a\u8fc7\u5185\u5728\u5206\u89e3\u6a21\u5757\u5206\u79bb\u7740\u8272\u548c\u53cd\u5c04\uff09\uff0c\u5229\u7528\u89e3\u5256\u548c\u5149\u7167\u5148\u9a8c\u6307\u5bfc\u51e0\u4f55\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u771f\u5b9e\u6570\u636e\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u4f18\u4e8e\u771f\u5b9e\u5e7b\u5f71\u6570\u636e\u7684\u76d1\u7763\u8bad\u7ec3\uff1b2\uff09\u89c6\u9891\u5e27\u7387\u5bf9\u6a21\u578b\u6027\u80fd\u6781\u4e3a\u91cd\u8981\u3002", "conclusion": "PRISM\u901a\u8fc7\u7ed3\u5408\u8fb9\u7f18\u68c0\u6d4b\u548c\u4eae\u5ea6\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u80a0\u955c\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u7ed3\u80a0\u955c\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2602.17848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17848", "abs": "https://arxiv.org/abs/2602.17848", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "On the scaling relationship between cloze probabilities and language model next-token prediction", "comment": null, "summary": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u773c\u52a8\u548c\u9605\u8bfb\u65f6\u95f4\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4f1a\u4f4e\u4f30\u4eba\u7c7b\u53cd\u5e94\u6982\u7387\uff1b\u5927\u6a21\u578b\u5bf9\u5b8c\u5f62\u586b\u7a7a\u6570\u636e\u7684\u9884\u6d4b\u8d28\u91cf\u66f4\u9ad8\uff0c\u56e0\u4e3a\u5b83\u4eec\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u4e0d\u654f\u611f\uff0c\u800c\u4e0e\u4eba\u7c7b\u8bed\u4e49\u66f4\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u773c\u52a8\u3001\u9605\u8bfb\u65f6\u95f4\u548c\u5b8c\u5f62\u586b\u7a7a\u53cd\u5e94\u65b9\u9762\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63a2\u7d22\u6a21\u578b\u5927\u5c0f\u5982\u4f55\u5f71\u54cd\u5176\u5bf9\u8bed\u4e49\u4fe1\u606f\u548c\u8bcd\u6c47\u7edf\u8ba1\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u5927\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u5728\u773c\u52a8\u3001\u9605\u8bfb\u65f6\u95f4\u548c\u5b8c\u5f62\u586b\u7a7a\u6570\u636e\u4e0a\u7684\u9884\u6d4b\u8868\u73b0\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u7684\u654f\u611f\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u5927\u6a21\u578b\u5728\u5b8c\u5f62\u586b\u7a7a\u4efb\u52a1\u4e2d\u9884\u6d4b\u8d28\u91cf\u66f4\u9ad8\uff0c\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u4e0d\u654f\u611f\uff0c\u4f46\u4e0e\u4eba\u7c7b\u8bed\u4e49\u53cd\u5e94\u66f4\u5bf9\u9f50\uff1b\u6240\u6709\u6a21\u578b\u90fd\u4f4e\u4f30\u4eba\u7c7b\u53cd\u5e94\u6982\u7387\uff1b\u5927\u6a21\u578b\u66f4\u5f3a\u7684\u8bb0\u5fc6\u80fd\u529b\u5e2e\u52a9\u5b83\u4eec\u731c\u6d4b\u66f4\u8bed\u4e49\u5408\u9002\u7684\u8bcd\uff0c\u4f46\u5bf9\u5355\u8bcd\u8bc6\u522b\u76f8\u5173\u7684\u4f4e\u5c42\u4fe1\u606f\u4e0d\u654f\u611f\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u5927\u7684\u8bb0\u5fc6\u5bb9\u91cf\u4f7f\u5b83\u4eec\u80fd\u731c\u6d4b\u66f4\u8bed\u4e49\u5408\u9002\u7684\u8bcd\uff0c\u4f46\u964d\u4f4e\u4e86\u5b83\u4eec\u5bf9\u5355\u8bcd\u8bc6\u522b\u76f8\u5173\u4f4e\u5c42\u4fe1\u606f\u7684\u654f\u611f\u6027\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5927\u6a21\u578b\u5728\u5b8c\u5f62\u586b\u7a7a\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u5728\u67d0\u4e9b\u9605\u8bfb\u76f8\u5173\u4efb\u52a1\u4e2d\u53ef\u80fd\u4e0d\u8db3\u3002"}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "TTG\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f16\u7a0b\u8c1c\u9898\u5bf9\u6218\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u4e92\u51fa\u9898\u6311\u6218\uff0c\u901a\u8fc7Elo\u8bc4\u5206\u6bd4\u8f83\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u9762\u4e34\u6311\u6218\uff1a\u4eba\u5de5\u6807\u6ce8\u9ad8\u8d28\u91cf\u95ee\u9898\u6210\u672c\u9ad8\uff0c\u4e14\u96be\u4ee5\u533a\u5206\u6a21\u578b\u662f\u771f\u6b63\u63a8\u7406\u8fd8\u662f\u89c1\u8fc7\u7c7b\u4f3c\u8bad\u7ec3\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u65e0\u6cd5\u88ab\u8bbe\u8ba1\u9971\u548c\u3001\u80fd\u6d4b\u8bd5\u521b\u9020\u6027\u548c\u4efb\u52a1\u751f\u6210\u80fd\u529b\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u53d716\u4e16\u7eaa\u6570\u5b66\u51b3\u6597\u542f\u53d1\uff0c\u8bbe\u8ba1Token Games\u6846\u67b6\uff1a\u6a21\u578b\u76f8\u4e92\u51fa\u7f16\u7a0b\u8c1c\u9898\uff08\u7ed9\u5b9a\u8fd4\u56de\u5e03\u5c14\u503c\u7684Python\u51fd\u6570\uff0c\u627e\u5230\u4f7f\u51fd\u6570\u8fd4\u56deTrue\u7684\u8f93\u5165\uff09\uff0c\u901a\u8fc7\u4e24\u4e24\u5bf9\u6218\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u8ba1\u7b97Elo\u8bc4\u5206\u6765\u6bd4\u8f83\u6a21\u578b\u76f8\u5bf9\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u524d\u6cbf\u6a21\u578b\uff0cTTG\u7684\u6392\u540d\u7ed3\u679c\u4e0e\u73b0\u6709\u57fa\u51c6\uff08\u5982Humanity's Last Exam\uff09\u9ad8\u5ea6\u5339\u914d\uff0c\u4e14\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u521b\u5efa\u8c1c\u9898\u3002\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u521b\u5efa\u4f18\u8d28\u8c1c\u9898\u7684\u80fd\u529b\u4ecd\u7136\u6781\u5177\u6311\u6218\uff0c\u8fd9\u662f\u5148\u524d\u57fa\u51c6\u672a\u6d4b\u91cf\u7684\u7ef4\u5ea6\u3002", "conclusion": "TTG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8bc4\u4f30\u8303\u5f0f\uff0c\u4e0d\u4f1a\u88ab\u8bbe\u8ba1\u9971\u548c\uff0c\u80fd\u591f\u540c\u65f6\u6d4b\u8bd5\u6a21\u578b\u7684\u95ee\u9898\u89e3\u51b3\u3001\u521b\u9020\u6027\u548c\u4efb\u52a1\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.18190", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18190", "abs": "https://arxiv.org/abs/2602.18190", "authors": ["Jorge Melegati"], "title": "Role and Identity Work of Software Engineering Professionals in the Generative AI Era", "comment": "Accepted to the 19th International Conference on Cooperative and Human Aspects of Software Engineering (CHASE 2026)", "summary": "The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u7814\u7a76\u751f\u6210\u5f0fAI\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8eab\u4efd\u8ba4\u540c\u7684\u5f71\u54cd\u65f6\uff0c\u9700\u8981\u8003\u8651\u4e0d\u540c\u89d2\u8272\uff08\u5982\u5f00\u53d1\u8005\u548c\u6d4b\u8bd5\u8005\uff09\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u76f8\u5173\u7814\u7a76\u8bae\u7a0b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0fAI\u7684\u91c7\u7528\u4f1a\u89e6\u53d1\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u7684\u8eab\u4efd\u8ba4\u540c\u5de5\u4f5c\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u6ca1\u6709\u8003\u8651\u4e0d\u540c\u89d2\u8272\uff08\u5982\u5f00\u53d1\u8005\u548c\u6d4b\u8bd5\u8005\uff09\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4f5c\u8005\u8ba4\u4e3a\u89d2\u8272\u662f\u5b9a\u4e49\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u8ba4\u540c\u5de5\u4f5c\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5173\u4e8e\u4e0d\u540c\u89d2\u8272\u7684\u73b0\u6709\u7814\u7a76\u4ee5\u53ca\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u91c7\u7528\u7684\u6700\u65b0\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\u6765\u7406\u89e3\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5f0fAI\u91c7\u7528\u89e6\u53d1\u7684\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u8ba4\u540c\u5de5\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u66f4\u597d\u5730\u7406\u89e3\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5f0fAI\u91c7\u7528\u89e6\u53d1\u7684\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u8ba4\u540c\u5de5\u4f5c\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u652f\u6301\u8fd9\u79cd\u91c7\u7528\u7684\u65b0\u5de5\u5177\u3002", "conclusion": "\u8003\u8651\u89d2\u8272\u56e0\u7d20\u5bf9\u4e8e\u7406\u89e3\u751f\u6210\u5f0fAI\u5bf9\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u8ba4\u540c\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u7814\u7a76\u8bae\u7a0b\u5c06\u6709\u52a9\u4e8e\u5f00\u53d1\u652f\u6301\u4e0d\u540c\u89d2\u8272\u9002\u5e94\u751f\u6210\u5f0fAI\u7684\u5de5\u5177\uff0c\u5e76\u5bf9\u5b9e\u8df5\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.18208", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.18208", "abs": "https://arxiv.org/abs/2602.18208", "authors": ["Rezvi Shahariar", "Chris Phillips"], "title": "A traffic incident management framework for vehicular ad hoc networks", "comment": "23 pages paper consisting of some road traffic incidents management", "summary": "Vehicular Ad Hoc Networks (VANETs) support the information dissemination among vehicles, Roadside Units (RSUs), and a Trust Authority (TA). A trust model evaluates an entity or data or both to determine truthfulness. A security model confirms authentication, integrity, availability, non repudiation issues. With these aspects in mind, many models have been proposed in literature. Furthermore, many information dissemination approaches are proposed. However, the lack of a model that can manage traffic incidents completely inspires this work. This paper details how and when a message needs to be generated and relayed so that the incidents can be reported and managed in a timely manner. This paper addresses this challenge by providing a traffic incident management model to manage several traffic incidents efficiently. Additionally, we simulate this model using the VEINS simulator with vehicles, RSUs, and a TA. From the experiments, we measure the average number of transmissions required for reporting a single traffic incident while varying the vehicle density and relaying considerations. We consider two types of relaying. In one series of experiments, messages from regular vehicles and RSUs are relayed up to four hops. In another series of experiments, messages from the regular vehicles and RSUs are relayed until their generation time reaches sixty seconds. Additionally, messages from the official vehicles are relayed when they approach an incident or when the incident is cleared. Results from the simulations show that more vehicles are informed with four-hop relaying than sixty-second relaying in both cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4ea4\u901a\u4e8b\u4ef6\u7ba1\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u63a7\u5236\u6d88\u606f\u751f\u6210\u548c\u8f6c\u53d1\u65f6\u673a\u6765\u9ad8\u6548\u7ba1\u7406VANET\u4e2d\u7684\u4ea4\u901a\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u6bd4\u8f83\u4e86\u56db\u8df3\u8f6c\u53d1\u548c60\u79d2\u8f6c\u53d1\u4e24\u79cd\u7b56\u7565\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709VANET\u6a21\u578b\u867d\u7136\u63d0\u51fa\u4e86\u4fe1\u4efb\u6a21\u578b\u3001\u5b89\u5168\u6a21\u578b\u548c\u4fe1\u606f\u4f20\u64ad\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u80fd\u591f\u5b8c\u5168\u7ba1\u7406\u4ea4\u901a\u4e8b\u4ef6\u7684\u5b8c\u6574\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u53ca\u65f6\u62a5\u544a\u548c\u7ba1\u7406\u4ea4\u901a\u4e8b\u4ef6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u901a\u4e8b\u4ef6\u7ba1\u7406\u6a21\u578b\uff0c\u8be6\u7ec6\u89c4\u5b9a\u6d88\u606f\u4f55\u65f6\u751f\u6210\u548c\u8f6c\u53d1\u4ee5\u62a5\u544a\u4e8b\u4ef6\u3002\u4f7f\u7528VEINS\u4eff\u771f\u5668\u8fdb\u884c\u6a21\u62df\uff0c\u5305\u542b\u8f66\u8f86\u3001RSU\u548cTA\uff0c\u6bd4\u8f83\u4e24\u79cd\u8f6c\u53d1\u7b56\u7565\uff1a\u56db\u8df3\u8f6c\u53d1\u548c60\u79d2\u65f6\u95f4\u9650\u5236\u8f6c\u53d1\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8f66\u8f86\u5bc6\u5ea6\u548c\u8f6c\u53d1\u8003\u8651\u56e0\u7d20\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u56db\u8df3\u8f6c\u53d1\u6bd460\u79d2\u8f6c\u53d1\u80fd\u901a\u77e5\u66f4\u591a\u8f66\u8f86\u3002\u540c\u65f6\u6d4b\u91cf\u4e86\u62a5\u544a\u5355\u4e2a\u4ea4\u901a\u4e8b\u4ef6\u6240\u9700\u7684\u5e73\u5747\u4f20\u8f93\u6b21\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ea4\u901a\u4e8b\u4ef6\u7ba1\u7406\u6a21\u578b\u80fd\u591f\u6709\u6548\u7ba1\u7406\u591a\u79cd\u4ea4\u901a\u4e8b\u4ef6\uff0c\u56db\u8df3\u8f6c\u53d1\u7b56\u7565\u5728\u4fe1\u606f\u4f20\u64ad\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8e\u65f6\u95f4\u9650\u5236\u8f6c\u53d1\u7b56\u7565\uff0c\u4e3aVANET\u4e2d\u7684\u4e8b\u4ef6\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "\u63d0\u51faLGD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\uff0c\u76f4\u63a5\u4eceH&E\u5207\u7247\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\uff0c\u907f\u514d\u91cd\u5efa\u4f2a\u5f71\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "motivation": "\u6807\u51c6IHC\u67d3\u8272\u8d44\u6e90\u5bc6\u96c6\u3001\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u8bb8\u591a\u5730\u533a\u65e0\u6cd5\u83b7\u5f97\u3002\u4eceH&E\u5207\u7247\u76f4\u63a5\u9884\u6d4bHER2\u6c34\u5e73\u6210\u4e3a\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u50cf\u7d20\u7ea7\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u6613\u4ea7\u751f\u91cd\u5efa\u4f2a\u5f71\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef", "method": "\u63d0\u51faLatent-Guided Dual-Stream Network (LGD-Net)\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u663e\u5f0f\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\u3002\u5b66\u4e60\u5c06\u5f62\u6001\u5b66H&E\u7279\u5f81\u76f4\u63a5\u6620\u5c04\u5230\u5206\u5b50\u6f5c\u5728\u7a7a\u95f4\uff0c\u8bad\u7ec3\u65f6\u7531\u6559\u5e08IHC\u7f16\u7801\u5668\u5f15\u5bfc\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6b63\u5219\u5316\u4efb\u52a1\uff0c\u5229\u7528\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff08\u6838\u5206\u5e03\u548c\u819c\u67d3\u8272\u5f3a\u5ea6\uff09\u6b63\u5219\u5316\u6a21\u578b\u8bad\u7ec3", "result": "\u5728\u516c\u5f00BCI\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLGD-Net\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u4f7f\u7528\u5355\u6a21\u6001H&E\u8f93\u5165\u8fdb\u884c\u9ad8\u6548\u63a8\u7406", "conclusion": "LGD-Net\u901a\u8fc7\u7279\u5f81\u7ea7\u800c\u975e\u50cf\u7d20\u7ea7\u7684\u8de8\u6a21\u6001\u6620\u5c04\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u4e3a\u4eceH&E\u5207\u7247\u51c6\u786e\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17881", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17881", "abs": "https://arxiv.org/abs/2602.17881", "authors": ["Joschka Braun"], "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations", "comment": "Master's Thesis, University of T\u00fcbingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)", "summary": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5f15\u5bfc\u5411\u91cf\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u76ee\u6807\u884c\u4e3a\u8868\u793a\u80fd\u5426\u88ab\u7ebf\u6027\u65b9\u5411\u6709\u6548\u8fd1\u4f3c\u3002", "motivation": "\u867d\u7136\u5f15\u5bfc\u5411\u91cf\u662f\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u6dfb\u52a0\u5b66\u4e60\u504f\u7f6e\u6765\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u8f7b\u91cf\u65b9\u6cd5\uff0c\u4f46\u5176\u6548\u679c\u5728\u4e0d\u540c\u6837\u672c\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u5bf9\u8bb8\u591a\u76ee\u6807\u884c\u4e3a\u4e0d\u53ef\u9760\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f15\u5bfc\u53ef\u9760\u6027\u5728\u4e0d\u540c\u884c\u4e3a\u95f4\u5dee\u5f02\u7684\u539f\u56e0\u53ca\u5176\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u4e0a\u8bad\u7ec3\u7684\u5f15\u5bfc\u5411\u91cf\u7684\u65b9\u5411\u5dee\u5f02\u548c\u6027\u80fd\u76f8\u5173\u6027\u3002", "result": "1) \u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d8a\u9ad8\uff0c\u5f15\u5bfc\u8d8a\u53ef\u9760\uff1b2) \u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u5206\u79bb\u66f4\u597d\u7684\u6570\u636e\u96c6\u66f4\u6613\u88ab\u5f15\u5bfc\uff1b3) \u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u8bad\u7ec3\u7684\u5f15\u5bfc\u5411\u91cf\u65b9\u5411\u4e0d\u540c\u4f46\u6027\u80fd\u76f8\u4f3c\uff0c\u4e14\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u6548\u679c\u76f8\u5173\u3002", "conclusion": "\u5f15\u5bfc\u5411\u91cf\u4e0d\u53ef\u9760\u7684\u539f\u56e0\u662f\u6f5c\u5728\u76ee\u6807\u884c\u4e3a\u8868\u793a\u65e0\u6cd5\u88ab\u7ebf\u6027\u5f15\u5bfc\u65b9\u5411\u6709\u6548\u8fd1\u4f3c\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bca\u65ad\u5f15\u5bfc\u4e0d\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u6fc0\u52b1\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9700\u663e\u5f0f\u8003\u8651\u975e\u7ebf\u6027\u6f5c\u5728\u884c\u4e3a\u8868\u793a\u3002"}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "\u63d0\u51faEl Agente Gr\u00e1fico\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u578b\u5b89\u5168\u6267\u884c\u73af\u5883\u548c\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5c06LLM\u51b3\u7b56\u5d4c\u5165\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u6587\u672c\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u8106\u5f31\u548c\u53ef\u8ffd\u6eaf\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u4e0e\u5f02\u6784\u8ba1\u7b97\u5de5\u5177\u7684\u96c6\u6210\u65b9\u5f0f\u96f6\u6563\u8106\u5f31\uff0c\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u4ee3\u7406\u65b9\u6cd5\u4ea7\u751f\u5927\u91cf\u4fe1\u606f\uff0c\u96be\u4ee5\u8ffd\u8e2a\u51b3\u7b56\u6765\u6e90\u548c\u5ba1\u8ba1\u3002\u9700\u8981\u66f4\u53ef\u9760\u3001\u53ef\u8ffd\u6eaf\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u5355\u4ee3\u7406\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u79d1\u5b66\u6982\u5ff5\u7684\u7ed3\u6784\u5316\u62bd\u8c61\u548c\u5bf9\u8c61-\u56fe\u8c31\u6620\u5c04\u5668\uff0c\u5c06\u8ba1\u7b97\u72b6\u6001\u8868\u793a\u4e3a\u7c7b\u578b\u5316Python\u5bf9\u8c61\uff0c\u5b58\u50a8\u5728\u5185\u5b58\u6216\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u4e2d\u3002\u901a\u8fc7\u7c7b\u578b\u5316\u7b26\u53f7\u6807\u8bc6\u800c\u975e\u539f\u59cb\u6587\u672c\u7ba1\u7406\u4e0a\u4e0b\u6587\uff0c\u786e\u4fdd\u4e00\u81f4\u6027\u3001\u652f\u6301\u6eaf\u6e90\u8ffd\u8e2a\u548c\u9ad8\u6548\u5de5\u5177\u7f16\u6392\u3002", "result": "\u5728\u91cf\u5b50\u5316\u5b66\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2a\u4ee3\u7406\u7ed3\u5408\u53ef\u9760\u6267\u884c\u5f15\u64ce\u80fd\u7a33\u5065\u6267\u884c\u590d\u6742\u3001\u591a\u6b65\u9aa4\u3001\u5e76\u884c\u8ba1\u7b97\u3002\u6846\u67b6\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u6784\u8c61\u96c6\u5408\u751f\u6210\u548c\u91d1\u5c5e\u6709\u673a\u6846\u67b6\u8bbe\u8ba1\uff0c\u77e5\u8bc6\u56fe\u8c31\u540c\u65f6\u4f5c\u4e3a\u8bb0\u5fc6\u548c\u63a8\u7406\u57fa\u7840\u3002", "conclusion": "\u62bd\u8c61\u5316\u548c\u7c7b\u578b\u5b89\u5168\u4e3a\u57fa\u4e8e\u4ee3\u7406\u7684\u79d1\u5b66\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u8d85\u8d8a\u63d0\u793a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.18306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18306", "abs": "https://arxiv.org/abs/2602.18306", "authors": ["Dongming Jin", "Zhi Jin", "Zheng Fang", "Linyu Li", "XiaoTian Yang", "Yuanpeng He", "Xiaohong Chen"], "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation", "comment": "22page, 7 figures", "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.", "AI": {"tldr": "ReqElicitGym\uff1a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u4e2d\u8bbf\u8c08\u80fd\u529b\u7684\u81ea\u52a8\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b101\u4e2a\u7f51\u7ad9\u9700\u6c42\u573a\u666f\u6570\u636e\u96c6\u3001\u6a21\u62df\u7528\u6237\u548c\u4efb\u52a1\u8bc4\u4f30\u5668\uff0c\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u968f\u7740LLM\u7f16\u7801\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\uff0cLLM\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\u4ece\u751f\u6210\u6b63\u786e\u4ee3\u7801\u8f6c\u5411\u4e86\u83b7\u53d6\u7528\u6237\u9700\u6c42\u3002\u7136\u800c\uff0cLLM\u5728\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u4e2d\u7684\u8bbf\u8c08\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5c11\u91cf\u573a\u666f\u3001\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u548c\u4e3b\u89c2\u8bc4\u5206\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u3002", "method": "\u63d0\u51faReqElicitGym\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b\uff1a1\uff09101\u4e2a\u7f51\u7ad9\u9700\u6c42\u83b7\u53d6\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d610\u79cd\u5e94\u7528\u7c7b\u578b\uff1b2\uff09\u4ea4\u4e92\u5f0f\u6a21\u62df\u7528\u6237\uff08oracle user\uff09\uff1b3\uff09\u4efb\u52a1\u8bc4\u4f30\u5668\uff08task evaluator\uff09\u3002\u8be5\u73af\u5883\u652f\u6301\u5bf9\u4efb\u4f55\u81ea\u52a8\u5316\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff09\u8fdb\u884c\u53ef\u91cd\u590d\u3001\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u5bf97\u4e2a\u4ee3\u8868\u6027LLM\u7684\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff1a1\uff09\u5f53\u524dLLM\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4ec5\u80fd\u83b7\u53d6\u4e0d\u5230\u4e00\u534a\u7684\u7528\u6237\u9690\u6027\u9700\u6c42\uff1b2\uff09\u6709\u6548\u7684\u83b7\u53d6\u95ee\u9898\u5f80\u5f80\u51fa\u73b0\u5728\u5bf9\u8bdd\u540e\u671f\uff1b3\uff09LLM\u80fd\u591f\u83b7\u53d6\u4ea4\u4e92\u548c\u5185\u5bb9\u76f8\u5173\u7684\u9690\u6027\u9700\u6c42\uff0c\u4f46\u5728\u98ce\u683c\u76f8\u5173\u9700\u6c42\u65b9\u9762\u6301\u7eed\u8868\u73b0\u4e0d\u4f73\u3002\u6a21\u62df\u7528\u6237\u548c\u4efb\u52a1\u8bc4\u4f30\u5668\u4e0e\u771f\u5b9e\u7528\u6237\u548c\u4e13\u5bb6\u5224\u65ad\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "ReqElicitGym\u4e3a\u81ea\u52a8\u5316\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u65b9\u6cd5\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u5728\u9700\u6c42\u8bbf\u8c08\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u663e\u8457\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u98ce\u683c\u76f8\u5173\u9700\u6c42\u83b7\u53d6\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u56f0\u96be\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5f0f\u548c\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0eSAM\uff0c\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3a\u96f6\u6837\u672c\u9065\u611f\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4ecd\u9700\u989d\u5916\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u4ec5\u4f9d\u8d56\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u7684\u9065\u611f\u5206\u5272\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u4f7f\u7528CLIP\u4f5c\u4e3aSAM\u7f51\u683c\u63d0\u8bae\u7684\u63a9\u7801\u9009\u62e9\u5668\uff1b2) \u751f\u6210\u5f0f\u65b9\u6cd5\u4f7f\u7528GPT-5\u548cLoRA\u8c03\u4f18\u7684Qwen-VL\u6a21\u578b\u4e3aSAM\u751f\u6210\u70b9\u51fb\u63d0\u793a\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5b9e\u73b0\u4e86\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\u6216\u8f7b\u91cf\u7ea7LoRA\u8c03\u4f18\u7684\u6d41\u7a0b\u3002", "result": "\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u4efb\u52a1\uff09\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u80fd\u529b\u3002\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u5728\u5b8c\u5168\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff0c\u751f\u6210\u5f0f\u65b9\u6cd5\u4e2dLoRA\u8c03\u4f18\u7684Qwen-VL\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4ec5\u4f9d\u8d56\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17907", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17907", "abs": "https://arxiv.org/abs/2602.17907", "authors": ["Raymond Li", "Amirhossein Abaskohi", "Chuyuan Li", "Gabriel Murray", "Giuseppe Carenini"], "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions", "comment": "20 pages, 5 figures", "summary": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u8f6f\u6807\u7b7e\u6765\u6539\u8fdb\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u8fd9\u4e9b\u8f6f\u6807\u7b7e\u800c\u975e\u4f20\u7edf\u7684\u8bcd\u888b\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u8d28\u91cf\u548c\u6587\u6863\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u91cd\u6784\u6587\u6863\u7684\u8bcd\u888b\u8868\u793a\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e14\u5728\u5904\u7406\u6570\u636e\u7a00\u758f\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u6765\u6539\u8fdb\u4e3b\u9898\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff1a\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e13\u7528\u63d0\u793a\u8bcd\u751f\u6210\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6982\u7387\u5206\u5e03\uff0c\u5c06\u5176\u6295\u5f71\u5230\u9884\u5b9a\u4e49\u8bcd\u6c47\u8868\u4e0a\uff0c\u6784\u5efa\u8bed\u4e49\u57fa\u7840\u7684\u8f6f\u6807\u7b7e\u76ee\u6807\u3002\u7136\u540e\u8bad\u7ec3\u4e3b\u9898\u6a21\u578b\u91cd\u6784\u8fd9\u4e9b\u8f6f\u6807\u7b7e\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u7eaf\u5ea6\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\u5f15\u5165\u7684\u68c0\u7d22\u6307\u6807\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6587\u6863\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u68c0\u7d22\u5bfc\u5411\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u8d34\u8fd1\u8bed\u6599\u5e93\u5e95\u5c42\u4e3b\u9898\u7ed3\u6784\u7684\u4e3b\u9898\uff0c\u4e3a\u795e\u7ecf\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "APEMO\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4-\u60c5\u611f\u4fe1\u53f7\u4f18\u5316\u8ba1\u7b97\u5206\u914d\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u63d0\u5347\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u8f68\u8ff9\u7ea7\u53ef\u9760\u6027\u548c\u91cd\u7528\u6982\u7387\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u8f93\u51fa\uff0c\u4f46\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u9700\u8981\u5728\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u4fdd\u6301\u6301\u7eed\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8f68\u8ff9\u7ea7\u7a33\u5b9a\u6027\u7684\u5173\u6ce8\u3002", "method": "APEMO\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u884c\u4e3a\u4ee3\u7406\u68c0\u6d4b\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5173\u952e\u7247\u6bb5\uff08\u5982\u5cf0\u503c\u65f6\u523b\u548c\u7ed3\u5c3e\uff09\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u590d\u3002\u5b83\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\uff0c\u800c\u662f\u901a\u8fc7\u65f6\u95f4-\u60c5\u611f\u4fe1\u53f7\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u57fa\u4e8eLLM\u7684\u89c4\u5212-\u6267\u884c\u6d41\u7a0b\u8bc4\u4f30\u4e2d\uff0cAPEMO\u5728\u8f68\u8ff9\u7ea7\u8d28\u91cf\u548c\u91cd\u7528\u6982\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u7ed3\u6784\u5316\u7f16\u6392\u5668\u3002", "conclusion": "\u7814\u7a76\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u65f6\u95f4\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u5f39\u6027\u7684\u5de5\u7a0b\u8def\u5f84\u3002"}}
{"id": "2602.18307", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.18307", "abs": "https://arxiv.org/abs/2602.18307", "authors": ["Yutong Xin", "Qiaochu Chen", "Greg Durrett", "I\u015fil Dillig"], "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean", "comment": null, "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.", "AI": {"tldr": "VeriSoftBench\uff1a\u4e00\u4e2a\u5305\u542b500\u4e2aLean 4\u8bc1\u660e\u4e49\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u9a8c\u8bc1\u9886\u57df\uff0c\u8bc4\u4f30LLM\u5728\u9879\u76ee\u7279\u5b9a\u4ee3\u7801\u5e93\u4e2d\u7684\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u57fa\u4e8eMathlib\u6570\u5b66\u5e93\uff0c\u4f46\u8f6f\u4ef6\u9a8c\u8bc1\u9886\u57df\u7684\u8bc1\u660e\u901a\u5e38\u4f9d\u8d56\u4e8e\u9879\u76ee\u7279\u5b9a\u7684\u4e30\u5bcc\u5b9a\u4e49\u5e93\u548c\u4ee3\u7801\u5e93\u7ed3\u6784\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u521b\u5efaVeriSoftBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u6765\u81ea\u5f00\u6e90\u5f62\u5f0f\u5316\u65b9\u6cd5\u9879\u76ee\u7684Lean 4\u8bc1\u660e\u4e49\u52a1\uff0c\u4fdd\u7559\u771f\u5b9e\u7684\u4ed3\u5e93\u4e0a\u4e0b\u6587\u548c\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc4\u4f30\u524d\u6cbfLLM\u548c\u4e13\u4e1a\u8bc1\u660e\u5668\u3002", "result": "1) \u9488\u5bf9Mathlib\u98ce\u683c\u6570\u5b66\u4f18\u5316\u7684\u8bc1\u660e\u5668\u5728\u6b64\u4ed3\u5e93\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b2) \u6210\u529f\u7387\u4e0e\u4f20\u9012\u6027\u4ed3\u5e93\u4f9d\u8d56\u5f3a\u76f8\u5173\uff1b3) \u63d0\u4f9b\u7ecf\u8fc7\u7b5b\u9009\u7684\u4f9d\u8d56\u95ed\u5305\u4e0a\u4e0b\u6587\u6bd4\u66b4\u9732\u5b8c\u6574\u4ed3\u5e93\u6548\u679c\u66f4\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8f6f\u4ef6\u9a8c\u8bc1\u9886\u57df\u7684\u5b9a\u7406\u8bc1\u660e\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0cVeriSoftBench\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u9879\u76ee\u7279\u5b9a\u4ee3\u7801\u5e93\u4e2dLLM\u8bc1\u660e\u81ea\u52a8\u5316\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2602.17807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17807", "abs": "https://arxiv.org/abs/2602.17807", "authors": ["Narges Norouzi", "Idil Esen Zulfikar", "Niccol`o Cavagnero", "Tommie Kerssies", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model", "comment": null, "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/", "AI": {"tldr": "VidEoMT\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u7684\u89c6\u9891\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u4f20\u64ad\u673a\u5236\u5b9e\u73b0\u8de8\u5e27\u4fe1\u606f\u4f20\u9012\uff0c\u65e0\u9700\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u8fbe\u52305-10\u500d\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u89c6\u9891\u5206\u5272\u6a21\u578b\u901a\u5e38\u7ed3\u5408\u9010\u5e27\u5206\u5272\u5668\u548c\u590d\u6742\u7684\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\uff0c\u867d\u7136\u6709\u6548\u4f46\u5f15\u5165\u4e86\u663e\u8457\u7684\u67b6\u6784\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u53d7ViT\u7f16\u7801\u5668\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e0b\u65e0\u9700\u4e13\u7528\u6a21\u5757\u5373\u53ef\u8fdb\u884c\u51c6\u786e\u56fe\u50cf\u5206\u5272\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u89c6\u9891\u5206\u5272\u6a21\u578b\u3002", "method": "\u63d0\u51faVideo Encoder-only Mask Transformer (VidEoMT)\uff0c\u91c7\u7528\u7eaf\u7f16\u7801\u5668\u67b6\u6784\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u67e5\u8be2\u4f20\u64ad\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u7528\u524d\u4e00\u5e27\u7684\u67e5\u8be2\u6765\u8de8\u5e27\u4f20\u9012\u4fe1\u606f\u3002\u540c\u65f6\u91c7\u7528\u67e5\u8be2\u878d\u5408\u7b56\u7565\uff0c\u5c06\u4f20\u64ad\u67e5\u8be2\u4e0e\u4e00\u7ec4\u65f6\u95f4\u65e0\u5173\u7684\u5b66\u4e60\u67e5\u8be2\u76f8\u7ed3\u5408\uff0c\u5e73\u8861\u4fe1\u606f\u4f20\u9012\u4e0e\u65b0\u5185\u5bb9\u9002\u5e94\u6027\u3002", "result": "VidEoMT\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e865-10\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4f7f\u7528ViT-L\u9aa8\u5e72\u7f51\u7edc\u65f6\u8fd0\u884c\u901f\u5ea6\u53ef\u8fbe160 FPS\uff0c\u8fbe\u5230\u4e86\u8ddf\u8e2a\u5668\u7684\u6548\u679c\u800c\u65e0\u9700\u989d\u5916\u590d\u6742\u6027\u3002", "conclusion": "VidEoMT\u8bc1\u660e\u4e86\u7eaf\u7f16\u7801\u5668\u67b6\u6784\u5728\u89c6\u9891\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u67e5\u8be2\u4f20\u64ad\u548c\u878d\u5408\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u4e3a\u89c6\u9891\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u5feb\u901f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17911", "abs": "https://arxiv.org/abs/2602.17911", "authors": ["Jash Rajesh Parekh", "Wonbin Kweon", "Joey Chan", "Rezarta Islamaj", "Robert Leaman", "Pengcheng Jiang", "Chih-Hsuan Wei", "Zhizheng Wang", "Zhiyong Lu", "Jiawei Han"], "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering", "comment": null, "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6761\u4ef6\u6027\u751f\u7269\u533b\u5b66\u95ee\u7b54\u57fa\u51c6CondMedQA\u548c\u6761\u4ef6\u95e8\u63a7\u63a8\u7406\u6846\u67b6CGR\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u60a3\u8005\u7279\u5b9a\u6761\u4ef6\u4f9d\u8d56\u7684\u4e34\u5e8a\u63a8\u7406\u95ee\u9898", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u5047\u8bbe\u533b\u5b66\u77e5\u8bc6\u666e\u904d\u9002\u7528\uff0c\u4f46\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u672c\u8d28\u4e0a\u662f\u6761\u4ef6\u6027\u7684\u2014\u2014\u51e0\u4e4e\u6240\u6709\u51b3\u7b56\u90fd\u4f9d\u8d56\u4e8e\u60a3\u8005\u7279\u5b9a\u56e0\u7d20\uff08\u5982\u5e76\u53d1\u75c7\u548c\u7981\u5fcc\u75c7\uff09\u3002\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u8fd9\u79cd\u6761\u4ef6\u63a8\u7406\uff0c\u68c0\u7d22\u589e\u5f3a\u6216\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u7f3a\u4e4f\u786e\u4fdd\u68c0\u7d22\u77e5\u8bc6\u9002\u7528\u4e8e\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u663e\u5f0f\u673a\u5236\u3002", "method": "\u63d0\u51faCondMedQA\u57fa\u51c6\uff08\u9996\u4e2a\u6761\u4ef6\u6027\u751f\u7269\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\uff09\uff0c\u5305\u542b\u7b54\u6848\u968f\u60a3\u8005\u6761\u4ef6\u53d8\u5316\u7684\u591a\u8df3\u95ee\u9898\u3002\u63d0\u51fa\u6761\u4ef6\u95e8\u63a7\u63a8\u7406\uff08CGR\uff09\u6846\u67b6\uff0c\u6784\u5efa\u6761\u4ef6\u611f\u77e5\u77e5\u8bc6\u56fe\u8c31\uff0c\u57fa\u4e8e\u67e5\u8be2\u6761\u4ef6\u9009\u62e9\u6027\u6fc0\u6d3b\u6216\u526a\u679d\u63a8\u7406\u8def\u5f84\u3002", "result": "CGR\u80fd\u66f4\u53ef\u9760\u5730\u9009\u62e9\u6761\u4ef6\u9002\u5f53\u7684\u7b54\u6848\uff0c\u540c\u65f6\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u663e\u5f0f\u5efa\u6a21\u6761\u4ef6\u6027\u5bf9\u7a33\u5065\u533b\u5b66\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6761\u4ef6\u6027\u5efa\u6a21\u5bf9\u4e8e\u7a33\u5065\u7684\u533b\u5b66\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0cCondMedQA\u57fa\u51c6\u548cCGR\u6846\u67b6\u4e3a\u89e3\u51b3\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u60a3\u8005\u7279\u5b9a\u6761\u4ef6\u4f9d\u8d56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86WorkflowPerturb\u57fa\u51c6\uff0c\u901a\u8fc7\u5411\u9ec4\u91d1\u5de5\u4f5c\u6d41\u65bd\u52a0\u53d7\u63a7\u6270\u52a8\u6765\u8bc4\u4f30\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6307\u6807\u7684\u654f\u611f\u6027\u548c\u6821\u51c6\u6027", "motivation": "LLM\u751f\u6210\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u8bc4\u4f30\u56f0\u96be\uff0c\u73b0\u6709\u6307\u6807\u5206\u6570\u672a\u6821\u51c6\uff0c\u5206\u6570\u53d8\u5316\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620\u5de5\u4f5c\u6d41\u9000\u5316\u7684\u4e25\u91cd\u7a0b\u5ea6", "method": "\u521b\u5efa\u5305\u542b4,973\u4e2a\u9ec4\u91d1\u5de5\u4f5c\u6d41\u548c44,757\u4e2a\u6270\u52a8\u53d8\u4f53\u7684\u57fa\u51c6\uff0c\u5e94\u7528\u4e09\u79cd\u6270\u52a8\u7c7b\u578b\uff08\u7f3a\u5931\u6b65\u9aa4\u3001\u538b\u7f29\u6b65\u9aa4\u3001\u63cf\u8ff0\u53d8\u5316\uff09\u548c\u4e09\u4e2a\u4e25\u91cd\u7ea7\u522b\uff0810%\u300130%\u300150%\uff09", "result": "\u57fa\u51c6\u4e86\u591a\u4e2a\u6307\u6807\u5bb6\u65cf\uff0c\u901a\u8fc7\u9884\u671f\u5206\u6570\u8f68\u8ff9\u548c\u6b8b\u5dee\u5206\u6790\u5176\u654f\u611f\u6027\u548c\u6821\u51c6\u6027\uff0c\u63ed\u793a\u4e86\u6307\u6807\u5bb6\u65cf\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02", "conclusion": "WorkflowPerturb\u652f\u6301\u4e25\u91cd\u7a0b\u5ea6\u611f\u77e5\u7684\u5de5\u4f5c\u6d41\u8bc4\u4f30\u5206\u6570\u89e3\u91ca\uff0c\u6570\u636e\u96c6\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03"}}
{"id": "2602.18357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18357", "abs": "https://arxiv.org/abs/2602.18357", "authors": ["Wallace Albertini", "Marina Cond\u00e9 Ara\u00fajo", "J\u00falia Cond\u00e9 Ara\u00fajo", "Antonio Pedro Santos Alves", "Marcos Kalinowski"], "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation", "comment": "Author version of the paper accepted for publication at CAIN 2026", "summary": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.", "AI": {"tldr": "\u63d0\u51faSCFC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u8bc4\u4f30AI\u7cfb\u7edf\u529f\u80fd\u6b63\u786e\u6027\uff0c\u8fde\u63a5\u4e1a\u52a1\u9700\u6c42\u4e0e\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a\u5b9a\u4e49\u91cf\u5316\u89c4\u683c\u9650\u3001\u5206\u5c42\u6982\u7387\u62bd\u6837\u3001\u81ea\u52a9\u6cd5\u4f30\u8ba1\u7f6e\u4fe1\u533a\u95f4\u3001\u8ba1\u7b97\u80fd\u529b\u6307\u6570\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff08\u5982ISO/IEC 25059\uff09\u7f3a\u4e4f\u5b9e\u7528\u4e14\u7edf\u8ba1\u7a33\u5065\u7684\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u5c06\u4e1a\u52a1\u9700\u6c42\u4e0e\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u63d0\u51faSCFC\u65b9\u6cd5\uff1a1) \u5b9a\u4e49\u91cf\u5316\u89c4\u683c\u9650\uff1b2) \u8fdb\u884c\u5206\u5c42\u548c\u6982\u7387\u62bd\u6837\uff1b3) \u5e94\u7528\u81ea\u52a9\u6cd5\u4f30\u8ba1\u6027\u80fd\u6307\u6807\u7684\u7f6e\u4fe1\u533a\u95f4\uff1b4) \u8ba1\u7b97\u80fd\u529b\u6307\u6570\u4f5c\u4e3a\u6700\u7ec8\u6307\u6807\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u5de5\u4e1aAI\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u8bc4\u4f30\uff0c\u6536\u96c6AI\u4e13\u5bb6\u5173\u4e8e\u65b9\u6cd5\u5b9e\u7528\u6027\u3001\u6613\u7528\u6027\u548c\u91c7\u7528\u610f\u5411\u7684\u53cd\u9988\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u662f\u53ef\u884c\u4e14\u6709\u4ef7\u503c\u7684\u3002", "conclusion": "SCFC\u65b9\u6cd5\u662f\u5c06\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u64cd\u4f5c\u5316\u7684\u53ef\u884c\u4e14\u6709\u4ef7\u503c\u7684\u65b9\u5f0f\uff0c\u5c06\u8bc4\u4f30\u4ece\u70b9\u4f30\u8ba1\u8f6c\u53d8\u4e3a\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u9648\u8ff0\u3002"}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08VQPP\uff09\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u517156K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u5c55\u793a\u4e86VQPP\u5728\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u4e2d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\uff08CBVR\uff09\u9886\u57df\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e13\u95e8\u7684\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u57fa\u51c6\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\u7684VQPP\u57fa\u51c6\uff0c\u5305\u542b56K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff0c\u63d0\u4f9b\u5b98\u65b9\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5212\u5206\u3002\u63a2\u7d22\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u4f7f\u7528\u6700\u4f73\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u67e5\u8be2\u91cd\u5199\u3002", "result": "\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5728\u68c0\u7d22\u6b65\u9aa4\u4e4b\u524d\u5b9e\u73b0\u5e94\u7528\u3002VQPP\u57fa\u51c6\u4e3a\u89c6\u9891\u9886\u57df\u7684QPP\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6bd4\u8f83\u7684\u57fa\u7840\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5efa\u7acb\u4e86\u9996\u4e2aVQPP\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002\u4ee3\u7801\u548c\u57fa\u51c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\u7684\u6307\u4ee4\u4f18\u5316\u65b9\u6cd5\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6307\u4ee4\u4f18\u5316\u80fd\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u4e0d\u540c\u63d0\u793a\u6280\u672f\u6548\u679c\u5404\u5f02\u3002", "motivation": "\u6307\u4ee4\u4f18\u5316\u4e3a\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u6307\u4ee4\u4f18\u5316\u7684\u7cfb\u7edf\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\uff0c\u8bc4\u4f30\u56db\u79cd\u5f00\u7bb1\u5373\u7528\u7684\u63d0\u793a\u6280\u672f\uff1a\u76f4\u63a5\u9884\u6d4b\u3001\u601d\u7ef4\u94fe\u3001\u5e26SQL\u5de5\u5177\u7684ReAct\u3001\u5e26Python\u6267\u884c\u7684CodeAct\u3002\u7814\u7a76\u4e09\u79cdDSPy\u4f18\u5316\u5668\uff08COPRO\u3001MiPROv2\u3001SIMBA\uff09\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6307\u4ee4\u4f18\u5316\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff1aMiPROv2\u5bf9\u601d\u7ef4\u94fe\u63d0\u4f9b\u6700\u7a33\u5b9a\u7684\u589e\u76ca\uff0cSIMBA\u5bf9ReAct\u667a\u80fd\u4f53\u63d0\u4f9b\u6700\u5927\u6536\u76ca\uff08\u5c24\u5176\u5728\u5927\u6a21\u578b\u89c4\u6a21\u4e0b\uff09\u3002\u884c\u4e3a\u5206\u6790\u663e\u793aSIMBA\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u9f13\u52b1\u66f4\u76f4\u63a5\u7684\u63a8\u7406\u8def\u5f84\uff0c\u63d0\u5347\u601d\u7ef4\u94fe\u4e2d\u7684\u6570\u503c\u6bd4\u8f83\u80fd\u529b\uff0c\u5e2e\u52a9ReAct\u667a\u80fd\u4f53\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "\u5728\u4e0d\u540c\u63d0\u793a\u6280\u672f\u4e2d\uff0c\u601d\u7ef4\u94fe\u5bf9\u8868\u683c\u4e8b\u5b9e\u68c0\u67e5\u4ecd\u7136\u6709\u6548\uff08\u5c24\u5176\u5bf9\u5c0f\u6a21\u578b\uff09\u3002\u867d\u7136\u7528\u5927\u6a21\u578b\u6784\u5efa\u7684ReAct\u667a\u80fd\u4f53\u53ef\u4ee5\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u7684\u6307\u4ee4\u4f18\u5316\u3002"}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7ed3\u5408\uff0c\u5229\u7528\u5f02\u6784\u673a\u5668\u4eba\u8f68\u8ff9\u9884\u8bad\u7ec3\u901a\u7528\u63a7\u5236\u7b56\u7565\uff0c\u4f46\u591a\u673a\u5668\u4eba\u7c7b\u578b\u4f1a\u5f15\u53d1\u68af\u5ea6\u51b2\u7a81\uff0c\u901a\u8fc7\u5f62\u6001\u76f8\u4f3c\u6027\u5206\u7ec4\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u5b66\u4e60\u6765\u5229\u7528\u5f02\u6784\u673a\u5668\u4eba\u8f68\u8ff9\u83b7\u53d6\u901a\u7528\u63a7\u5236\u5148\u9a8c\u3002", "method": "1) \u7cfb\u7edf\u5206\u6790\u79bb\u7ebfRL\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u8303\u5f0f\uff1b2) \u6784\u5efa\u5305\u542b16\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u8fd0\u52a8\u6570\u636e\u96c6\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u901a\u8fc7\u7ec4\u68af\u5ea6\u66f4\u65b0\u6a21\u578b\u4ee5\u51cf\u5c11\u673a\u5668\u4eba\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u3002", "result": "1) \u79bb\u7ebfRL\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7ed3\u5408\u5728\u5305\u542b\u5927\u91cf\u6b21\u4f18\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u7eaf\u884c\u4e3a\u514b\u9686\uff1b2) \u968f\u7740\u6b21\u4f18\u6570\u636e\u6bd4\u4f8b\u548c\u673a\u5668\u4eba\u7c7b\u578b\u589e\u52a0\uff0c\u5f62\u6001\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u4f1a\u963b\u788d\u5b66\u4e60\uff1b3) \u5f62\u6001\u5206\u7ec4\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u673a\u5668\u4eba\u95f4\u51b2\u7a81\uff0c\u4f18\u4e8e\u73b0\u6709\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\u3002", "conclusion": "\u79bb\u7ebfRL\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7ed3\u5408\u662f\u6709\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5904\u7406\u591a\u673a\u5668\u4eba\u7c7b\u578b\u5e26\u6765\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u7b80\u5355\u7684\u5f62\u6001\u5206\u7ec4\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szir\u00e1nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\uff0c\u6307\u51fa\u5176\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5bfc\u81f4\u62a5\u544a\u7684\u9ad8\u51c6\u786e\u7387\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\uff0c\u4f5c\u8005\u53d1\u73b0\u5176\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5e27\u7ea7\u522b\u7684\u968f\u673a\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\u4f1a\u5bfc\u81f4\u540c\u4e00\u53d7\u8bd5\u8005\u7684\u6837\u672c\u540c\u65f6\u51fa\u73b0\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\uff0c\u9020\u6210\u6570\u636e\u6cc4\u9732\uff0c\u65e0\u6cd5\u771f\u5b9e\u8bc4\u4f30\u6a21\u578b\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5df2\u53d1\u5e03\u7684\u6df7\u6dc6\u77e9\u9635\u3001\u5b66\u4e60\u66f2\u7ebf\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\uff0c\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u8bc1\u660e\u4e86\u8bc4\u4f30\u534f\u8bae\u7684\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ed6\u4eec\u5c55\u793a\u4e86\u5e27\u7ea7\u522b\u7684\u968f\u673a\u5206\u5272\u5982\u4f55\u4e0d\u53ef\u907f\u514d\u5730\u6df7\u5408\u540c\u4e00\u53d7\u8bd5\u8005\u7684\u6837\u672c\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u62a5\u544a\u7684\u9ad8\u51c6\u786e\u7387\u6307\u6807\uff08\u63a5\u8fd1\u5b8c\u7f8e\uff09\u662f\u7531\u4e8e\u6570\u636e\u6cc4\u9732\u9020\u6210\u7684\uff0c\u800c\u975e\u6a21\u578b\u771f\u6b63\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8bc4\u4f30\u534f\u8bae\u672a\u80fd\u6d4b\u91cf\u6a21\u578b\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u8fd9\u5728\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u5206\u6790\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\uff0c\u91c7\u7528\u72ec\u7acb\u4e8e\u53d7\u8bd5\u8005\u7684\u6570\u636e\u5212\u5206\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u53ef\u9760\u8bc6\u522b\u672a\u89c1\u4e2a\u4f53\u624b\u52bf\u7684\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\uff09\uff0c\u5fc5\u987b\u786e\u4fdd\u8bc4\u4f30\u534f\u8bae\u80fd\u591f\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.17949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17949", "abs": "https://arxiv.org/abs/2602.17949", "authors": ["Victoria Blake", "Mathew Miller", "Jamie Novak", "Sze-yuan Ooi", "Blanca Gallego"], "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications", "comment": "30 pages, 6 figures, 4 tables", "summary": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.", "AI": {"tldr": "CUICurate\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316UMLS\u6982\u5ff5\u96c6\u6784\u5efa\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u548cLLM\u8fc7\u6ee4\u5206\u7c7b\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u5de5\u5177\u901a\u5e38\u5c06\u81ea\u7531\u6587\u672c\u6620\u5c04\u5230UMLS\u6982\u5ff5\u552f\u4e00\u6807\u8bc6\u7b26(CUIs)\uff0c\u4f46\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u7684\u662f\u5305\u542b\u76f8\u5173\u540c\u4e49\u8bcd\u3001\u5b50\u7c7b\u578b\u548c\u8d85\u7c7b\u578b\u7684\u6982\u5ff5\u96c6\u3002\u76ee\u524d\u6784\u5efa\u8fd9\u6837\u7684\u6982\u5ff5\u96c6\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u3001\u6267\u884c\u4e0d\u4e00\u81f4\u7684\uff0c\u5e76\u4e14\u73b0\u6709\u5de5\u5177\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86CUICurate\u6846\u67b6\uff0c\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210(GraphRAG)\u3002\u9996\u5148\u6784\u5efa\u5e76\u5d4c\u5165UMLS\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u8bed\u4e49\u68c0\u7d22\uff0c\u7136\u540e\u9488\u5bf9\u6bcf\u4e2a\u76ee\u6807\u6982\u5ff5\u4eceKG\u4e2d\u68c0\u7d22\u5019\u9009CUIs\uff0c\u63a5\u7740\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(\u6bd4\u8f83\u4e86GPT-5\u548cGPT-5-mini)\u8fdb\u884c\u8fc7\u6ee4\u548c\u5206\u7c7b\u3002", "result": "\u5728\u4e94\u4e2a\u8bcd\u6c47\u5f02\u8d28\u6027\u4e34\u5e8a\u6982\u5ff5\u4e0a\u8bc4\u4f30\uff0cCUICurate\u751f\u6210\u7684\u6982\u5ff5\u96c6\u6bd4\u4eba\u5de5\u57fa\u51c6\u66f4\u5927\u66f4\u5b8c\u6574\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u7cbe\u786e\u5ea6\u3002GPT-5-mini\u5728\u8fc7\u6ee4\u9636\u6bb5\u53ec\u56de\u7387\u66f4\u9ad8\uff0c\u800cGPT-5\u7684\u5206\u7c7b\u7ed3\u679c\u66f4\u63a5\u8fd1\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u3002\u8f93\u51fa\u7a33\u5b9a\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "CUICurate\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u6765\u652f\u6301UMLS\u6982\u5ff5\u96c6\u6784\u5efa\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u548cLLM\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u751f\u6210\u805a\u7126\u7684\u5019\u9009\u6982\u5ff5\u96c6\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u8868\u578b\u548c\u5206\u6790\u9700\u6c42\u7684\u4e34\u5e8aNLP\u6d41\u7a0b\u3002"}}
{"id": "2602.18095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18095", "abs": "https://arxiv.org/abs/2602.18095", "authors": ["Hyunseok Oh", "Sam Stern", "Youngki Lee", "Matthai Philipose"], "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory", "comment": null, "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.", "AI": {"tldr": "Logitext\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u8bed\u8a00\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u7ea6\u675f\u8bc4\u4f30\u548cSMT\u6c42\u89e3\u5b9e\u73b0\u6587\u672c-\u903b\u8f91\u8054\u5408\u63a8\u7406\uff0c\u6269\u5c55\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5230\u975e\u5b8c\u5168\u5f62\u5f0f\u5316\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7ed3\u5408LLM\u548c\u6c42\u89e3\u5668\uff0c\u4f46\u4ec5\u9650\u4e8e\u6570\u5b66\u6216\u7a0b\u5e8f\u5408\u6210\u7b49\u5b8c\u5168\u5f62\u5f0f\u5316\u4efb\u52a1\uff0c\u65e0\u6cd5\u5904\u7406\u53ea\u6709\u90e8\u5206\u903b\u8f91\u7ed3\u6784\u7684\u81ea\u7136\u6587\u6863\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9e\u73b0\u6587\u672c\u548c\u903b\u8f91\u7684\u53ef\u9760\u4ea4\u7ec7\u63a8\u7406\u3002", "method": "\u63d0\u51faLogitext\u795e\u7ecf\u7b26\u53f7\u8bed\u8a00\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f(NLTCs)\uff0c\u4f7f\u90e8\u5206\u903b\u8f91\u7ed3\u6784\u663e\u5f0f\u5316\u3002\u5f00\u53d1\u7b97\u6cd5\u6574\u5408LLM\u7ea6\u675f\u8bc4\u4f30\u548c\u53ef\u6ee1\u8db3\u6027\u6a21\u7406\u8bba(SMT)\u6c42\u89e3\uff0c\u5b9e\u73b0\u8054\u5408\u6587\u672c-\u903b\u8f91\u63a8\u7406\u3002", "result": "\u5728\u65b0\u5185\u5bb9\u5ba1\u6838\u57fa\u51c6\u3001LegalBench\u548cSuper-Natural Instructions\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLogitext\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\u3002\u8fd9\u662f\u9996\u6b21\u5c06LLM\u63a8\u7406\u89c6\u4e3aSMT\u7406\u8bba\uff0c\u6269\u5c55\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5230\u975e\u5b8c\u5168\u5f62\u5f0f\u5316\u9886\u57df\u3002", "conclusion": "Logitext\u901a\u8fc7\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f\uff0c\u7ed3\u5408LLM\u548cSMT\u6c42\u89e3\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u672c\u548c\u903b\u8f91\u7684\u8054\u5408\u63a8\u7406\uff0c\u4e3a\u5904\u7406\u90e8\u5206\u903b\u8f91\u7ed3\u6784\u7684\u81ea\u7136\u6587\u6863\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\u548c\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u5197\u4f59\u95ee\u9898", "motivation": "\u968f\u7740\u89c6\u9891\u9aa8\u5e72\u67b6\u6784\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5206\u6790\u957f\u8fbe\u6570\u5341\u5206\u949f\u7684\u957f\u89c6\u9891\u53d8\u5f97\u53ef\u884c\u4e14\u666e\u904d\u3002\u4f46\u89c6\u9891\u5e8f\u5217\u56fa\u6709\u7684\u5197\u4f59\u6027\u7ed9\u73b0\u6709\u6a21\u578b\u5e26\u6765\u4e24\u5927\u6311\u6218\uff1a1) \u5728\u5185\u5b58\u9650\u5236\u5185\u9ad8\u6548\u5904\u7406\u66f4\u591a\u5e27\uff1b2) \u4ece\u5927\u91cf\u8f93\u5165\u6570\u636e\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668(AVS)\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668(SVC)\uff0c\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e2\u64c5\u957f\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u4e5f\u5728\u6807\u51c6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5904\u7406\u957f\u89c6\u9891\u590d\u6742\u6027\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u80fd\u81ea\u9002\u5e94\u6709\u6548\u5730\u4ece\u4e0d\u540c\u65f6\u957f\u89c6\u9891\u4e2d\u6355\u83b7\u5173\u952e\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u7684\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u5224\u522b\u4fe1\u606f\uff0c\u4e3a\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.17981", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17981", "abs": "https://arxiv.org/abs/2602.17981", "authors": ["Amine Kobeissi", "Philippe Langlais"], "title": "Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering", "comment": null, "summary": "Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91d1\u878d\u95ee\u7b54\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u867d\u7136\u68c0\u7d22\u5230\u6b63\u786e\u6587\u6863\uff0c\u4f46\u9057\u6f0f\u4e86\u5305\u542b\u7b54\u6848\u7684\u5177\u4f53\u9875\u9762\u6216\u5757\uff0c\u5bfc\u81f4\u751f\u6210\u5668\u57fa\u4e8e\u4e0d\u5b8c\u6574\u4e0a\u4e0b\u6587\u63a8\u65ad\u3002\u4f5c\u8005\u8bc4\u4f30\u4e86\u591a\u7c92\u5ea6\u68c0\u7d22\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u91d1\u878d\u6587\u6863\u7684\u9875\u9762\u7ea7\u68c0\u7d22\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u5728\u91d1\u878d\u76d1\u7ba1\u6587\u4ef6\u95ee\u7b54\u4e2d\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u867d\u7136\u5e38\u7528\uff0c\u4f46\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u80fd\u5426\u68c0\u7d22\u5230\u786e\u5207\u7684\u4e0a\u4e0b\u6587\u6765\u652f\u6301\u7b54\u6848\u3002\u4f5c\u8005\u5173\u6ce8\u4e00\u4e2a\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff1a\u68c0\u7d22\u5230\u6b63\u786e\u6587\u6863\u4f46\u9057\u6f0f\u4e86\u5305\u542b\u7b54\u6848\u7684\u5177\u4f53\u9875\u9762\u6216\u5757\uff0c\u5bfc\u81f4\u751f\u6210\u5668\u57fa\u4e8e\u4e0d\u5b8c\u6574\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u65ad\u3002\u5c3d\u7ba1\u8fd9\u4e2a\u95ee\u9898\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u91d1\u878d\u95ee\u7b54\u6587\u732e\u4e2d\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002", "method": "1) \u5728\u591a\u7c92\u5ea6\u5c42\u9762\uff08\u6587\u6863\u3001\u9875\u9762\u3001\u5757\uff09\u8bc4\u4f30\u68c0\u7d22\u6027\u80fd\uff1b2) \u5f15\u5165\u57fa\u4e8eoracle\u7684\u5206\u6790\u6765\u63d0\u4f9b\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\u7684\u7ecf\u9a8c\u4e0a\u754c\uff1b3) \u5728FinanceBench\u7684150\u4e2a\u95ee\u9898\u5b50\u96c6\u4e0a\u590d\u73b0\u548c\u6bd4\u8f83\u591a\u79cd\u68c0\u7d22\u7b56\u7565\uff08\u5bc6\u96c6\u3001\u7a00\u758f\u3001\u6df7\u5408\u3001\u5206\u5c42\u68c0\u7d22\uff0c\u5305\u62ec\u91cd\u6392\u5e8f\u548c\u67e5\u8be2\u91cd\u6784\uff09\uff1b4) \u63d0\u51fa\u9886\u57df\u5fae\u8c03\u7684\u9875\u9762\u8bc4\u5206\u5668\uff0c\u5c06\u9875\u9762\u4f5c\u4e3a\u6587\u6863\u548c\u5757\u4e4b\u95f4\u7684\u4e2d\u95f4\u68c0\u7d22\u5355\u5143\uff0c\u4e13\u95e8\u9488\u5bf9\u91d1\u878d\u6587\u6863\u5fae\u8c03\u53cc\u7f16\u7801\u5668\u8fdb\u884c\u9875\u9762\u7ea7\u76f8\u5173\u6027\u5224\u65ad\u3002", "result": "1) \u4e0d\u540c\u65b9\u6cd5\u4e2d\uff0c\u6587\u6863\u53d1\u73b0\u80fd\u529b\u7684\u63d0\u5347\u901a\u5e38\u80fd\u8f6c\u5316\u4e3a\u66f4\u5f3a\u7684\u9875\u9762\u53ec\u56de\u7387\uff1b2) Oracle\u6027\u80fd\u5206\u6790\u8868\u660e\u9875\u9762\u548c\u5757\u7ea7\u68c0\u7d22\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff1b3) \u63d0\u51fa\u7684\u9886\u57df\u5fae\u8c03\u9875\u9762\u8bc4\u5206\u5668\u663e\u8457\u63d0\u5347\u4e86\u9875\u9762\u53ec\u56de\u7387\u548c\u5757\u68c0\u7d22\u6027\u80fd\uff0c\u901a\u8fc7\u5229\u7528\u9875\u9762\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u6bb5\u843d\u7684\u5206\u5c42\u68c0\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u91d1\u878d\u95ee\u7b54\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6587\u6863\u5185\u68c0\u7d22\u5931\u8d25\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u91d1\u878d\u6587\u6863\u7684\u9875\u9762\u7ea7\u68c0\u7d22\u4f18\u5316\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e13\u95e8\u9488\u5bf9\u91d1\u878d\u6587\u6863\u5fae\u8c03\u7684\u9875\u9762\u8bc4\u5206\u5668\u80fd\u6709\u6548\u63d0\u5347\u9875\u9762\u53ec\u56de\u7387\u548c\u5757\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u6587\u6863\u5185\u68c0\u7d22\u5931\u8d25\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "SOMtime\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u8868\u793a\u4e2d\u5373\u4f7f\u654f\u611f\u5c5e\u6027\u88ab\u6392\u9664\uff0c\u4ecd\u80fd\u6062\u590d\u51fa\u4e0e\u5e74\u9f84\u3001\u6536\u5165\u7b49\u654f\u611f\u5c5e\u6027\u5bf9\u9f50\u7684\u6f5c\u5728\u8f74\uff0c\u63ed\u793a\u4e86\"\u516c\u5e73\u6027\u65e0\u77e5\"\u5728\u8868\u793a\u5c42\u9762\u7684\u5931\u8d25\u3002", "motivation": "\u6311\u6218\u65e0\u76d1\u7763\u8868\u793a\u4e2d\u654f\u611f\u5c5e\u6027\u88ab\u6392\u9664\u5373\u4e2d\u7acb\u7684\u5047\u8bbe\uff0c\u8bc1\u660e\u5373\u4f7f\u654f\u611f\u5c5e\u6027\u88ab\u660e\u786e\u6392\u9664\u5728\u8f93\u5165\u4e4b\u5916\uff0c\u5b83\u4eec\u4ecd\u53ef\u80fd\u4f5c\u4e3a\u4e3b\u5bfc\u6f5c\u5728\u8f74\u5728\u5d4c\u5165\u4e2d\u51fa\u73b0\u3002", "method": "\u4f7f\u7528SOMtime\uff08\u57fa\u4e8e\u9ad8\u5bb9\u91cf\u81ea\u7ec4\u7ec7\u6620\u5c04\u7684\u62d3\u6251\u4fdd\u6301\u8868\u793a\u65b9\u6cd5\uff09\uff0c\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08\u4e94\u4e2a\u56fd\u5bb6\u7684\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u548c\u4eba\u53e3\u666e\u67e5\u6536\u5165\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0ePCA\u3001UMAP\u3001t-SNE\u548c\u81ea\u7f16\u7801\u5668\u7b49\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "SOMtime\u6062\u590d\u4e86\u4e0e\u6392\u9664\u7684\u654f\u611f\u5c5e\u6027\u5bf9\u9f50\u7684\u5355\u8c03\u6392\u5e8f\uff0c\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\u9ad8\u8fbe0.85\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u901a\u5e38\u4f4e\u4e8e0.23\uff1b\u65e0\u76d1\u7763\u5206\u5272SOMtime\u5d4c\u5165\u4f1a\u4ea7\u751f\u4eba\u53e3\u7edf\u8ba1\u504f\u659c\u7684\u805a\u7c7b\u3002", "conclusion": "\"\u516c\u5e73\u6027\u65e0\u77e5\"\u5728\u5e8f\u6570\u654f\u611f\u5c5e\u6027\u7684\u8868\u793a\u5c42\u9762\u5931\u8d25\uff0c\u516c\u5e73\u6027\u5ba1\u8ba1\u5fc5\u987b\u6269\u5c55\u5230\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u7684\u65e0\u76d1\u7763\u7ec4\u4ef6\u3002"}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u4f20\u7edf\u7684\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\uff08\u7279\u522b\u662f\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff09\u4e0a\u8868\u73b0\u843d\u540e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\u7684\u539f\u56e0\u3002", "method": "\u5bf9\u5927\u91cf\u6700\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u5305\u62ec\u4e0d\u540cLLM\u548c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6548\u679c\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u4f7f\u7528\u66f4\u597d\u7684LLM\u5bf9\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u90fd\u6709\u540c\u7b49\u63d0\u5347\uff1b2\uff09\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5bf9\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u6709\u4e0d\u6210\u6bd4\u4f8b\u7684\u663e\u8457\u63d0\u5347\uff1b3\uff09\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u672a\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u89c6\u89c9\u4e2d\u5fc3\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u8d28\u91cf\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.18029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18029", "abs": "https://arxiv.org/abs/2602.18029", "authors": ["Ali El Filali", "In\u00e8s Bedar"], "title": "Towards More Standardized AI Evaluation: From Models to Agents", "comment": "19 pages, 3 figures", "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u5df2\u4e0d\u9002\u7528\u4e8eAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u8bc4\u4f30\u5e94\u4ece\"\u6027\u80fd\u5267\u573a\"\u8f6c\u53d8\u4e3a\u652f\u6301\u4fe1\u4efb\u3001\u8fed\u4ee3\u548c\u6cbb\u7406\u7684\u6d4b\u91cf\u5b66\u79d1\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ece\u9759\u6001\u6a21\u578b\u53d1\u5c55\u4e3a\u590d\u5408\u578b\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\uff0c\u8bc4\u4f30\u4e0d\u518d\u662f\u6700\u7ec8\u68c0\u67e5\u70b9\uff0c\u800c\u6210\u4e3a\u6838\u5fc3\u63a7\u5236\u529f\u80fd\u3002\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u57fa\u4e8e\u6a21\u578b\u4e2d\u5fc3\u65f6\u4ee3\u7684\u5047\u8bbe\uff08\u9759\u6001\u57fa\u51c6\u3001\u805a\u5408\u5206\u6570\u3001\u4e00\u6b21\u6027\u6210\u529f\u6807\u51c6\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8d8a\u6765\u8d8a\u6a21\u7cca\u800c\u975e\u9610\u660e\u7cfb\u7edf\u884c\u4e3a\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u8bc4\u4f30\u7ba1\u9053\u5982\u4f55\u5f15\u5165\u9759\u9ed8\u6545\u969c\u6a21\u5f0f\u3001\u89e3\u91ca\u9ad8\u57fa\u51c6\u5206\u6570\u4e3a\u4f55\u7ecf\u5e38\u8bef\u5bfc\u56e2\u961f\uff0c\u4ee5\u53ca\u63a2\u8ba8\u4ee3\u7406\u7cfb\u7edf\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u6027\u80fd\u6d4b\u91cf\u7684\u610f\u4e49\uff0c\u6765\u9610\u660e\u8bc4\u4f30\u5728AI\u65f6\u4ee3\u7684\u4f5c\u7528\u3002", "result": "\u8bba\u6587\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1a\u8bc4\u4f30\u7ba1\u9053\u672c\u8eab\u4f1a\u5f15\u5165\u6545\u969c\u6a21\u5f0f\uff0c\u9ad8\u57fa\u51c6\u5206\u6570\u5177\u6709\u8bef\u5bfc\u6027\uff0c\u4ee3\u7406\u7cfb\u7edf\u6539\u53d8\u4e86\u6027\u80fd\u6d4b\u91cf\u7684\u672c\u8d28\u542b\u4e49\u3002", "conclusion": "\u8bc4\u4f30\u4e0d\u5e94\u662f\"\u6027\u80fd\u5267\u573a\"\uff0c\u800c\u5e94\u6210\u4e3a\u652f\u6301\u975e\u786e\u5b9a\u6027\u7cfb\u7edf\u4e2d\u4fe1\u4efb\u5efa\u7acb\u3001\u8fed\u4ee3\u6539\u8fdb\u548c\u6cbb\u7406\u7684\u6d4b\u91cf\u5b66\u79d1\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u7cfb\u7edf\u65f6\u4ee3\u9700\u8981\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u7684\u89d2\u8272\u548c\u610f\u4e49\u3002"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "OMAD\uff1a\u9996\u4e2a\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u8054\u5408\u71b5\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u4e0e\u534f\u8c03\uff0c\u572810\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u591a\u6a21\u6001\u8868\u793a\uff0c\u4f46\u5728\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3b\u8981\u969c\u788d\u662f\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u6027\u963b\u788d\u4e86\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51faOMAD\u6846\u67b6\uff1a1\uff09\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\uff0c\u5b9e\u73b0\u65e0\u4f3c\u7136\u4f9d\u8d56\u7684\u6709\u6548\u63a2\u7d22\uff1b2\uff09\u5728CTDE\u8303\u5f0f\u4e0b\u4f7f\u7528\u8054\u5408\u5206\u5e03\u503c\u51fd\u6570\u4f18\u5316\u5206\u6563\u6269\u6563\u7b56\u7565\uff1b3\uff09\u5229\u7528\u53ef\u5904\u7406\u7684\u71b5\u589e\u5f3a\u76ee\u6807\u6307\u5bfc\u6269\u6563\u7b56\u7565\u540c\u6b65\u66f4\u65b0\uff0c\u786e\u4fdd\u7a33\u5b9a\u534f\u8c03\u3002", "result": "\u5728MPE\u548cMAMuJoCo\u768410\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad82.5\u500d\u52305\u500d\u3002", "conclusion": "OMAD\u6210\u529f\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u71b5\u6700\u5927\u5316\u65b9\u6cd5\u514b\u670d\u4e86\u6269\u6563\u6a21\u578b\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u63a2\u7d22\u80fd\u529b\u548c\u534f\u8c03\u6027\u80fd\u3002"}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f\u591a\u6a21\u6001\u6d4b\u8ddd\u6570\u636e\uff08\u5982\u96f7\u8fbe\u6216\u6fc0\u5149\u96f7\u8fbe\uff09\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u91cd\u5efa\u7a20\u5bc6\u6df1\u5ea6\u56fe\u6765\u66ff\u4ee3\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u5728\u4f4e\u7eb9\u7406\u3001\u6076\u52a3\u5929\u6c14\u548c\u906e\u6321\u4e25\u91cd\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u6df1\u5ea6\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u5408\u6210\u89c6\u56fe\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6df1\u5ea6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u6781\u7a00\u758f\u7684\u6d4b\u8ddd\u4f20\u611f\u6570\u636e\uff08\u5982\u6c7d\u8f66\u96f7\u8fbe\u6216\u6fc0\u5149\u96f7\u8fbe\uff09\uff0c\u91c7\u7528\u5c40\u90e8\u9ad8\u65af\u8fc7\u7a0b\u516c\u5f0f\u5728\u89d2\u5ea6\u57df\u5efa\u6a21\u6df1\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u5e76\u663e\u5f0f\u91cf\u5316\u89c2\u6d4b\u6709\u9650\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u91cd\u5efa\u7684\u6df1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u53ef\u76f4\u63a5\u66ff\u6362\u73b0\u6709\u6269\u6563\u6e32\u67d3\u6d41\u7a0b\u4e2d\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u771f\u5b9e\u591a\u6a21\u6001\u9a7e\u9a76\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u7528\u7a00\u758f\u6d4b\u8ddd\u91cd\u5efa\u6df1\u5ea6\u66ff\u4ee3\u7eaf\u89c6\u89c9\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u89c6\u9891\u751f\u6210\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u9760\u51e0\u4f55\u5148\u9a8c\u5bf9\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89d2\u5408\u6210\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u6781\u7aef\u7a00\u758f\u60c5\u51b5\u4e0b\uff0c\u591a\u6a21\u6001\u4f20\u611f\u4e5f\u80fd\u5e26\u6765\u5b9e\u9645\u76ca\u5904\u3002"}}
{"id": "2602.18092", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18092", "abs": "https://arxiv.org/abs/2602.18092", "authors": ["Matthew DiGiuseppe", "Joshua Robison"], "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities", "comment": "39 pages, 10 figures", "summary": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u7528\u6237\u8ba4\u4e3a\u804a\u5929AI\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u65f6\uff0c\u5176\u7ea0\u6b63\u9519\u8bef\u4fe1\u606f\u7684\u6548\u679c\u4f1a\u663e\u8457\u964d\u4f4e28%\uff0c\u8868\u660eAI\u7684\u8bf4\u670d\u529b\u53d7\u653f\u6cbb\u4e2d\u7acb\u6027\u611f\u77e5\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u5165\u653f\u6cbb\u9886\u57df\uff0c\u7cbe\u82f1\u9636\u5c42\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u5176\u63cf\u7ed8\u4e3a\u5177\u6709\u610f\u8bc6\u5f62\u6001\u503e\u5411\u3002\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5\u8fd9\u79cd\u53ef\u4fe1\u5ea6\u653b\u51fb\u662f\u5426\u4f1a\u964d\u4f4e\u57fa\u4e8eLLM\u7684\u8bf4\u670d\u6548\u679c\uff0c\u63a2\u7a76AI\u8bf4\u670d\u529b\u7684\u653f\u6cbb\u6761\u4ef6\u6027\u3002", "method": "\u5728\u7f8e\u56fd\u8fdb\u884c\u4e86\u4e00\u9879\u9884\u6ce8\u518c\u8c03\u67e5\u5b9e\u9a8c\uff08N=2144\uff09\uff0c\u53c2\u4e0e\u8005\u4e0eChatGPT\u8fdb\u884c\u4e09\u8f6e\u5bf9\u8bdd\uff0c\u8ba8\u8bba\u4e2a\u4eba\u6301\u6709\u7684\u7ecf\u6d4e\u653f\u7b56\u8bef\u89e3\u3002\u5b9e\u9a8c\u7ec4\u6536\u5230\u7b80\u77ed\u4fe1\u606f\u8868\u660eLLM\u5bf9\u53c2\u4e0e\u8005\u6240\u5c5e\u515a\u6d3e\u5b58\u5728\u504f\u89c1\uff0c\u5bf9\u7167\u7ec4\u4e3a\u4e2d\u6027\u4fe1\u606f\u3002", "result": "\u76f8\u6bd4\u4e2d\u6027\u5bf9\u7167\u7ec4\uff0c\u8868\u660eLLM\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u7684\u4fe1\u606f\u4f7f\u8bf4\u670d\u6548\u679c\u964d\u4f4e28%\u3002\u8f6c\u5f55\u5206\u6790\u663e\u793a\u8b66\u544a\u6539\u53d8\u4e86\u4e92\u52a8\u65b9\u5f0f\uff1a\u53d7\u8bbf\u8005\u66f4\u9891\u7e41\u5730\u53cd\u9a73\uff0c\u53c2\u4e0e\u5ea6\u66f4\u4f4e\uff0c\u63a5\u53d7\u5ea6\u66f4\u5dee\u3002", "conclusion": "\u5bf9\u8bdd\u5f0fAI\u7684\u8bf4\u670d\u6548\u679c\u5177\u6709\u653f\u6cbb\u6761\u4ef6\u6027\uff0c\u53d7\u515a\u6d3e\u4e00\u81f4\u6027\u611f\u77e5\u7684\u5236\u7ea6\u3002\u5f53\u7528\u6237\u8ba4\u4e3aAI\u5b58\u5728\u653f\u6cbb\u504f\u89c1\u65f6\uff0c\u5176\u7ea0\u6b63\u9519\u8bef\u4fe1\u606f\u7684\u80fd\u529b\u4f1a\u663e\u8457\u51cf\u5f31\u3002"}}
{"id": "2602.17929", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17929", "abs": "https://arxiv.org/abs/2602.17929", "authors": ["Athanasios Angelakis"], "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging", "comment": "15 pages, 12 figures, 7 tables. Code and models available at https://github.com/Bluesman79/ZACH-ViT", "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.\n  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.", "AI": {"tldr": "ZACH-ViT\u662f\u4e00\u79cd\u7d27\u51d1\u578b\u89c6\u89c9Transformer\uff0c\u79fb\u9664\u4e86\u4f4d\u7f6e\u5d4c\u5165\u548c[CLS]\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfVision Transformer\u4f9d\u8d56\u4f4d\u7f6e\u5d4c\u5165\u548c\u7c7b\u522b\u6807\u8bb0\u7f16\u7801\u56fa\u5b9a\u7a7a\u95f4\u5148\u9a8c\uff0c\u8fd9\u5728\u533b\u5b66\u6210\u50cf\u7b49\u7a7a\u95f4\u5e03\u5c40\u4fe1\u606f\u8f83\u5f31\u6216\u4e0d\u4e00\u81f4\u7684\u573a\u666f\u4e2d\u53ef\u80fd\u963b\u788d\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faZACH-ViT\uff0c\u79fb\u9664\u4f4d\u7f6e\u5d4c\u5165\u548c[CLS]\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u6b8b\u5dee\u6295\u5f71\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728\u7d27\u51d1\u53c2\u6570\u9884\u7b97\u4e0b\u8fd0\u884c\u3002", "result": "\u57287\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cZACH-ViT\uff080.25M\u53c2\u6570\uff09\u5728BloodMNIST\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5728PathMNIST\u4e0a\u4e0eTransMIL\u7ade\u4e89\uff0c\u5728\u5177\u6709\u5f3a\u89e3\u5256\u5148\u9a8c\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u51cf\u5f31\u3002", "conclusion": "\u5c06\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u4e0e\u6570\u636e\u7ed3\u6784\u5bf9\u9f50\u6bd4\u8ffd\u6c42\u901a\u7528\u57fa\u51c6\u4e3b\u5bfc\u66f4\u91cd\u8981\u3002ZACH-ViT\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u90e8\u7f72\u6f5c\u529b\uff0c\u652f\u6301\u4e9a\u79d2\u7ea7\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2602.18137", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18137", "abs": "https://arxiv.org/abs/2602.18137", "authors": ["Vincent Grari", "Ciprian Tomoiaga", "Sylvain Lamprier", "Tatsunori Hashimoto", "Marcin Detyniecki"], "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs", "comment": "9 pages, 1 Figure", "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u95ee\u7b54\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5f85\u9002\u5e94\u6a21\u578b\u4e0e\u4e13\u5bb6\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u6311\u6218\u6027\u95ee\u9898\uff0c\u5728\u5c11\u91cf\u5408\u6210\u6837\u672c\u4e0b\u63d0\u5347LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "LLM\u5728\u4e13\u4e1a\u9886\u57df\u9002\u5e94\u56f0\u96be\uff0c\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\uff08\u5982\u8f6c\u8ff0\u3001\u77e5\u8bc6\u63d0\u53d6\uff09\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1) \u5bf9\u89e3\u91ca\u6027\u63a8\u7406\u80fd\u529b\u652f\u6301\u4e0d\u8db3\uff1b2) \u751f\u6210\u7684\u6570\u636e\u96c6\u8fc7\u5927\u4e14\u5197\u4f59\uff0c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u95ee\u7b54\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u9a71\u52a8\u8fc7\u7a0b\uff0c\u5bf9\u6bd4\u5f85\u9002\u5e94\u6a21\u578b\u4e0e\u57fa\u4e8e\u53c2\u8003\u6587\u6863\u7684\u4e13\u5bb6\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u6311\u6218\u6027\u95ee\u9898\uff0c\u63ed\u793a\u5e76\u89e3\u51b3\u7406\u89e3\u5dee\u8ddd\u3002", "result": "\u5728LegalBench\u8bed\u6599\u5e93\u7684\u4e13\u4e1a\u5b50\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u5408\u6210\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u6311\u6218\u6027\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u9002\u5e94\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.17951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17951", "abs": "https://arxiv.org/abs/2602.17951", "authors": ["Guoheng Sun", "Tingting Du", "Kaixi Feng", "Chenxiang Luo", "Xingguo Ding", "Zheyu Shen", "Ziyao Wang", "Yexiao He", "Ang Li"], "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, na\u00efve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.", "AI": {"tldr": "ROCKET\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u5668\u5c06VLA\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u4e0e3D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684Vision-Language-Action\u6a21\u578b\u901a\u5e38\u57282D\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u8868\u793a\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u5355\u5c42\u8fdb\u884c\u76d1\u7763\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u4fe1\u606f\uff0c\u800c\u7b80\u5355\u7684\u591a\u5c42\u5bf9\u9f50\u4f1a\u5bfc\u81f4\u68af\u5ea6\u5e72\u6270\u3002", "method": "\u63d0\u51faROCKET\u6846\u67b6\uff1a1\uff09\u5c06\u591a\u5c42\u5bf9\u9f50\u5f62\u5f0f\u5316\u4e3a\u5c06\u4e00\u4e2a\u6b8b\u5dee\u6d41\u5bf9\u9f50\u5230\u53e6\u4e00\u4e2a\u6b8b\u5dee\u6d41\uff1b2\uff09\u4f7f\u7528\u5171\u4eab\u6295\u5f71\u5668\u901a\u8fc7\u5c42\u4e0d\u53d8\u6620\u5c04\u5c06VLA\u9aa8\u5e72\u7684\u591a\u4e2a\u5c42\u4e0e\u5f3a\u5927\u76843D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u5bf9\u9f50\uff1b3\uff09\u63d0\u51faMatryoshka\u98ce\u683c\u7684\u7a00\u758f\u6fc0\u6d3b\u65b9\u6848\u6765\u5e73\u8861\u591a\u4e2a\u5bf9\u9f50\u635f\u5931\uff1b4\uff09\u7ed3\u5408\u514d\u8bad\u7ec3\u5c42\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728LIBERO\u6570\u636e\u96c6\u4e0a\u4ec5\u9700\u7ea64%\u7684\u8ba1\u7b97\u9884\u7b97\u5c31\u8fbe\u523098.5%\u7684SOTA\u6210\u529f\u7387\uff1b\u5728LIBERO-Plus\u548cRoboTwin\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff1b\u9002\u7528\u4e8e\u591a\u79cdVLA\u6a21\u578b\u3002", "conclusion": "ROCKET\u901a\u8fc7\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u7684\u95ee\u9898\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18145", "abs": "https://arxiv.org/abs/2602.18145", "authors": ["Siya Qi", "Yudong Chen", "Runcong Zhao", "Qinglin Zhu", "Zhanghao Hu", "Wei Liu", "Yulan He", "Zheng Yuan", "Lin Gui"], "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention", "comment": "25 pages, 10 figures", "summary": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u5206\u6790\u7684\u6ce8\u610f\u529b\u673a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6ce8\u610f\u529b\u5206\u5e03\u4e2d\u7684\u9ad8\u9891\u6210\u5206\u6765\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u751f\u6210\u4e2d\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u6ce8\u610f\u529b\u6458\u8981\uff0c\u65e0\u6cd5\u6355\u6349\u6ce8\u610f\u529b\u4e2d\u7684\u7ec6\u5fae\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u786e\u4fdd\u6a21\u578b\u751f\u6210\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u4fe1\u53f7\u5904\u7406\u89c6\u89d2\uff0c\u5c06\u6ce8\u610f\u529b\u5206\u5e03\u5efa\u6a21\u4e3a\u79bb\u6563\u4fe1\u53f7\uff0c\u63d0\u53d6\u53cd\u6620\u6ce8\u610f\u529b\u5feb\u901f\u5c40\u90e8\u53d8\u5316\u7684\u9ad8\u9891\u6210\u5206\u3002\u57fa\u4e8e\u9ad8\u9891\u6ce8\u610f\u529b\u7279\u5f81\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u5728RAGTruth\u548cHalluRAG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u9a8c\u8bc1\u3001\u5185\u90e8\u8868\u793a\u548c\u4f20\u7edf\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6ce8\u610f\u529b\u5206\u5e03\u4e2d\u7684\u9ad8\u9891\u6210\u5206\u80fd\u6709\u6548\u53cd\u6620\u5e7b\u89c9\u76f8\u5173\u7684\u788e\u7247\u5316\u548c\u4e0d\u7a33\u5b9a\u63a5\u5730\u884c\u4e3a\uff0c\u57fa\u4e8e\u9891\u7387\u5206\u6790\u7684\u6ce8\u610f\u529b\u7279\u5f81\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18000", "abs": "https://arxiv.org/abs/2602.18000", "authors": ["Xuting Lan", "Mingliang Zhou", "Xuekai Wei", "Jielu Yan", "Yueting Huang", "Huayan Pu", "Jun Luo", "Weijia Jia"], "title": "Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching", "comment": null, "summary": "Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bb0\u5fc6\u9a71\u52a8\u7684\u8d28\u91cf\u611f\u77e5\u6846\u67b6MQAF\uff0c\u901a\u8fc7\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u5728\u6709\u65e0\u53c2\u8003\u56fe\u50cf\u65f6\u52a8\u6001\u5207\u6362\u53cc\u6a21\u5f0f\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5728\u7406\u60f3\u53c2\u8003\u6e90\u4e0d\u53ef\u5f97\u7684\u5b9e\u9645\u5e94\u7528\u3002\u53d7\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u79ef\u7d2f\u89c6\u89c9\u8bb0\u5fc6\u80fd\u529b\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5efa\u7acb\u7c7b\u4f3c\u751f\u7269\u8bb0\u5fc6\u673a\u5236\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u8bb0\u5fc6\u9a71\u52a8\u7684\u8d28\u91cf\u611f\u77e5\u6846\u67b6MQAF\uff0c\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u91c7\u7528\u53cc\u6a21\u5f0f\u7b56\u7565\uff1a\u6709\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u53c2\u8003\u4fe1\u606f\u5e76\u4e0e\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u6bd4\u8f83\uff1b\u65e0\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u4f9d\u8d56\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u63a8\u65ad\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u9002\u5e94\u65e0\u53c2\u8003\u548c\u5168\u53c2\u8003\u4e24\u79cd\u4efb\u52a1\u3002", "conclusion": "MQAF\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u5728\u6709\u65e0\u53c2\u8003\u56fe\u50cf\u60c5\u51b5\u4e0b\u7684\u7075\u6d3b\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.18152", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18152", "abs": "https://arxiv.org/abs/2602.18152", "authors": ["Ortal Hadad", "Edoardo Loru", "Jacopo Nudo", "Niccol\u00f2 Di Marco", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "The Statistical Signature of LLMs", "comment": null, "summary": "Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u65e0\u635f\u538b\u7f29\u5206\u6790\u53d1\u73b0LLM\u751f\u6210\u6587\u672c\u6bd4\u4eba\u7c7b\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u548c\u53ef\u538b\u7f29\u6027\uff0c\u63ed\u793a\u4e86\u6982\u7387\u751f\u6210\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u4f46\u5728\u5c0f\u5c3a\u5ea6\u4ea4\u4e92\u73af\u5883\u4e2d\u8fd9\u79cd\u5dee\u5f02\u51cf\u5f31\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6982\u7387\u91c7\u6837\u751f\u6210\u6587\u672c\u65f6\uff0c\u5982\u4f55\u91cd\u5851\u8bed\u8a00\u7684\u7ed3\u6784\u7edf\u8ba1\u7ec4\u7ec7\u3002\u76ee\u524d\u5bf9\u8fd9\u4e00\u8fc7\u7a0b\u5982\u4f55\u6539\u53d8\u6587\u672c\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\u7f3a\u4e4f\u5b8c\u6574\u7684\u7279\u5f81\u63cf\u8ff0\u3002", "method": "\u4f7f\u7528\u65e0\u635f\u538b\u7f29\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u4e09\u79cd\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\uff1a\u63a7\u5236\u7684\u4eba\u7c7b-LLM\u5ef6\u7eed\u3001\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u751f\u6210\u4e2d\u4ecb\uff08\u7ef4\u57fa\u767e\u79d1 vs Grokipedia\uff09\u3001\u5b8c\u5168\u5408\u6210\u7684\u793e\u4ea4\u4e92\u52a8\u73af\u5883\uff08Moltbook vs Reddit\uff09\u3002", "result": "\u538b\u7f29\u5206\u6790\u63ed\u793a\u4e86\u6982\u7387\u751f\u6210\u7684\u6301\u4e45\u7ed3\u6784\u7279\u5f81\uff1a\u5728\u63a7\u5236\u548c\u4e2d\u4ecb\u73af\u5883\u4e2d\uff0cLLM\u751f\u6210\u7684\u8bed\u8a00\u6bd4\u4eba\u7c7b\u6587\u672c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u548c\u53ef\u538b\u7f29\u6027\uff0c\u8868\u660e\u8f93\u51fa\u96c6\u4e2d\u5728\u9ad8\u5ea6\u5faa\u73af\u7684\u7edf\u8ba1\u6a21\u5f0f\u4e2d\u3002\u4f46\u5728\u788e\u7247\u5316\u4e92\u52a8\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u5206\u79bb\u51cf\u5f31\uff0c\u8868\u660e\u5728\u5c0f\u5c3a\u5ea6\u4e0a\u8868\u9762\u53ef\u533a\u5206\u6027\u5b58\u5728\u57fa\u672c\u9650\u5236\u3002", "conclusion": "\u65e0\u635f\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u7a33\u5065\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u751f\u6210\u7cfb\u7edf\u5982\u4f55\u91cd\u5851\u6587\u672c\u751f\u4ea7\uff0c\u4e3a\u901a\u4fe1\u7684\u6f14\u5316\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u89c6\u89d2\u3002\u8fd9\u79cd\u57fa\u4e8e\u53ef\u538b\u7f29\u6027\u7684\u5206\u79bb\u5728\u4e0d\u540c\u6a21\u578b\u3001\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u4e00\u81f4\u51fa\u73b0\uff0c\u53ef\u76f4\u63a5\u4ece\u8868\u9762\u6587\u672c\u89c2\u5bdf\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u6216\u8bed\u4e49\u8bc4\u4f30\u3002"}}
{"id": "2602.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18006", "abs": "https://arxiv.org/abs/2602.18006", "authors": ["Ahsan Baidar Bakht", "Mohamad Alansari", "Muhayy Ud Din", "Muzammal Naseer", "Sajid Javed", "Irfan Hussain", "Jiri Matas", "Arif Mahmood"], "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method", "comment": null, "summary": "Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4f2a\u591a\u6a21\u6001\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6MUOT_3M\uff08300\u4e07\u5e27\uff09\u548c\u57fa\u4e8eSAM\u7684\u591a\u6a21\u6001\u5230\u5355\u6a21\u6001\u8ddf\u8e2a\u5668MUTrack\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u5bf9\u6d77\u6d0b\u673a\u5668\u4eba\u3001\u751f\u6001\u76d1\u6d4b\u548c\u6d77\u6d0b\u63a2\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u89c4\u6a21\u5c0f\u4e14\u4ec5\u9650RGB\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5728\u989c\u8272\u5931\u771f\u3001\u6d51\u6d4a\u548c\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "1) \u6784\u5efaMUOT_3M\u57fa\u51c6\uff1a\u5305\u542b300\u4e07\u5e27\u30013030\u4e2a\u89c6\u9891\uff0c\u5177\u6709RGB\u3001\u589e\u5f3aRGB\u3001\u6df1\u5ea6\u548c\u8bed\u8a00\u56db\u79cd\u6a21\u6001\uff1b2) \u63d0\u51faMUTrack\u8ddf\u8e2a\u5668\uff1a\u57fa\u4e8eSAM\u67b6\u6784\uff0c\u91c7\u7528\u89c6\u89c9\u51e0\u4f55\u5bf9\u9f50\u3001\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u548c\u56db\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u591a\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u5230\u5355\u6a21\u6001\u5b66\u751f\u6a21\u578b\u3002", "result": "MUTrack\u5728\u4e94\u4e2aUOT\u57fa\u51c6\u4e0a\u6bd4\u6700\u5f3aSOTA\u57fa\u7ebf\u9ad8\u51fa8.40% AUC\u548c7.80%\u7cbe\u5ea6\uff0c\u8fd0\u884c\u901f\u5ea6\u8fbe24 FPS\u3002MUOT_3M\u57fa\u51c6\u5305\u542b32\u4e2a\u8ddf\u8e2a\u5c5e\u6027\u3001677\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\u3002", "conclusion": "MUOT_3M\u548cMUTrack\u4e3a\u53ef\u6269\u5c55\u3001\u591a\u6a21\u6001\u8bad\u7ec3\u4f46\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u6c34\u4e0b\u8ddf\u8e2a\u5efa\u7acb\u4e86\u65b0\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u6001\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2602.18154", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18154", "abs": "https://arxiv.org/abs/2602.18154", "authors": ["Mirae Kim", "Seonghun Jeong", "Youngjun Kwak"], "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset", "comment": "lrec 2026 accepted paper", "summary": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.", "AI": {"tldr": "FENCE\u662f\u4e00\u4e2a\u7528\u4e8e\u91d1\u878d\u9886\u57df\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u7684\u53cc\u8bed\uff08\u97e9\u8bed-\u82f1\u8bed\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b\u91d1\u878d\u76f8\u5173\u67e5\u8be2\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u5a01\u80c1\uff0c\u80fd\u6709\u6548\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u8fbe\u523099%\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u7b49\u654f\u611f\u9886\u57df\u90e8\u7f72\u65f6\u9762\u4e34\u8d8a\u72f1\u653b\u51fb\u98ce\u9669\uff0c\u7279\u522b\u662f\u591a\u6a21\u6001\u6a21\u578b\u56e0\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u800c\u653b\u51fb\u9762\u66f4\u5e7f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u91d1\u878d\u9886\u57df\u7684\u8d8a\u72f1\u68c0\u6d4b\u8d44\u6e90\u3002", "method": "\u521b\u5efaFENCE\u53cc\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u91d1\u878d\u76f8\u5173\u67e5\u8be2\u548c\u56fe\u50cf\u5a01\u80c1\uff1b\u5728\u5546\u4e1a\u548c\u5f00\u6e90VLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u6f0f\u6d1e\uff1b\u57fa\u4e8eFENCE\u8bad\u7ec3\u57fa\u7ebf\u68c0\u6d4b\u5668\u5e76\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGPT-4o\u5b58\u5728\u53ef\u6d4b\u91cf\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5f00\u6e90\u6a21\u578b\u66b4\u9732\u66f4\u5927\u98ce\u9669\uff1b\u57fa\u4e8eFENCE\u8bad\u7ec3\u7684\u57fa\u7ebf\u68c0\u6d4b\u5668\u8fbe\u523099%\u5206\u5e03\u5185\u51c6\u786e\u7387\uff0c\u5728\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "FENCE\u4e3a\u91d1\u878d\u9886\u57df\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e13\u6ce8\u8d44\u6e90\uff0c\u652f\u6301\u5728\u654f\u611f\u9886\u57df\u6784\u5efa\u66f4\u5b89\u5168\u53ef\u9760\u7684AI\u7cfb\u7edf\uff0c\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u53ef\u9760\u68c0\u6d4b\u6a21\u578b\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.18016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18016", "abs": "https://arxiv.org/abs/2602.18016", "authors": ["Jiamin Luo", "Xuqian Gu", "Jingjing Wang", "Jiahong Lu"], "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating", "comment": null, "summary": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.", "AI": {"tldr": "\u63d0\u51faL-AVC\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u7f16\u8f91\u56fe\u50cf\u7684\u4e3b\u89c2\u60c5\u611f\uff0c\u5e76\u63d0\u51faEPEM\u65b9\u6cd5\uff0c\u5305\u542bEIC\u6a21\u5757\u9ad8\u6548\u5bf9\u9f50\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\u548cPER\u6a21\u5757\u7cbe\u786e\u4fdd\u7559\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u5236\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u8bed\u8a00\u3001\u5e03\u5c40\u3001\u8fb9\u7f18\u68c0\u6d4b\uff09\u4e0e\u7f16\u8f91\u56fe\u50cf\u7684\u5ba2\u89c2\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u4e3b\u89c2\u60c5\u611f\u5185\u5bb9\uff0c\u4e14\u7f3a\u4e4f\u901a\u7528\u7684\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faEPEM\u65b9\u6cd5\uff1a1) EIC\u6a21\u5757\u4f7fLLM\u5728\u7f16\u8f91\u524d\u540e\u9ad8\u6548\u5bf9\u9f50\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\uff1b2) PER\u6a21\u5757\u7cbe\u786e\u4fdd\u7559\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u3002\u5728\u6784\u5efa\u7684L-AVC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728L-AVC\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cEPEM\u65b9\u6cd5\u5728L-AVC\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u60c5\u611f\u4fe1\u606f\u5bf9L-AVC\u7684\u91cd\u8981\u6027\u4ee5\u53caEPEM\u5728\u9ad8\u6548\u7cbe\u786e\u64cd\u7eb5\u60c5\u611f\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684L-AVC\u4efb\u52a1\u548cEPEM\u65b9\u6cd5\u586b\u8865\u4e86\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u9886\u57df\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\u548c\u7cbe\u786e\u7684\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u4fdd\u7559\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u4e3b\u89c2\u60c5\u611f\u7684\u6709\u6548\u7f16\u8f91\u3002"}}
{"id": "2602.18171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18171", "abs": "https://arxiv.org/abs/2602.18171", "authors": ["Wojciech Michaluk", "Tymoteusz Urban", "Mateusz Kubita", "Soveatin Kuntur", "Anna Wroblewska"], "title": "Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models", "comment": null, "summary": "Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408Transformer\u6587\u672c\u5d4c\u5165\u4e0e\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u65b9\u6cd5\u68c0\u6d4b\u70b9\u51fb\u8bf1\u9975\u6807\u9898\uff0cXGBoost\u6a21\u578b\u5728\u589e\u5f3a\u7279\u5f81\u4e0a\u8fbe\u523091% F1\u5206\u6570\uff0c\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u70b9\u51fb\u8bf1\u9975\u6807\u9898\u964d\u4f4e\u5728\u7ebf\u4fe1\u606f\u8d28\u91cf\u5e76\u635f\u5bb3\u7528\u6237\u4fe1\u4efb\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898", "method": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408Transformer\u6587\u672c\u5d4c\u5165\u548c\u8bed\u8a00\u5b66\u4fe1\u606f\u7279\u5f81\uff0c\u4f7f\u7528NLP\u6280\u672f\u8bc4\u4f30\u591a\u79cd\u5411\u91cf\u5316\u65b9\u6cd5\uff0c\u6700\u7ec8\u91c7\u7528XGBoost\u5206\u7c7b\u5668\u5904\u7406\u589e\u5f3a\u7279\u5f81", "result": "\u6700\u4f73\u6a21\u578b\uff08XGBoost\u7ed3\u5408\u589e\u5f3a\u7279\u5f81\uff09F1\u5206\u6570\u8fbe91%\uff0c\u4f18\u4e8eTF-IDF\u3001Word2Vec\u3001GloVe\u3001LLM\u63d0\u793a\u5206\u7c7b\u548c\u7eaf\u7279\u5f81\u57fa\u7ebf", "conclusion": "\u63d0\u51fa\u7684\u7279\u5f81\u96c6\u901a\u8fc7\u7a81\u51fa\u7b2c\u4e8c\u4eba\u79f0\u4ee3\u8bcd\u3001\u6700\u9ad8\u7ea7\u3001\u6570\u5b57\u548c\u6ce8\u610f\u529b\u5bfc\u5411\u6807\u70b9\u7b49\u8bed\u8a00\u5b66\u7ebf\u7d22\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u900f\u660e\u4e14\u6821\u51c6\u826f\u597d\u7684\u70b9\u51fb\u8bf1\u9975\u9884\u6d4b"}}
{"id": "2602.18019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18019", "abs": "https://arxiv.org/abs/2602.18019", "authors": ["Yujie Jin", "Wenxin Zhang", "Jingjing Wang", "Guodong Zhou"], "title": "DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE", "comment": null, "summary": "In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\uff08DeepSVU\uff09\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u4e0d\u4ec5\u8bc6\u522b\u5a01\u80c1\uff0c\u8fd8\u8981\u5f52\u56e0\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\uff08UPRM\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5a01\u80c1\uff0c\u4f46\u7f3a\u4e4f\u751f\u6210\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\u7684\u80fd\u529b\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u66f4\u6df1\u5165\u7684DeepSVU\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\uff08UPRM\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7edf\u4e00\u7269\u7406\u4e16\u754c\u589e\u5f3aMoE\uff08UPE\uff09\u5757\u7528\u4e8e\u5efa\u6a21\u7c97\u5230\u7ec6\u7684\u7269\u7406\u4e16\u754c\u4fe1\u606f\uff0c\u7269\u7406\u4e16\u754c\u6743\u8861\u6b63\u5219\u5316\u5668\uff08PTR\uff09\u7528\u4e8e\u81ea\u9002\u5e94\u6743\u8861\u8fd9\u4e9b\u56e0\u7d20\u3002", "result": "\u5728DeepSVU\u6307\u4ee4\u6570\u636e\u96c6\uff08UCF-C\u6307\u4ee4\u548cCUVA\u6307\u4ee4\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUPRM\u4f18\u4e8e\u591a\u4e2a\u5148\u8fdb\u7684\u89c6\u9891LLM\u548c\u975eVLM\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7269\u7406\u4e16\u754c\u4fe1\u606f\u7684\u91cd\u8981\u6027\u4ee5\u53caUPRM\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7c97\u5230\u7ec6\u7684\u7269\u7406\u4e16\u754c\u4fe1\u606f\u5bf9DeepSVU\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684UPRM\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u6b64\u7c7b\u4fe1\u606f\uff0c\u4e3a\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18176", "abs": "https://arxiv.org/abs/2602.18176", "authors": ["Kaisen Yang", "Jayden Teoh", "Kaicheng Yang", "Yitong Zhang", "Alex Lamb"], "title": "Improving Sampling for Masked Diffusion Models via Information Gain", "comment": "https://github.com/yks23/Information-Gain-Sampler", "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.", "AI": {"tldr": "\u63d0\u51faInfo-Gain Sampler\uff0c\u4e00\u79cd\u7528\u4e8e\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u65b0\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u5373\u65f6\u4e0d\u786e\u5b9a\u6027\u548c\u672a\u6765\u4fe1\u606f\u589e\u76ca\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u7801\u5668\u91c7\u7528\u8d2a\u5fc3\u7b56\u7565\uff0c\u53ea\u5173\u6ce8\u5c40\u90e8\u786e\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u5f53\u524d\u89e3\u7801\u51b3\u7b56\u5bf9\u540e\u7eed\u6b65\u9aa4\u7684\u5f71\u54cd\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528MDMs\u7684\u975e\u56e0\u679c\u7279\u6027\u6765\u6700\u5c0f\u5316\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u589e\u76ca\u91c7\u6837\u5668\uff0c\u8fd9\u662f\u4e00\u4e2a\u539f\u5219\u6027\u7684\u89e3\u7801\u6846\u67b6\uff0c\u4e0d\u4ec5\u8003\u8651\u5f53\u524d\u4f4d\u7f6e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u8bc4\u4f30\u5f53\u524d\u89e3\u7801\u51b3\u7b56\u5982\u4f55\u91cd\u5851\u6240\u6709\u5269\u4f59\u63a9\u7801\u4f4d\u7f6e\u7684token\u6982\u7387/\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5e73\u8861\u5373\u65f6\u4e0d\u786e\u5b9a\u6027\u548c\u672a\u6765\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5728\u63a8\u7406\u3001\u7f16\u7801\u3001\u521b\u610f\u5199\u4f5c\u548c\u56fe\u50cf\u751f\u6210\u7b49\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e0a\uff0cInfo-Gain Sampler\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u91c7\u6837\u5668\uff1a\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.6%\uff0c\u521b\u610f\u5199\u4f5c\u80dc\u738763.1%\uff0c\u63a8\u7406\u4efb\u52a1\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u4ece78.4\u964d\u81f348.6\u3002", "conclusion": "Info-Gain Sampler\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u8003\u8651\u89e3\u7801\u51b3\u7b56\u7684\u672a\u6765\u5f71\u54cd\uff0c\u5145\u5206\u5229\u7528\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u975e\u56e0\u679c\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.18020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18020", "abs": "https://arxiv.org/abs/2602.18020", "authors": ["Jiabing Yang", "Yixiang Chen", "Yuan Xu", "Peiyan Li", "Xiangnan Wu", "Zichen Wen", "Bowen Fang", "Tao Yu", "Zhengbo Zhang", "Yingda Li", "Kai Wang", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.", "AI": {"tldr": "\u63d0\u51faUAOR\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c2\u6d4b\u91cd\u6ce8\u5165\uff09\u6a21\u5757\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347VLA\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u68c0\u7d22\u5728\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\u5c06\u89c2\u6d4b\u4fe1\u606f\u91cd\u6ce8\u5165\u5230\u4e0b\u4e00\u5c42FFN\u4e2d\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u89c2\u6d4b\u7ebf\u7d22\uff08\u5982\u6df1\u5ea6\u56fe\u3001\u70b9\u4e91\uff09\u6216\u8f85\u52a9\u6a21\u5757\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u5668\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6570\u636e\u6536\u96c6\u548c\u989d\u5916\u8bad\u7ec3\u3002\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\u6765\u63d0\u5347VLA\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faUAOR\u6a21\u5757\uff0c\u5f53\u8bed\u8a00\u6a21\u578b\u5c42\u8868\u73b0\u51fa\u9ad8\u4e0d\u786e\u5b9a\u6027\uff08\u901a\u8fc7\u52a8\u4f5c\u71b5\u8861\u91cf\uff09\u65f6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u68c0\u7d22\u5c06\u5173\u952e\u89c2\u6d4b\u4fe1\u606f\u91cd\u6ce8\u5165\u5230\u4e0b\u4e00\u5c42\u7684FFN\u4e2d\u3002\u8fd9\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u3001\u5373\u63d2\u5373\u7528\u7684\u673a\u5236\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u5404\u79cdVLA\u6a21\u578b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002\u7279\u522b\u5730\uff0cUAOR\u65e0\u9700\u989d\u5916\u89c2\u6d4b\u7ebf\u7d22\u6216\u6a21\u5757\uff0c\u6210\u4e3a\u73b0\u6709VLA\u6d41\u7a0b\u7684\u901a\u7528\u5b9e\u7528\u63d2\u4ef6\u3002", "conclusion": "UAOR\u662f\u4e00\u79cd\u6709\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684VLA\u6a21\u578b\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89c2\u6d4b\u91cd\u6ce8\u5165\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u5173\u6ce8\u89c2\u6d4b\u4fe1\u606f\uff0c\u751f\u6210\u66f4\u81ea\u4fe1\u548c\u53ef\u9760\u7684\u52a8\u4f5c\u3002"}}
{"id": "2602.18217", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18217", "abs": "https://arxiv.org/abs/2602.18217", "authors": ["Kohei Kajikawa", "Shinnosuke Isono", "Ethan Gotlieb Wilcox"], "title": "Information-Theoretic Storage Cost in Sentence Comprehension", "comment": null, "summary": "Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u8fde\u7eed\u5904\u7406\u5b58\u50a8\u6210\u672c\u5ea6\u91cf\uff0c\u66ff\u4ee3\u4f20\u7edf\u79bb\u6563\u8bed\u6cd5\u5ea6\u91cf\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\uff0c\u5728\u82f1\u8bed\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u5b9e\u65f6\u53e5\u5b50\u7406\u89e3\u5bf9\u5de5\u4f5c\u8bb0\u5fc6\u8d1f\u8377\u5927\uff0c\u73b0\u6709\u57fa\u4e8e\u7b26\u53f7\u8bed\u6cd5\u7684\u5ea6\u91cf\u91c7\u7528\u79bb\u6563\u7edf\u4e00\u6210\u672c\uff0c\u9700\u8981\u66f4\u8fde\u7eed\u3001\u7406\u8bba\u4e2d\u7acb\u7684\u5ea6\u91cf\u65b9\u6cd5", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u5f62\u5f0f\u5316\u5904\u7406\u5b58\u50a8\u6210\u672c\uff0c\u5b9a\u4e49\u4e3a\u524d\u6587\u5bf9\u672a\u6765\u4e0a\u4e0b\u6587\u643a\u5e26\u7684\u4fe1\u606f\u91cf\uff08\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff09\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1", "result": "\u5728\u82f1\u8bed\u4e2d\u9a8c\u8bc1\uff1a1)\u6062\u590d\u4e2d\u5fc3\u5d4c\u5165\u548c\u5173\u7cfb\u4ece\u53e5\u7684\u5904\u7406\u4e0d\u5bf9\u79f0\u6027\uff1b2)\u4e0e\u8bed\u6cd5\u57fa\u5b58\u50a8\u6210\u672c\u76f8\u5173\uff1b3)\u5728\u81ea\u7136\u6570\u636e\u96c6\u4e0a\u9884\u6d4b\u9605\u8bfb\u65f6\u95f4\u65b9\u5dee\u4f18\u4e8e\u4f20\u7edf\u4fe1\u606f\u57fa\u9884\u6d4b\u5668", "conclusion": "\u4fe1\u606f\u8bba\u5b58\u50a8\u6210\u672c\u5ea6\u91cf\u662f\u8fde\u7eed\u3001\u7406\u8bba\u4e2d\u7acb\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u66ff\u4ee3\u4f20\u7edf\u79bb\u6563\u8bed\u6cd5\u5ea6\u91cf\uff0c\u4e3a\u5fc3\u7406\u8bed\u8a00\u5b66\u7406\u8bba\u63d0\u4f9b\u65b0\u5de5\u5177"}}
{"id": "2602.18022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18022", "abs": "https://arxiv.org/abs/2602.18022", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers", "comment": null, "summary": "Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(\u03b4_k, \u03b4_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).", "AI": {"tldr": "\u63d0\u51faDual-Channel Attention Guidance (DCAG)\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u64cd\u7eb5DiT\u4e2d\u7684Key\u548cValue\u901a\u9053\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\uff0c\u76f8\u6bd4\u4ec5\u64cd\u4f5cKey\u7684\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u5e73\u8861\u7f16\u8f91\u6548\u679c\u4e0e\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDiT\u7684\u6269\u6563\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u9700\u8981\u65e0\u8bad\u7ec3\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002\u5f53\u524d\u6ce8\u610f\u529b\u64cd\u7eb5\u65b9\u6cd5\u4ec5\u5173\u6ce8Key\u7a7a\u95f4\u6765\u8c03\u8282\u6ce8\u610f\u529b\u8def\u7531\uff0c\u5b8c\u5168\u5ffd\u7565\u4e86\u63a7\u5236\u7279\u5f81\u805a\u5408\u7684Value\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u6548\u679c\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u9996\u5148\u53d1\u73b0DiT\u591a\u6a21\u6001\u6ce8\u610f\u529b\u5c42\u4e2d\u7684Key\u548cValue\u6295\u5f71\u90fd\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u7f6e-\u589e\u91cf\u7ed3\u6784\u3002\u57fa\u4e8e\u6b64\u63d0\u51faDCAG\u6846\u67b6\uff0c\u540c\u65f6\u64cd\u7eb5Key\u901a\u9053\uff08\u63a7\u5236\u6ce8\u610f\u529b\u4f4d\u7f6e\uff09\u548cValue\u901a\u9053\uff08\u63a7\u5236\u7279\u5f81\u805a\u5408\uff09\u3002\u7406\u8bba\u5206\u6790\u8868\u660eKey\u901a\u9053\u901a\u8fc7\u975e\u7ebf\u6027softmax\u51fd\u6570\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u63a7\u5236\uff0cValue\u901a\u9053\u901a\u8fc7\u7ebf\u6027\u52a0\u6743\u6c42\u548c\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u8865\u5145\u3002", "result": "\u5728PIE-Bench\u57fa\u51c6\u6d4b\u8bd5\uff08700\u5f20\u56fe\u50cf\uff0c10\u4e2a\u7f16\u8f91\u7c7b\u522b\uff09\u4e0a\uff0cDCAG\u5728\u6240\u6709\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4ec5\u4f7f\u7528Key\u5f15\u5bfc\u7684\u65b9\u6cd5\u3002\u5728\u5c40\u90e8\u7f16\u8f91\u4efb\u52a1\u4e2d\u6539\u8fdb\u6700\u663e\u8457\uff1a\u5bf9\u8c61\u5220\u9664\uff08LPIPS\u51cf\u5c114.9%\uff09\u548c\u5bf9\u8c61\u6dfb\u52a0\uff08LPIPS\u51cf\u5c113.2%\uff09\u3002", "conclusion": "DCAG\u901a\u8fc7\u540c\u65f6\u64cd\u7eb5Key\u548cValue\u901a\u9053\uff0c\u5728\u4e8c\u7ef4\u53c2\u6570\u7a7a\u95f4(\u03b4_k, \u03b4_v)\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u5355\u901a\u9053\u65b9\u6cd5\u66f4\u7cbe\u786e\u7684\u7f16\u8f91-\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u4e3a\u57fa\u4e8eDiT\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2602.18232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18232", "abs": "https://arxiv.org/abs/2602.18232", "authors": ["Lexiang Tang", "Weihao Gao", "Bingchen Zhao", "Lu Ma", "Qiao jin", "Bang Yang", "Yuexian Zou"], "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning", "comment": null, "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.", "AI": {"tldr": "\u63d0\u51faConfidence-Driven Contrastive Decoding\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u4f4e\u7f6e\u4fe1\u5ea6token\u5e76\u9009\u62e9\u6027\u5e72\u9884\uff0c\u63d0\u5347LLM\u63a8\u7406\u53ef\u9760\u6027\u540c\u65f6\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u5047\u8bbe\u5747\u5300\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u80fd\u63d0\u5347\u6b63\u786e\u6027\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u5c40\u90e8\u5316\uff1a\u5c11\u6570\u4f4e\u7f6e\u4fe1\u5ea6token\u5bf9\u63a8\u7406\u9519\u8bef\u548c\u4e0d\u5fc5\u8981\u8f93\u51fa\u6269\u5c55\u8d21\u732e\u4e0d\u6210\u6bd4\u4f8b", "method": "\u63d0\u51faThinking by Subtraction\u65b9\u6cd5\uff0c\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u5bf9\u6bd4\u89e3\u7801\uff1a\u68c0\u6d4b\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u4f4e\u7f6e\u4fe1\u5ea6token\uff0c\u5728\u8fd9\u4e9b\u4f4d\u7f6e\u9009\u62e9\u6027\u5e72\u9884\uff1b\u6784\u5efa\u5bf9\u6bd4\u53c2\u8003\uff08\u5c06\u9ad8\u7f6e\u4fe1\u5ea6token\u66ff\u6362\u4e3a\u6700\u5c0f\u5360\u4f4d\u7b26\uff09\uff0c\u5728\u4f4e\u7f6e\u4fe1\u5ea6\u4f4d\u7f6e\u901a\u8fc7\u51cf\u53bb\u53c2\u8003\u5206\u5e03\u6765\u7cbe\u70bc\u9884\u6d4b", "result": "CCD\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\uff0cKV\u7f13\u5b58\u5f00\u9500\u6700\u5c0f\uff1b\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4f4e\u7f6e\u4fe1\u5ea6\u5e72\u9884\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\uff0c\u907f\u514d\u8ba1\u7b97\u5197\u4f59", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u4f4e\u7f6e\u4fe1\u5ea6token\u7684\u5c40\u90e8\u5316\u5e72\u9884\uff0cCCD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f93\u51fa\u6269\u5c55\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u63a8\u7406\u589e\u5f3a\u65b9\u6848"}}
{"id": "2602.18043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18043", "abs": "https://arxiv.org/abs/2602.18043", "authors": ["Hongyu Qu", "Xiangbo Shu", "Rui Yan", "Hailiang Gao", "Wenguan Wang", "Jinhui Tang"], "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition", "comment": "Accepted to TPAMI 2026", "summary": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.", "AI": {"tldr": "DiST\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3-\u878d\u5408\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\u6765\u5b66\u4e60\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u5728\u4e94\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u8bed\u4e49\u7c97\u7cd9\u7684\u7c7b\u522b\u540d\u79f0\u4f5c\u4e3a\u8f85\u52a9\u4e0a\u4e0b\u6587\uff0c\u4f46\u8fd9\u6837\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6709\u9650\uff0c\u65e0\u6cd5\u4e3a\u6355\u6349\u52a8\u4f5c\u4e2d\u7684\u65b0\u9896\u7a7a\u95f4\u548c\u65f6\u95f4\u6982\u5ff5\u63d0\u4f9b\u8db3\u591f\u7684\u80cc\u666f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faDiST\u6846\u67b6\uff1a1\uff09\u5206\u89e3\u9636\u6bb5\uff1a\u5c06\u539f\u59cb\u52a8\u4f5c\u540d\u79f0\u89e3\u8026\u4e3a\u591a\u6837\u5316\u7684\u65f6\u7a7a\u5c5e\u6027\u63cf\u8ff0\uff1b2\uff09\u878d\u5408\u9636\u6bb5\uff1a\u63d0\u51fa\u7a7a\u95f4/\u65f6\u95f4\u77e5\u8bc6\u8865\u507f\u5668\uff08SKC/TKC\uff09\u5206\u522b\u53d1\u73b0\u5224\u522b\u6027\u7684\u5bf9\u8c61\u7ea7\u548c\u5e27\u7ea7\u539f\u578b\uff0cSKC\u5728\u7a7a\u95f4\u77e5\u8bc6\u6307\u5bfc\u4e0b\u81ea\u9002\u5e94\u805a\u5408\u91cd\u8981\u8865\u4e01\u6807\u8bb0\uff0cTKC\u5229\u7528\u65f6\u95f4\u5c5e\u6027\u8f85\u52a9\u5e27\u95f4\u65f6\u5e8f\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\uff0cDiST\u80fd\u591f\u5b66\u4e60\u8868\u8fbe\u6027\u5f3a\u7684\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u4e3a\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u548c\u591a\u6837\u5316\u65f6\u95f4\u6a21\u5f0f\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2602.18262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18262", "abs": "https://arxiv.org/abs/2602.18262", "authors": ["Aaron Louis Eidt", "Nils Feldhus"], "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA", "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA", "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.", "AI": {"tldr": "ELIA\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0fWeb\u5e94\u7528\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5206\u6790\u6280\u672f\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u964d\u4f4e\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u7406\u89e3\u590d\u6742\u7684LLM\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u867d\u7136\u5f00\u53d1\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u6765\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\uff0c\u4f46\u5176\u590d\u6742\u6027\u9020\u6210\u4e86\u53ef\u8bbf\u95ee\u6027\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u5de5\u5177\u53ea\u80fd\u88ab\u4e13\u5bb6\u4f7f\u7528\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u7cfb\u7edf\u6765\u7b80\u5316\u8fd9\u4e9b\u5206\u6790\u7ed3\u679c\uff0c\u8ba9\u66f4\u5e7f\u6cdb\u7684\u53d7\u4f17\u80fd\u591f\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1\u3001\u6784\u5efa\u548c\u8bc4\u4f30ELIA\u7cfb\u7edf\uff0c\u6574\u5408\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a\u5f52\u56e0\u5206\u6790\u3001\u51fd\u6570\u5411\u91cf\u5206\u6790\u548c\u7535\u8def\u8ffd\u8e2a\u3002\u5f15\u5165\u521b\u65b0\u65b9\u6cd5\uff1a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u4ea7\u751f\u7684\u590d\u6742\u53ef\u89c6\u5316\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7684\u7528\u6237\u7814\u7a76\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u7528\u6237\u660e\u663e\u504f\u597d\u4ea4\u4e92\u5f0f\u3001\u53ef\u63a2\u7d22\u7684\u754c\u9762\u800c\u975e\u7b80\u5355\u7684\u9759\u6001\u53ef\u89c6\u5316\u3002AI\u9a71\u52a8\u7684\u89e3\u91ca\u5e2e\u52a9\u975e\u4e13\u5bb6\u5f25\u5408\u4e86\u77e5\u8bc6\u5dee\u8ddd\uff1b\u7edf\u8ba1\u5206\u6790\u663e\u793a\u7528\u6237\u7684\u5148\u9a8cLLM\u7ecf\u9a8c\u4e0e\u5176\u7406\u89e3\u5206\u6570\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u76f8\u5173\u6027\uff0c\u8868\u660e\u7cfb\u7edf\u51cf\u5c11\u4e86\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u7406\u89e3\u969c\u788d\u3002", "conclusion": "AI\u7cfb\u7edf\u786e\u5b9e\u53ef\u4ee5\u7b80\u5316\u590d\u6742\u7684\u6a21\u578b\u5206\u6790\uff0c\u4f46\u5176\u771f\u6b63\u6f5c\u529b\u5728\u4e8e\u4e0e\u6df1\u601d\u719f\u8651\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u4f18\u5148\u8003\u8651\u4ea4\u4e92\u6027\u3001\u7279\u5f02\u6027\u548c\u53d9\u4e8b\u6307\u5bfc\uff0c\u4ece\u800c\u89e3\u9501\u7cfb\u7edf\u7684\u5168\u90e8\u80fd\u529b\u3002"}}
{"id": "2602.18047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18047", "abs": "https://arxiv.org/abs/2602.18047", "authors": ["Rong Fu", "Wenxin Zhang", "Yibo Meng", "Jia Yee Tan", "Jiaxuan Lu", "Rui Lu", "Jiekai Wu", "Zhaolu Kang", "Simon Fong"], "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras", "comment": "36 pages, 12 figures", "summary": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.", "AI": {"tldr": "CityGuard\uff1a\u4e00\u79cd\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u76d1\u63a7\u7684\u62d3\u6251\u611f\u77e5Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6563\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\u3001\u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\u548c\u5dee\u5206\u9690\u79c1\u5d4c\u5165\u6620\u5c04\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u57ce\u5e02\u89c4\u6a21\u884c\u4eba\u91cd\u8bc6\u522b\u3002", "motivation": "\u57ce\u5e02\u89c4\u6a21\u884c\u4eba\u91cd\u8bc6\u522b\u9762\u4e34\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\u7b49\u6311\u6218\uff0c\u540c\u65f6\u9700\u8981\u9075\u5b88\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\uff0c\u9632\u6b62\u539f\u59cb\u56fe\u50cf\u5171\u4eab\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u6709\u6548\u8fdb\u884c\u8eab\u4efd\u68c0\u7d22\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5206\u6563\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\uff1a\u6839\u636e\u7279\u5f81\u5206\u5e03\u8c03\u6574\u5b9e\u4f8b\u7ea7\u8fb9\u754c\uff0c\u589e\u5f3a\u7c7b\u5185\u7d27\u51d1\u6027\uff1b2. \u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\uff1a\u5c06\u7c97\u7565\u51e0\u4f55\u4fe1\u606f\uff08\u5982GPS\u6216\u90e8\u7f72\u5e73\u9762\u56fe\uff09\u6ce8\u5165\u57fa\u4e8e\u56fe\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u6295\u5f71\u4e00\u81f4\u7684\u8de8\u89c6\u89d2\u5bf9\u9f50\uff1b3. \u5dee\u5206\u9690\u79c1\u5d4c\u5165\u6620\u5c04\uff1a\u7ed3\u5408\u7d27\u51d1\u8fd1\u4f3c\u7d22\u5f15\uff0c\u652f\u6301\u5b89\u5168\u4e14\u6210\u672c\u9ad8\u6548\u7684\u90e8\u7f72\u3002", "result": "\u5728Market-1501\u548c\u5176\u4ed6\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u7d22\u7cbe\u5ea6\u548c\u67e5\u8be2\u541e\u5410\u91cf\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6570\u636e\u5e93\u89c4\u6a21\u68c0\u7d22\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "CityGuard\u6846\u67b6\u80fd\u591f\u751f\u6210\u5bf9\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\u5177\u6709\u9c81\u68d2\u6027\u7684\u63cf\u8ff0\u7b26\uff0c\u5e76\u5728\u4e25\u683c\u7684\u5dee\u5206\u9690\u79c1\u6838\u7b97\u4e0b\u5b9e\u73b0\u9690\u79c1\u4e0e\u6548\u7528\u7684\u53ef\u8c03\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u5173\u952e\u7684\u57ce\u5e02\u8eab\u4efd\u5339\u914d\u5e94\u7528\u3002"}}
{"id": "2602.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18324", "abs": "https://arxiv.org/abs/2602.18324", "authors": ["Alexandra Ciobotaru", "Ana-Maria Bucur", "Liviu P. Dinu"], "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus", "comment": "This article was accepted at LREC 2026", "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.", "AI": {"tldr": "\u521b\u5efa\u4e86\u9996\u4e2a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6291\u90c1\u75c7\u548c\u7126\u8651\u75c7\u8bed\u6599\u5e93PsihoRo\uff0c\u5305\u542b205\u540d\u53d7\u8bbf\u8005\u7684\u6587\u672c\u6570\u636e\uff0c\u586b\u8865\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5fc3\u7406\u5065\u5eb7NLP\u8d44\u6e90\u7684\u7a7a\u767d\u3002", "motivation": "\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u76ee\u524d\u6ca1\u6709\u5f00\u6e90\u7684\u5fc3\u7406\u5065\u5eb7\u8bed\u6599\u5e93\uff0c\u800c\u82f1\u8bed\u7b49\u8bed\u8a00\u5df2\u6709\u4e30\u5bcc\u7684\u5fc3\u7406NLP\u8d44\u6e90\u3002\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u4ece\u793e\u4ea4\u5a92\u4f53\u6536\u96c6\u5b58\u5728\u5047\u8bbe\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5305\u542b6\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u8868\u683c\uff0c\u914d\u5408\u6807\u51c6\u5316\u7684PHQ-9\u548cGAD-7\u7b5b\u67e5\u95ee\u5377\u6536\u96c6\u6570\u636e\u3002\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u3001\u7f57\u9a6c\u5c3c\u4e9a\u8bedLIWC\u6587\u672c\u5206\u6790\u3001\u60c5\u611f\u68c0\u6d4b\u548c\u4e3b\u9898\u5efa\u6a21\u6765\u5206\u6790\u8bed\u6599\u7279\u5f81\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b205\u540d\u53d7\u8bbf\u8005\u6587\u672c\u7684PsihoRo\u8bed\u6599\u5e93\uff0c\u867d\u7136\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46\u8fd9\u662f\u5206\u6790\u7f57\u9a6c\u5c3c\u4e9a\u4eba\u53e3\u5fc3\u7406\u5065\u5eb7\u6587\u672c\u7684\u7b2c\u4e00\u6b65\u3002", "conclusion": "PsihoRo\u586b\u8865\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5fc3\u7406\u5065\u5eb7\u8bed\u6599\u5e93\u7684\u7a7a\u767d\uff0c\u4e3aNLP\u793e\u533a\u63d0\u4f9b\u4e86\u9996\u4e2a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6291\u90c1\u75c7\u548c\u7126\u8651\u75c7\u6587\u672c\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u8be5\u8d44\u6e90\u7684\u91cd\u8981\u7279\u5f81\u3002"}}
{"id": "2602.18057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18057", "abs": "https://arxiv.org/abs/2602.18057", "authors": ["Hongsong Wang", "Wenjing Yan", "Qiuxia Lai", "Xin Geng"], "title": "Temporal Consistency-Aware Text-to-Motion Generation", "comment": "Code is on https://github.com/Giat995/TCA-T2M/", "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.", "AI": {"tldr": "TCA-T2M\uff1a\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\u548c\u8fd0\u52a8\u7ea6\u675f\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u5408\u7406\u6027", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u901a\u5e38\u5ffd\u7565\u8de8\u5e8f\u5217\u65f6\u95f4\u4e00\u81f4\u6027\uff08\u5373\u76f8\u540c\u52a8\u4f5c\u5728\u4e0d\u540c\u5b9e\u4f8b\u95f4\u7684\u5171\u4eab\u65f6\u95f4\u7ed3\u6784\uff09\uff0c\u5bfc\u81f4\u8bed\u4e49\u9519\u4f4d\u548c\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u52a8\u4f5c", "method": "\u63d0\u51faTCA-T2M\u6846\u67b6\uff1a1\uff09\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7a7a\u95f4VQ-VAE\u7528\u4e8e\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\uff1b2\uff09\u63a9\u7801\u8fd0\u52a8\u53d8\u6362\u5668\u7528\u4e8e\u6587\u672c\u6761\u4ef6\u52a8\u4f5c\u751f\u6210\uff1b3\uff09\u8fd0\u52a8\u5b66\u7ea6\u675f\u5757\u51cf\u8f7b\u79bb\u6563\u5316\u4f2a\u5f71\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027", "result": "\u5728HumanML3D\u548cKIT-ML\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u5bf9\u9c81\u68d2\u548c\u8fde\u8d2f\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u91cd\u8981\u6027", "conclusion": "TCA-T2M\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u4e3a\u52a8\u4f5c\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.18326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18326", "abs": "https://arxiv.org/abs/2602.18326", "authors": ["Tao Wu", "Adam Kapelner"], "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning", "comment": "8 pages, 3 figures, 4 tables", "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e3a\u9ad8\u4e2d\u751f\u7684\u6bcd\u8bed\u8bcd\u6c47\u6559\u5b66\u81ea\u52a8\u7b5b\u9009\u4f18\u8d28\u4e0a\u4e0b\u6587\u4f8b\u53e5\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807RCC\u66f2\u7ebf\u6765\u53ef\u89c6\u5316\u6a21\u578b\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u4e3a\u9ad8\u4e2d\u751f\u7684\u6bcd\u8bed\u8bcd\u6c47\u6559\u5b66\u81ea\u52a8\u8bc6\u522b\u9ad8\u8d28\u91cf\u7684\u4e0a\u4e0b\u6587\u4f8b\u53e5\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4eba\u5de5\u7b5b\u9009\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u4f4e\u6210\u672c\u7684\u4f18\u8d28\u6559\u5b66\u8d44\u6e90\u4f9b\u7ed9\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff1a(1) \u57fa\u4e8eMPNet\u7edf\u4e00\u4e0a\u4e0b\u6587\u5316\u5d4c\u5165\u7684\u65e0\u76d1\u7763\u76f8\u4f3c\u5ea6\u7b56\u7565\uff1b(2) \u57fa\u4e8e\u6307\u4ee4\u611f\u77e5\u3001\u5fae\u8c03Qwen3\u5d4c\u5165\u548c\u76d1\u7763\u975e\u7ebf\u6027\u56de\u5f52\u5934\u7684\u6846\u67b6\uff1b(3) \u65b9\u6cd5(2)\u52a0\u4e0a\u624b\u5de5\u7279\u5f81\u3002\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\"\u4fdd\u6301\u80fd\u529b\u66f2\u7ebf\"\u6765\u53ef\u89c6\u5316\u6a21\u578b\u6027\u80fd\u6743\u8861\u3002", "result": "\u6a21\u578b(3)\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4ec5\u4e22\u5f0370%\u4f18\u8d28\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86440:1\u7684\u4f18\u8d28-\u52a3\u8d28\u4e0a\u4e0b\u6587\u6bd4\u4f8b\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u8bcd\u6c47\u6559\u5b66\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u73b0\u4ee3\u5d4c\u5165\u6a21\u578b\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u4eba\u7c7b\u76d1\u7763\u6307\u5bfc\u4e0b\uff0c\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u5927\u89c4\u6a21\u751f\u6210\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u8bcd\u6c47\u6559\u5b66\u4e0a\u4e0b\u6587\uff0c\u4e3a\u5404\u79cd\u76ee\u6807\u8bcd\u6c47\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u8d44\u6e90\u3002"}}
{"id": "2602.18089", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18089", "abs": "https://arxiv.org/abs/2602.18089", "authors": ["Kunwar Arpit Singh", "Ankush Prakash", "Haroon R Lone"], "title": "DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text", "comment": null, "summary": "Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.", "AI": {"tldr": "DohaScript\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4e66\u5199\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6570\u636e\u96c6\uff0c\u5305\u542b531\u4f4d\u8d21\u732e\u8005\u4e66\u5199\u7684\u76f8\u540c\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\uff0c\u65e8\u5728\u89e3\u51b3\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u5b57\u624b\u5199\u6587\u672c\u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u5b57\u6709\u6570\u4ebf\u4f7f\u7528\u8005\uff0c\u4f46\u5176\u624b\u5199\u6587\u672c\u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u4e25\u91cd\u4e0d\u8db3\u3002\u73b0\u6709\u8d44\u6e90\u89c4\u6a21\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u5b57\u7b26\u6216\u77ed\u8bcd\uff0c\u7f3a\u4e4f\u53d7\u63a7\u8bcd\u6c47\u5185\u5bb9\u548c\u4e66\u5199\u8005\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5fb7\u74e6\u7eb3\u683c\u91cc\u624b\u5199\u4f53\u8fde\u7eed\u3001\u878d\u5408\u548c\u7ed3\u6784\u590d\u6742\u7684\u7279\u6027\u3002", "method": "\u521b\u5efaDohaScript\u6570\u636e\u96c6\uff0c\u6536\u96c6531\u4f4d\u72ec\u7279\u8d21\u732e\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6587\u672c\u3002\u6570\u636e\u96c6\u8bbe\u8ba1\u4e3a\u5e73\u884c\u98ce\u683c\u8bed\u6599\u5e93\uff0c\u6240\u6709\u4e66\u5199\u8005\u8f6c\u5f55\u76f8\u540c\u7684\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\u3002\u5305\u542b\u53bb\u8bc6\u522b\u5316\u7684\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff0c\u57fa\u4e8e\u5ba2\u89c2\u6e05\u6670\u5ea6\u548c\u5206\u8fa8\u7387\u6807\u51c6\u7684\u4e25\u683c\u8d28\u91cf\u7b5b\u9009\uff0c\u4ee5\u53ca\u9875\u9762\u7ea7\u5e03\u5c40\u96be\u5ea6\u6807\u6ce8\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\u4e86\u6e05\u6670\u7684\u8d28\u91cf\u5206\u79bb\u548c\u5bf9\u672a\u89c1\u4e66\u5199\u8005\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002\u6570\u636e\u96c6\u652f\u6301\u624b\u5199\u8bc6\u522b\u3001\u4e66\u5199\u8005\u8bc6\u522b\u3001\u98ce\u683c\u5206\u6790\u548c\u751f\u6210\u5efa\u6a21\u7b49\u4efb\u52a1\u3002", "conclusion": "DohaScript\u65e8\u5728\u4f5c\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u811a\u672c\u73af\u5883\u4e0b\u8fde\u7eed\u624b\u5199\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u672c\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2602.18064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18064", "abs": "https://arxiv.org/abs/2602.18064", "authors": ["Ziyue Wang", "Linghan Cai", "Chang Han Low", "Haofeng Liu", "Junde Wu", "Jingyu Wang", "Rui Wang", "Lei Song", "Jiang Bian", "Jingjing Fu", "Yueming Jin"], "title": "3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis", "comment": "19 pages, 7 figures", "summary": "3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \\href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.", "AI": {"tldr": "3DMedAgent\uff1a\u4e00\u79cd\u7edf\u4e00\u4ee3\u7406\uff0c\u4f7f2D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6267\u884c\u901a\u75283D CT\u5206\u6790\uff0c\u65e0\u97003D\u7279\u5b9a\u5fae\u8c03\uff0c\u901a\u8fc7\u534f\u8c03\u5f02\u6784\u5de5\u5177\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u5b9e\u73b0\u4ece\u611f\u77e5\u5230\u7406\u89e3\u7684\u6e10\u8fdb\u5206\u6790\u3002", "motivation": "\u73b0\u67093D\u5206\u6790\u65b9\u6cd5\u91c7\u7528\u5b64\u7acb\u7684\u4efb\u52a1\u7279\u5b9a\u5efa\u6a21\u6216\u4efb\u52a1\u65e0\u5173\u7684\u7aef\u5230\u7aef\u8303\u5f0f\uff0c\u963b\u788d\u4e86\u611f\u77e5\u8bc1\u636e\u7684\u7cfb\u7edf\u79ef\u7d2f\u3002\u540c\u65f6\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9762\u54112D\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4f53\u79ef\u533b\u5b66\u6570\u636e\u7684\u611f\u77e5\u548c\u5206\u6790\u80fd\u529b\u3002", "method": "3DMedAgent\u901a\u8fc7\u7075\u6d3b\u7684MLLM\u4ee3\u7406\u534f\u8c03\u5f02\u6784\u89c6\u89c9\u548c\u6587\u672c\u5de5\u5177\uff0c\u5c06\u590d\u67423D\u5206\u6790\u9010\u6b65\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u7684\u5b50\u4efb\u52a1\uff1a\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u89c6\u56fe\u3001\u4ece3D\u4f53\u79ef\u5230\u4fe1\u606f\u4e30\u5bcc\u76842D\u5207\u7247\u3001\u4ece\u89c6\u89c9\u8bc1\u636e\u5230\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u3002\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\u7ef4\u62a4\u957f\u671f\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u805a\u5408\u4e2d\u95f4\u5de5\u5177\u8f93\u51fa\uff0c\u652f\u6301\u67e5\u8be2\u81ea\u9002\u5e94\u3001\u8bc1\u636e\u9a71\u52a8\u7684\u591a\u6b65\u63a8\u7406\u3002", "result": "\u5728\u8d85\u8fc740\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c3DMedAgent\u57283D\u80f8\u90e8\u6210\u50cf\u7684DeepChestVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6301\u7eed\u4f18\u4e8e\u901a\u7528\u3001\u533b\u5b66\u548c3D\u7279\u5b9a\u7684MLLMs\uff0c\u5c55\u793a\u4e86\u5411\u901a\u75283D\u4e34\u5e8a\u52a9\u624b\u6269\u5c55\u7684\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "3DMedAgent\u4e3a2D MLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u97003D\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u6267\u884c\u901a\u75283D CT\u5206\u6790\u7684\u7edf\u4e00\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u534f\u8c03\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u5b9e\u73b0\u4ece\u4f4e\u5c42\u611f\u77e5\u5230\u9ad8\u5c42\u4e34\u5e8a\u7406\u89e3\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u4ee3\u8868\u4e86\u5411\u901a\u75283D\u4e34\u5e8a\u52a9\u624b\u53d1\u5c55\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2602.18346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18346", "abs": "https://arxiv.org/abs/2602.18346", "authors": ["Pavithra PM Nair", "Preethu Rose Anish"], "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System", "comment": null, "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.", "AI": {"tldr": "Vichara\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u89e3\u91ca\u4e0a\u8bc9\u6848\u4ef6\u5224\u51b3\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5904\u7406\u6cd5\u5f8b\u6587\u4ef6\u5e76\u91c7\u7528IRAC\u6846\u67b6\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u5370\u5ea6\u6cd5\u9662\u9762\u4e34\u5927\u91cf\u6848\u4ef6\u79ef\u538b\uff0c\u7279\u522b\u662f\u4e0a\u8bc9\u6848\u4ef6\uff0c\u9700\u8981AI\u6280\u672f\u6765\u5e2e\u52a9\u9884\u6d4b\u5224\u51b3\u5e76\u63d0\u9ad8\u53f8\u6cd5\u6548\u7387\u3002", "method": "Vichara\u6846\u67b6\u5904\u7406\u82f1\u6587\u4e0a\u8bc9\u6848\u4ef6\u6587\u4ef6\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u51b3\u7b56\u70b9\uff08\u5305\u542b\u6cd5\u5f8b\u95ee\u9898\u3001\u51b3\u5b9a\u673a\u6784\u3001\u7ed3\u679c\u3001\u63a8\u7406\u548c\u65f6\u95f4\u80cc\u666f\uff09\uff0c\u91c7\u7528IRAC\u6846\u67b6\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u5e76\u4f7f\u7528GPT-4o mini\u7b49\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728PredEx\u548cILDC_expert\u6570\u636e\u96c6\u4e0a\uff0cVichara\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u51b3\u9884\u6d4b\u57fa\u51c6\uff0cGPT-4o mini\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u6570\uff1aPredEx 81.5\uff0cILDC_expert 80.3\uff09\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u663e\u793a\u5176\u89e3\u91ca\u5728\u6e05\u6670\u5ea6\u3001\u5173\u8054\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Vichara\u6846\u67b6\u4e3a\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5224\u51b3\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u6848\u4ef6\u79ef\u538b\u95ee\u9898\uff0c\u63d0\u9ad8\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u7684\u5de5\u4f5c\u6548\u7387\u3002"}}
{"id": "2602.18066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18066", "abs": "https://arxiv.org/abs/2602.18066", "authors": ["Daniel Busch", "Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Richard Meyes", "Tobias Meisen"], "title": "Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation", "comment": "This Paper has been accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u51cf\u5c11\u5bf9BEV\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u4f7f\u752850%\u6807\u6ce8\u6570\u636e\u8fbe\u5230\u4f18\u4e8e\u5168\u76d1\u7763\u57fa\u7ebf\u7684\u6027\u80fd", "motivation": "\u5f53\u524d\u591a\u6444\u50cf\u5934BEV\u8bed\u4e49\u5730\u56fe\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u6807\u6ce8\u4e0d\u4e00\u81f4\u7684BEV\u5730\u9762\u771f\u503c\uff0c\u9700\u8981\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u9ad8\u53ef\u6269\u5c55\u6027", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1a\u5c06BEVFormer\u9884\u6d4b\u53ef\u5fae\u5206\u91cd\u6295\u5f71\u5230\u56fe\u50cf\u5e73\u9762\uff0c\u4f7f\u7528Mask2Former\u751f\u6210\u7684\u591a\u89c6\u89d2\u8bed\u4e49\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u52a0\u5165\u65f6\u5e8f\u4e00\u81f4\u6027\u635f\u5931\uff1b2) \u76d1\u7763\u5fae\u8c03\uff1a\u4ec5\u970050%\u6570\u636e\u96c6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5b66\u5230\u7684\u4e30\u5bcc\u5148\u9a8c", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u8fbe+2.5pp mIoU\uff0c\u540c\u65f6\u51cf\u5c1150%\u6807\u6ce8\u6570\u636e\u4f7f\u7528\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c", "conclusion": "\u53ef\u5fae\u5206\u91cd\u6295\u5f71\u52a0\u76f8\u673a\u89c6\u89d2\u4f2a\u6807\u7b7e\u80fd\u4ea7\u751f\u53ef\u8fc1\u79fb\u7684BEV\u7279\u5f81\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u53ef\u6269\u5c55\u8def\u5f84"}}
{"id": "2602.18351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18351", "abs": "https://arxiv.org/abs/2602.18351", "authors": ["Jordan Robinson", "Angus R. Williams", "Katie Atkinson", "Anthony G. Cohn"], "title": "Validating Political Position Predictions of Arguments", "comment": "13 pages, 6 figures, 6 tables. Under review", "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $\u03b1=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($\u03b1=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c3a\u5ea6\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u5f0f\u548c\u6210\u5bf9\u6807\u6ce8\uff0c\u7528\u4e8e\u653f\u6cbb\u7acb\u573a\u9884\u6d4b\u7b49\u4e3b\u89c2\u8fde\u7eed\u5c5e\u6027\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u572822\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u653f\u6cbb\u7acb\u573a\u77e5\u8bc6\u5e93", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u8868\u793a\u5e38\u9700\u6355\u6349\u4e3b\u89c2\u8fde\u7eed\u5c5e\u6027\uff08\u5982\u653f\u6cbb\u7acb\u573a\uff09\uff0c\u8fd9\u4e0e\u5e7f\u6cdb\u63a5\u53d7\u7684\u6210\u5bf9\u9a8c\u8bc1\u9ec4\u91d1\u6807\u51c6\u76f8\u51b2\u7a81\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218", "method": "\u91c7\u7528\u53cc\u5c3a\u5ea6\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u5f0f\u548c\u6210\u5bf9\u4eba\u7c7b\u6807\u6ce8\uff0c\u4f7f\u752822\u4e2a\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5305\u542b23,228\u4e2a\u8bba\u70b9\u7684\u5927\u89c4\u6a21\u653f\u6cbb\u7acb\u573a\u9884\u6d4b\u77e5\u8bc6\u5e93\uff0c\u57fa\u4e8e\u82f1\u56fd\u300aQuestion Time\u300b\u8282\u76ee\u768430\u573a\u8fa9\u8bba", "result": "\u70b9\u5f0f\u8bc4\u4f30\u663e\u793a\u4e2d\u7b49\u6c34\u5e73\u7684\u4eba\u6a21\u4e00\u81f4\u6027\uff08\u03b1=0.578\uff09\uff0c\u53cd\u6620\u5185\u5728\u4e3b\u89c2\u6027\uff1b\u6210\u5bf9\u9a8c\u8bc1\u663e\u793a\u4eba\u6a21\u6392\u540d\u5bf9\u9f50\u66f4\u5f3a\uff08\u6700\u4f73\u6a21\u578b\u03b1=0.86\uff09\uff0c\u8bc1\u660e\u53ef\u4ece\u70b9\u5f0f\u9884\u6d4b\u4e2d\u63d0\u53d6\u5e8f\u6570\u7ed3\u6784", "conclusion": "\u8d21\u732e\u5305\u62ec\uff1a\u5b9e\u7528\u7684\u4e3b\u89c2\u8fde\u7eed\u77e5\u8bc6\u9a8c\u8bc1\u65b9\u6cd5\uff1b\u5df2\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u8bba\u8bc1\u77e5\u8bc6\u5e93\uff1b\u8bc1\u660e\u53ef\u4ece\u4e3b\u89c2\u73b0\u5b9e\u8bdd\u8bed\u4e2d\u63d0\u53d6\u5e8f\u6570\u7ed3\u6784\uff0c\u63a8\u8fdb\u4f20\u7edf\u7b26\u53f7\u6216\u5206\u7c7b\u65b9\u6cd5\u4e0d\u8db3\u9886\u57df\u7684\u77e5\u8bc6\u8868\u793a\u80fd\u529b"}}
{"id": "2602.18094", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18094", "abs": "https://arxiv.org/abs/2602.18094", "authors": ["Ling Lin", "Yang Bai", "Heng Su", "Congcong Zhu", "Yaoxing Wang", "Yang Zhou", "Huazhu Fu", "Jingrun Chen"], "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models", "comment": "54 pages, 21 figures", "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.", "AI": {"tldr": "OODBench\uff1a\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u6837\u672c\uff0c\u63ed\u793a\u4e86\u73b0\u6709VLM\u5728OOD\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u81ea\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u6570\u636e\u5f80\u5f80\u4e0d\u6ee1\u8db3\u72ec\u7acb\u540c\u5206\u5e03\uff08IID\uff09\u5047\u8bbe\uff0c\u800c\u5206\u5e03\u5916\uff08OOD\uff09\u5bf9\u8c61\u5904\u7406\u4e0d\u5f53\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u8f85\u52a9\uff09\u3002\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30VLM\u5904\u7406OOD\u6570\u636e\u80fd\u529b\u7684\u6709\u6548\u57fa\u51c6\u3002", "method": "\u63d0\u51faOODBench\uff0c\u4e00\u79cd\u4e3b\u8981\u81ea\u52a8\u5316\u3001\u6700\u5c0f\u4eba\u5de5\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u65b0\u57fa\u51c6\u5e76\u8bc4\u4f30VLM\u5904\u7406OOD\u6570\u636e\u7684\u80fd\u529b\u3002\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u5b9e\u4f8b-\u7c7b\u522b\u5bf9\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\"\u57fa\u7840\u5230\u9ad8\u7ea7\u6e10\u8fdb\u5f0f\"\u63d0\u793a\u95ee\u9898\u7684\u53ef\u9760\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5f53\u524dVLM\u5728OODBench\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u4f7f\u5e95\u5c42\u56fe\u50cf\u7c7b\u522b\u5f88\u5e38\u89c1\u3002\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30OOD\u6570\u636e\u5bf9\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u7684\u5f71\u54cd\u3002", "conclusion": "OODBench\u4e3aVLM\u5728OOD\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u53d1\u73b0\u4e3a\u672a\u6765OOD\u6570\u636e\u83b7\u53d6\u548c\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.18083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18083", "abs": "https://arxiv.org/abs/2602.18083", "authors": ["Ioannis Kontogiorgakis", "Athanasios Askitopoulos", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Fotios Balampanis", "Charalampos Kontoes"], "title": "Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation", "comment": "This paper has been submitted to IEEE IGARSS 2026", "summary": "Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408Sentinel-1 SAR\u3001Sentinel-2\u5149\u5b66\u5f71\u50cf\u548cERA-5\u518d\u5206\u6790\u6570\u636e\u768410\u7c73\u5206\u8fa8\u7387\u571f\u58e4\u6e7f\u5ea6\u4f30\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6b27\u6d32\u690d\u88ab\u8986\u76d6\u533a\u57df\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u519c\u573a\u7ea7\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u536b\u661f\u571f\u58e4\u6e7f\u5ea6\u4ea7\u54c1\u5206\u8fa8\u7387\u592a\u4f4e\uff08>1\u516c\u91cc\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u519c\u573a\u7ea7\u5e94\u7528\u9700\u6c42\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u5206\u8fa8\u7387\uff0810\u7c73\uff09\u7684\u571f\u58e4\u6e7f\u5ea6\u4f30\u7b97\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u3001\u6c34\u8d44\u6e90\u7ba1\u7406\u548c\u6c14\u5019\u76d1\u6d4b\u3002", "method": "\u7ed3\u5408Sentinel-1 SAR\u3001Sentinel-2\u5149\u5b66\u5f71\u50cf\u548cERA-5\u518d\u5206\u6790\u6570\u636e\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u548c\u65f6\u95f4\u53c2\u6570\u5316\u7b56\u7565\uff0c\u91c7\u7528\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u786e\u4fdd\u5730\u7406\u6cdb\u5316\u80fd\u529b\u3002\u8bc4\u4f30IBM-NASA Prithvi\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u4e0e\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u7684\u6548\u679c\u3002", "result": "\u6df7\u5408\u65f6\u95f4\u5339\u914d\u7b56\u7565\uff08Sentinel-2\u5f53\u65e5\u91c7\u96c6\u4e0eSentinel-1\u4e0b\u964d\u8f68\u9053\uff09\u8fbe\u5230R\u00b2=0.514\uff0c\u52a0\u516510\u5929ERA5\u56de\u6eaf\u7a97\u53e3\u540e\u63d0\u5347\u81f3R\u00b2=0.518\u3002Prithvi\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u76f8\u6bd4\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u6539\u8fdb\u6709\u9650\uff08R\u00b2=0.515 vs. 0.514\uff09\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u5149\u8c31\u6307\u6570\u7ed3\u5408\u57fa\u4e8e\u6811\u7684\u96c6\u6210\u65b9\u6cd5\u4e3a\u6cdb\u6b27\u6d32\u7530\u95f4\u5c3a\u5ea6\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u5728\u7a00\u758f\u6570\u636e\u56de\u5f52\u4efb\u52a1\u4e2d\u4ecd\u5177\u6709\u5f88\u5f3a\u7ade\u4e89\u529b\u3002"}}
{"id": "2602.18420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18420", "abs": "https://arxiv.org/abs/2602.18420", "authors": ["Jiamin Yao", "Eren Gultepe"], "title": "SPQ: An Ensemble Technique for Large Language Model Compression", "comment": "Accepted to LREC 2026 Main Conference", "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/", "AI": {"tldr": "SPQ\u662f\u4e00\u79cd\u7ed3\u5408SVD\u3001\u526a\u679d\u548c\u91cf\u5316\u7684LLM\u538b\u7f29\u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u538b\u7f29\u6280\u672f\u6765\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faSPQ\u96c6\u6210\u538b\u7f29\u6280\u672f\uff1a1\uff09\u4f7f\u7528\u4fdd\u7559\u65b9\u5dee\u7684SVD\u5c06\u6ce8\u610f\u529b\u6295\u5f71\u538b\u7f29\u4e3a\u4f4e\u79e9\u56e0\u5b50\uff1b2\uff09\u57fa\u4e8e\u6fc0\u6d3b\u7684\u526a\u679d\u53bb\u9664MLP\u5c42\u4e2d\u7684\u5197\u4f59\u795e\u7ecf\u5143\uff1b3\uff09\u5bf9\u6240\u6709\u7ebf\u6027\u5c42\u5e94\u75288\u4f4d\u7ebf\u6027\u91cf\u5316\u3002", "result": "\u5728LLaMA-2-7B\u4e0a\u5b9e\u73b075%\u5185\u5b58\u51cf\u5c11\uff0c\u56f0\u60d1\u5ea6\u4ece5.47\u964d\u81f34.91\uff08WikiText-2\uff09\uff0c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u4fdd\u6301\uff0c\u5185\u5b58\u4f7f\u7528\uff086.86GB\uff09\u4f4e\u4e8eGPTQ\uff087.16GB\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4GPTQ\u5feb1.9\u500d\u3002", "conclusion": "SPQ\u901a\u8fc7\u4e92\u8865\u7684\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\uff0c\u5728\u5185\u5b58\u51cf\u5c11\u3001\u6027\u80fd\u4fdd\u6301\u548c\u63a8\u7406\u52a0\u901f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2602.18425", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18425", "abs": "https://arxiv.org/abs/2602.18425", "authors": ["Deniz Qian", "Hung-Ting Chen", "Eunsol Choi"], "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering", "comment": "18 pages, 12 figures, 12 tables", "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.", "AI": {"tldr": "RVR\u662f\u4e00\u4e2a\u591a\u8f6e\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22-\u9a8c\u8bc1-\u68c0\u7d22\u7684\u8fed\u4ee3\u8fc7\u7a0b\u6700\u5927\u5316\u7b54\u6848\u8986\u76d6\u7387\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u9700\u8981\u5e7f\u6cdb\u6709\u6548\u7b54\u6848\u7684\u67e5\u8be2\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8986\u76d6\u591a\u6837\u5316\u7684\u6587\u6863\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5927\u5316\u7b54\u6848\u8986\u76d6\u7387\u7684\u68c0\u7d22\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u68c0\u7d22-\u9a8c\u8bc1-\u68c0\u7d22\uff08RVR\uff09\u6846\u67b6\uff1a1\uff09\u68c0\u7d22\u5668\u83b7\u53d6\u5019\u9009\u6587\u6863\u96c6\uff1b2\uff09\u9a8c\u8bc1\u5668\u8bc6\u522b\u9ad8\u8d28\u91cf\u5b50\u96c6\uff1b3\uff09\u7528\u5df2\u9a8c\u8bc1\u6587\u6863\u589e\u5f3a\u67e5\u8be2\u8fdb\u884c\u4e0b\u4e00\u8f6e\u68c0\u7d22\uff0c\u91cd\u590d\u6b64\u8fc7\u7a0b\u4ee5\u53d1\u73b0\u672a\u8986\u76d6\u7684\u7b54\u6848\u3002", "result": "\u5728QAMPARI\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u81f3\u5c1110%\u76f8\u5bf9\u548c3%\u7edd\u5bf9\u589e\u76ca\u7684\u5b8c\u6574\u53ec\u56de\u7387\uff0c\u5728QUEST\u548cWebQuestionsSP\u6570\u636e\u96c6\u4e0a\u4e5f\u6709\u7a33\u5b9a\u63d0\u5347\uff0c\u4f18\u4e8e\u5305\u62ec\u667a\u80fd\u641c\u7d22\u65b9\u6cd5\u5728\u5185\u7684\u57fa\u7ebf\u3002", "conclusion": "RVR\u662f\u4e00\u79cd\u6709\u6548\u7684\u8fed\u4ee3\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u548c\u68c0\u7d22\u5668\u9002\u5e94\u65b0\u63a8\u7406\u573a\u666f\uff0c\u4e3a\u5168\u9762\u7b54\u6848\u53ec\u56de\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18093", "abs": "https://arxiv.org/abs/2602.18093", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Qianli Ma", "Zhi Yao", "Weijia Jia"], "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.", "AI": {"tldr": "PrediT\uff1a\u57fa\u4e8e\u7ebf\u6027\u591a\u6b65\u9884\u6d4b\u7684DiT\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u6a21\u578b\u8f93\u51fa\u800c\u975e\u7b80\u5355\u91cd\u7528\u7279\u5f81\uff0c\u5b9e\u73b05.54\u500d\u5ef6\u8fdf\u964d\u4f4e", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiT)\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6cd5\u57fa\u4e8e\u7279\u5f81\u7f13\u5b58\u548c\u91cd\u7528\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u6f02\u79fb\u548c\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d", "method": "\u63d0\u51faPrediT\u6846\u67b6\uff0c\u5c06\u7279\u5f81\u9884\u6d4b\u5efa\u6a21\u4e3a\u7ebf\u6027\u591a\u6b65\u95ee\u9898\uff0c\u4f7f\u7528\u7ecf\u5178\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u4ece\u5386\u53f2\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u6a21\u578b\u8f93\u51fa\uff1b\u5305\u542b\u5728\u9ad8\u52a8\u6001\u533a\u57df\u6fc0\u6d3b\u7684\u6821\u6b63\u5668\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff1b\u52a8\u6001\u6b65\u957f\u8c03\u5236\u673a\u5236\u901a\u8fc7\u76d1\u63a7\u7279\u5f81\u53d8\u5316\u7387\u81ea\u9002\u5e94\u8c03\u6574\u9884\u6d4b\u8303\u56f4", "result": "\u5728\u5404\u79cd\u57fa\u4e8eDiT\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8fbe5.54\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u8d28\u91cf\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "PrediT\u901a\u8fc7\u5c06\u7279\u5f81\u9884\u6d4b\u5f62\u5f0f\u5316\u4e3a\u7ebf\u6027\u591a\u6b65\u95ee\u9898\uff0c\u7ed3\u5408\u6821\u6b63\u5668\u548c\u81ea\u9002\u5e94\u6b65\u957f\u8c03\u5236\uff0c\u5b9e\u73b0\u4e86DiT\u6a21\u578b\u7684\u9ad8\u6548\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2602.18429", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18429", "abs": "https://arxiv.org/abs/2602.18429", "authors": ["Harshul Raj Surana", "Arijit Maji", "Aryan Vats", "Akash Ghosh", "Sriparna Saha", "Amit Sheth"], "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VIRAASAT\u6570\u636e\u96c6\u548cSCoM\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLMs\u5728\u5370\u5ea6\u6587\u5316\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u6587\u5316\u7279\u5b9a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u7b26\u53f7\u94fe\u5f0f\u64cd\u4f5c\u65b9\u6cd5\u6765\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u9700\u8981\u4e30\u5bcc\u793e\u4f1a\u6587\u5316\u77e5\u8bc6\u548c\u672c\u5730\u80cc\u666f\u7684\u4efb\u52a1\uff08\u7279\u522b\u662f\u5370\u5ea6\u6587\u5316\u76f8\u5173\u4efb\u52a1\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u7684\u6587\u5316\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u624b\u5de5\u5236\u4f5c\u3001\u4ec5\u5305\u542b\u6d4b\u8bd5\u4e8b\u5b9e\u8bb0\u5fc6\u7684\u5355\u8df3\u95ee\u9898\u3001\u6269\u5c55\u6210\u672c\u8fc7\u9ad8\uff0c\u5bfc\u81f4\u8fd9\u4e00\u7f3a\u9677\u672a\u88ab\u5145\u5206\u8861\u91cf\u3002", "method": "1. \u5f15\u5165VIRAASAT\uff1a\u534a\u81ea\u52a8\u591a\u8df3\u65b9\u6cd5\u751f\u6210\u5370\u5ea6\u6587\u5316\u7279\u5b9aQA\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5305\u542b700\u591a\u4e2a\u4e13\u5bb6\u7b56\u5212\u6587\u5316\u5b9e\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u6db5\u76d613\u4e2a\u5173\u952e\u6587\u5316\u5c5e\u6027\uff0c\u8986\u76d6\u5370\u5ea6\u6240\u670928\u4e2a\u90a6\u548c8\u4e2a\u8054\u90a6\u5c5e\u5730\uff1b2. \u63d0\u51faSCoM\uff08\u7b26\u53f7\u94fe\u5f0f\u64cd\u4f5c\uff09\u6846\u67b6\uff1a\u8bad\u7ec3\u6a21\u578b\u5185\u90e8\u6a21\u62df\u539f\u5b50\u77e5\u8bc6\u56fe\u8c31\u64cd\u4f5c\uff0c\u53ef\u9760\u904d\u5386\u56fe\u8c31\u62d3\u6251\u7ed3\u6784\u3002", "result": "VIRAASAT\u751f\u6210\u8d85\u8fc73,200\u4e2a\u591a\u8df3\u95ee\u9898\uff0c\u9700\u8981\u94fe\u5f0f\u6587\u5316\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524dSOTA LLMs\u5728VIRAASAT\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0cCoT\u5fae\u8c03\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4f4e\u6982\u7387\u4e8b\u5b9e\u3002SCoM\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u6bd4\u6807\u51c6CoT\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "VIRAASAT\u6570\u636e\u96c6\u548cSCoM\u6846\u67b6\u4e3a\u6784\u5efa\u6587\u5316\u611f\u77e5\u63a8\u7406\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u6587\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u64cd\u4f5c\u63d0\u5347\u4e86\u591a\u8df3\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.18178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18178", "abs": "https://arxiv.org/abs/2602.18178", "authors": ["Poonam Poonam", "Pere-Pau V\u00e1zquez", "Timo Ropinski"], "title": "Evaluating Graphical Perception Capabilities of Vision Transformers", "comment": null, "summary": "Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.", "AI": {"tldr": "ViTs\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u4e0eCNNs\u76f8\u6bd4\u4e5f\u5b58\u5728\u611f\u77e5\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5176\u5728\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528", "motivation": "\u867d\u7136ViTs\u5728\u591a\u79cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u800c\u56fe\u5f62\u611f\u77e5\u5bf9\u4e8e\u53ef\u89c6\u5316\u89e3\u91ca\u81f3\u5173\u91cd\u8981", "method": "\u91c7\u7528Cleveland\u548cMcGill\u5f00\u521b\u7684\u89c6\u89c9\u5224\u65ad\u4efb\u52a1\u6846\u67b6\uff0c\u5728\u53d7\u63a7\u7684\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u5bf9\u6bd4ViTs\u3001CNNs\u548c\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u8868\u73b0", "result": "ViTs\u867d\u7136\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u53ef\u89c6\u5316\u9886\u57df\u7684\u7c7b\u4eba\u56fe\u5f62\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u660e\u663e\u5dee\u8ddd", "conclusion": "ViTs\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u5bf9\u5176\u5728\u53ef\u89c6\u5316\u7cfb\u7edf\u548c\u56fe\u5f62\u611f\u77e5\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u91cd\u8981\u8003\u91cf"}}
{"id": "2602.18252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18252", "abs": "https://arxiv.org/abs/2602.18252", "authors": ["Rishika Bhagwatkar", "Irina Rish", "Nicolas Flammarion", "Francesco Croce"], "title": "On the Adversarial Robustness of Discrete Image Tokenizers", "comment": null, "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u7684\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u9ad8\u6548\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u4e0eCLIP\u7f16\u7801\u5668\u4e0d\u540c\uff0c\u8fd9\u4e9b\u5206\u8bcd\u5668\u7684\u5b89\u5168\u6027\u9700\u8981\u8bc4\u4f30\uff0c\u4ee5\u5f00\u53d1\u5b89\u5168\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002", "method": "1. \u63d0\u51fa\u9488\u5bf9\u79bb\u6563\u5206\u8bcd\u5668\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u7279\u5f81\u6765\u6539\u53d8\u63d0\u53d6\u7684token\uff1b2. \u53d7\u9c81\u68d2CLIP\u7f16\u7801\u5668\u542f\u53d1\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u5fae\u8c03\u6d41\u884c\u5206\u8bcd\u5668\uff0c\u4fdd\u6301\u5176\u4ed6\u7ec4\u4ef6\u4e0d\u53d8\u3002", "result": "\u653b\u51fb\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u3001\u5e94\u7528\u65e0\u5173\uff0c\u5728\u5206\u7c7b\u3001\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u4e2d\u6709\u6548\u3002\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5bf9\u65e0\u76d1\u7763\u548c\u7aef\u5230\u7aef\u76d1\u7763\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u548c\u6570\u636e\u3002", "conclusion": "\u5206\u8bcd\u5668\u9c81\u68d2\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u4e3a\u5f00\u53d1\u5b89\u5168\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u6b65\u9aa4\uff0c\u76f8\u6bd4\u76d1\u7763\u65b9\u6cd5\u66f4\u5177\u901a\u7528\u6027\u3002"}}
{"id": "2602.18193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18193", "abs": "https://arxiv.org/abs/2602.18193", "authors": ["Yiran Yang", "Zhaowei Liu", "Yuan Yuan", "Yukun Song", "Xiong Ma", "Yinghao Song", "Xiangji Zeng", "Lu Sun", "Yulu Wang", "Hai Zhou", "Shuai Cui", "Zhaohan Gong", "Jiefei Zhang"], "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards", "comment": "7 pages, 3 figures. To appear in AAAI 2026", "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.", "AI": {"tldr": "BLM-Guard\uff1a\u4e00\u4e2a\u7528\u4e8e\u77ed\u89c6\u9891\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u7684\u6846\u67b6\uff0c\u878d\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u89c4\u5219\u7b56\u7565\u539f\u5219\u548c\u6279\u8bc4\u8005\u5f15\u5bfc\u5956\u52b1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u64cd\u7eb5\u68c0\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u591a\u6a21\u6001\u5e7f\u544a\u5305\u542b\u6b3a\u9a97\u6027\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u5b57\u5e55\u5185\u5bb9\uff0c\u9700\u8981\u6bd4\u793e\u533a\u5b89\u5168\u8fc7\u6ee4\u5668\u66f4\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5ba1\u6838\u673a\u5236", "method": "1. \u89c4\u5219\u9a71\u52a8\u7684ICoT\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u3001\u63a8\u7406\u94fe\u548c\u6807\u7b7e\uff1b2. \u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u5e73\u8861\u56e0\u679c\u4e00\u81f4\u6027\u548c\u7b56\u7565\u9075\u5faa\u7684\u590d\u5408\u5956\u52b1\u4f18\u5316\u6a21\u578b\uff1b3. \u591a\u4efb\u52a1\u67b6\u6784\u5efa\u6a21\u6a21\u6001\u5185\u64cd\u7eb5\u548c\u8de8\u6a21\u6001\u4e0d\u5339\u914d", "result": "\u5728\u771f\u5b9e\u77ed\u89c6\u9891\u5e7f\u544a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBLM-Guard\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "BLM-Guard\u4e3a\u5546\u4e1a\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u7b56\u7565\u539f\u5219\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u591a\u6a21\u6001\u5e7f\u544a\u4e2d\u7684\u6b3a\u9a97\u6027\u5185\u5bb9"}}
{"id": "2602.18199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18199", "abs": "https://arxiv.org/abs/2602.18199", "authors": ["Gahyeon Shim", "Soogeun Park", "Hyemin Ahn"], "title": "A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion", "comment": null, "summary": "Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.", "AI": {"tldr": "DMC\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u672c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u4fee\u6b63\u6587\u672c\u751f\u6210\u52a8\u4f5c\u4e2d\u7684\u7269\u7406\u4e0d\u5408\u7406\u6027\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u751f\u6210\u52a8\u4f5c\u6280\u672f\u867d\u7136\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u7684\u52a8\u753b\u5f80\u5f80\u5b58\u5728\u7269\u7406\u4e0d\u5408\u7406\u7684\u95ee\u9898\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faDistortion-aware Motion Calibrator (DMC)\u540e\u5904\u7406\u6a21\u5757\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b66\u4e60\u4ece\u6545\u610f\u626d\u66f2\u7684\u52a8\u4f5c\u548c\u539f\u59cb\u6587\u672c\u63cf\u8ff0\u4e2d\u6062\u590d\u7269\u7406\u5408\u7406\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u4f9d\u8d56\u590d\u6742\u7684\u7269\u7406\u5efa\u6a21\u3002", "result": "DMC\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u52a8\u4f5c\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u7269\u7406\u5408\u7406\u6027\uff1a\u5728T2M\u4e0aFID\u5206\u6570\u964d\u4f4e42.74%\uff0c\u5728T2M-GPT\u4e0a\u964d\u4f4e13.20%\uff0c\u540c\u65f6\u83b7\u5f97\u6700\u9ad8\u7684R-Precision\u3002\u5e94\u7528\u4e8eMoMask\u65f6\uff0c\u7a7f\u900f\u7387\u51cf\u5c1133.0%\uff0c\u6f02\u6d6e\u4f2a\u5f71\u66f4\u63a5\u8fd1\u771f\u5b9e\u53c2\u8003\u3002", "conclusion": "DMC\u4f5c\u4e3a\u4e00\u4e2a\u6709\u524d\u666f\u7684\u540e\u5904\u7406\u52a8\u4f5c\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u5404\u79cd\u6587\u672c\u751f\u6210\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u6587\u672c\u8bed\u4e49\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u63d0\u5347\u751f\u6210\u52a8\u4f5c\u7684\u8d28\u91cf\u3002"}}
{"id": "2602.18282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18282", "abs": "https://arxiv.org/abs/2602.18282", "authors": ["Shiyan Du", "Conghan Yue", "Xinyu Cheng", "Dongyu Zhang"], "title": "DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control", "comment": "Accepted by AAAI 2026", "summary": "Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.", "AI": {"tldr": "DEIG\u662f\u4e00\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u53ef\u63a7\u591a\u5b9e\u4f8b\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668\u548c\u7ec6\u8282\u878d\u5408\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6587\u672c\u63cf\u8ff0\u4e0b\u7684\u8bed\u4e49\u7406\u89e3\u95ee\u9898\uff0c\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u5b9e\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u5c5e\u6027\u7ed1\u5b9a\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u6587\u672c\u63cf\u8ff0\u65f6\u4ecd\u9762\u4e34\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9632\u6b62\u5b9e\u4f8b\u95f4\u5c5e\u6027\u6cc4\u6f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "DEIG\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668(IDE)\uff0c\u5c06\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u5b9e\u4f8b\u611f\u77e5\u8868\u793a\uff1b2) \u7ec6\u8282\u878d\u5408\u6a21\u5757(DFM)\uff0c\u5e94\u7528\u57fa\u4e8e\u5b9e\u4f8b\u7684\u63a9\u7801\u6ce8\u610f\u529b\u9632\u6b62\u5b9e\u4f8b\u95f4\u5c5e\u6027\u6cc4\u6f0f\u3002\u8fd8\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548cDEIG-Bench\u57fa\u51c6\u3002", "result": "DEIG\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u540c\u65f6\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u6807\u51c6\u57fa\u4e8e\u6269\u6563\u7684\u6d41\u7a0b\u4e2d\u3002", "conclusion": "DEIG\u901a\u8fc7\u521b\u65b0\u7684\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u548c\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5b9e\u4f8b\u751f\u6210\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u4e30\u5bcc\u5c40\u90e8\u5316\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u5339\u914d\u7684\u89c6\u89c9\u8fde\u8d2f\u591a\u5b9e\u4f8b\u573a\u666f\u3002"}}
{"id": "2602.18309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18309", "abs": "https://arxiv.org/abs/2602.18309", "authors": ["Ziyue Liu", "Davide Talon", "Federico Girella", "Zanxi Ruan", "Mattia Mondo", "Loris Bazzani", "Yiming Wang", "Marco Cristani"], "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation", "comment": "Project page: https://intelligolabs.github.io/lots/", "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.", "AI": {"tldr": "LOTS\u662f\u4e00\u4e2a\u7ed3\u5408\u5168\u5c40\u8349\u56fe\u5f15\u5bfc\u4e0e\u591a\u4e2a\u5c40\u90e8\u8349\u56fe-\u6587\u672c\u5bf9\u7684\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u6761\u4ef6\u7f16\u7801\u548c\u6269\u6563\u5bf9\u5f15\u5bfc\u5b9e\u73b0\u8349\u56fe\u7ed3\u6784\u4fdd\u6301\u4e0e\u6587\u672c\u8bed\u4e49\u589e\u5f3a\u3002", "motivation": "\u8349\u56fe\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u65e9\u671f\u65f6\u5c1a\u6784\u601d\u7684\u7b80\u6d01\u8868\u8fbe\u5a92\u4ecb\uff0c\u6587\u672c\u63cf\u8ff0\u5219\u8865\u5145\u6750\u8d28\u3001\u989c\u8272\u548c\u98ce\u683c\u7ec6\u8282\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u8349\u56fe\u89c6\u89c9\u7ed3\u6784\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u6587\u672c\u7684\u5c40\u90e8\u5c5e\u6027\u6307\u5bfc\u3002", "method": "\u63d0\u51faLOTS\u6846\u67b6\uff1a1) \u591a\u7ea7\u6761\u4ef6\u7f16\u7801\u9636\u6bb5\u72ec\u7acb\u7f16\u7801\u5c40\u90e8\u7279\u5f81\u4e8e\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u534f\u8c03\uff1b2) \u6269\u6563\u5bf9\u5f15\u5bfc\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5728\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6761\u4ef6\u3002\u540c\u65f6\u521b\u5efaSketchy\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u4e1a\u8349\u56fe\u548c\u975e\u4e13\u4e1a\u8349\u56fe\u4e24\u79cd\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u5c40\u90e8\u8bed\u4e49\u6307\u5bfc\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002Sketchy\u6570\u636e\u96c6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e13\u4e1a\u8349\u56fe\u548c\u975e\u4e13\u4e1a\u8349\u56fe\uff0c\u652f\u6301\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "conclusion": "LOTS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u8349\u56fe\u5f15\u5bfc\u4e0e\u591a\u4e2a\u5c40\u90e8\u8349\u56fe-\u6587\u672c\u5bf9\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\u7684\u7ed3\u6784\u4fdd\u6301\u548c\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18314", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18314", "abs": "https://arxiv.org/abs/2602.18314", "authors": ["Tianyi Song", "Danail Stoyanov", "Evangelos Mazomenos", "Francisco Vasconcelos"], "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.", "AI": {"tldr": "Diff2DGS\uff1a\u7528\u4e8e\u624b\u672f\u573a\u666f\u5b9e\u65f6\u91cd\u5efa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4fee\u590d\u906e\u6321\u533a\u57df\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b\u76842D\u9ad8\u65af\u6e85\u5c04\u6355\u6349\u7ec4\u7ec7\u53d8\u5f62\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6df1\u5ea6\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u906e\u6321\u533a\u57df\u8d28\u91cf\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u7cbe\u5ea6\u8bc4\u4f30\u3002EndoNeRF\u548cStereoMIS\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u7f3a\u5c113D\u771f\u503c\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u6a21\u5757\uff0c\u5229\u7528\u65f6\u95f4\u5148\u9a8c\u4fee\u590d\u88ab\u624b\u672f\u5668\u68b0\u906e\u6321\u7684\u7ec4\u7ec7\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5e26\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b\uff08LDM\uff09\u76842D\u9ad8\u65af\u6e85\u5c04\uff082DGS\uff09\u6355\u6349\u52a8\u6001\u7ec4\u7ec7\u53d8\u5f62\u548c\u89e3\u5256\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728EndoNeRF\u4e0a\u8fbe\u523038.02 dB PSNR\uff0c\u5728StereoMIS\u4e0a\u8fbe\u523034.40 dB PSNR\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\u4ec5\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u4e0d\u4e00\u5b9a\u80fd\u83b7\u5f97\u6700\u4f733D\u91cd\u5efa\u7cbe\u5ea6\uff0c\u56e0\u6b64\u8fdb\u4e00\u6b65\u4f18\u5316\u6df1\u5ea6\u8d28\u91cf\u4ee5\u786e\u4fdd\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "Diff2DGS\u5728\u624b\u672f\u573a\u666f\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u5916\u89c2\u548c\u51e0\u4f55\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5728SCARED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6df1\u5ea6\u7cbe\u5ea6\u5b9a\u91cf\u5206\u6790\uff0c\u4e3a\u624b\u672f\u5bfc\u822a\u548c\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u91cd\u5efa\u65b9\u6848\u3002"}}
{"id": "2602.18322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18322", "abs": "https://arxiv.org/abs/2602.18322", "authors": ["Ziteng Cui", "Shuhong Liu", "Xiaoyu Dong", "Xuangeng Chu", "Lin Gu", "Ming-Hsuan Yang", "Tatsuya Harada"], "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis", "comment": "Journal extension version of CVPR 2025 paper: arXiv:2504.01503", "summary": "High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.", "AI": {"tldr": "\u63d0\u51faLuminance-GS++\uff0c\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u5168\u5c40\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\u6765\u89e3\u51b3\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u91c7\u96c6\u9762\u4e34\u590d\u6742\u5149\u7167\u53d8\u5316\u548c\u76f8\u673a\u6210\u50cf\u7ba1\u9053\u56fa\u6709\u5c40\u9650\u6027\u7684\u6311\u6218\u3002\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u7167\u5dee\u5f02\u3001\u4f20\u611f\u5668\u54cd\u5e94\u548cISP\u914d\u7f6e\u5dee\u5f02\u5bfc\u81f4\u5149\u5ea6\u548c\u8272\u5f69\u4e0d\u4e00\u81f4\uff0c\u8fdd\u53cd\u4e86\u73b0\u4ee33D\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u6240\u4f9d\u8d56\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u89c6\u89d2\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\u8fdb\u884c\u7cbe\u786e\u8272\u5f69\u6821\u6b63\u3002\u8bbe\u8ba1\u65e0\u76d1\u7763\u76ee\u6807\uff0c\u8054\u5408\u5f3a\u5236\u6267\u884c\u4eae\u5ea6\u6821\u6b63\u4ee5\u53ca\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4f4e\u5149\u7167\u3001\u8fc7\u66dd\u548c\u590d\u6742\u4eae\u5ea6/\u8272\u5f69\u53d8\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4fdd\u6301\u663e\u5f0f3DGS\u8868\u793a\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "Luminance-GS++\u901a\u8fc7\u89e3\u51b3\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728\u4fdd\u63013DGS\u5b9e\u65f6\u6e32\u67d3\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.18329", "categories": ["cs.CV", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.18329", "abs": "https://arxiv.org/abs/2602.18329", "authors": ["Qingsong Wang", "Jiaxing He", "Bingzhe Hou", "Tieru Wu", "Yang Cao", "Cailing Yao"], "title": "G-LoG Bi-filtration for Medical Image Classification", "comment": null, "summary": "Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.", "AI": {"tldr": "\u63d0\u51faG-LoG\u53cc\u6ee4\u8fc7\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\uff0c\u751f\u6210\u9002\u5408\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u7684\u7279\u5f81\uff0c\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u53c2\u6570\u6ee4\u8fc7\uff0c\u4e14MLP\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5728\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\uff0c\u6784\u5efa\u5b9e\u7528\u7684\u6ee4\u8fc7\u7ed3\u6784\u4ee5\u68c0\u6d4b\u62d3\u6251\u548c\u51e0\u4f55\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u5b9a\u4e49G-LoG\u53cc\u6ee4\u8fc7\u65b9\u6cd5\u3002\u5c06\u4f53\u79ef\u56fe\u50cf\u5efa\u6a21\u4e3a\u6709\u754c\u51fd\u6570\uff0c\u8bc1\u660e\u4ece\u8be5\u53cc\u6ee4\u8fc7\u83b7\u5f97\u7684\u6301\u4e45\u6027\u6a21\u5757\u7684\u4ea4\u9519\u8ddd\u79bb\u76f8\u5bf9\u4e8e\u6709\u754c\u51fd\u6570\u7684\u6700\u5927\u8303\u6570\u662f\u7a33\u5b9a\u7684\u3002", "result": "\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cG-LoG\u53cc\u6ee4\u8fc7\u663e\u8457\u4f18\u4e8e\u5355\u53c2\u6570\u6ee4\u8fc7\u3002\u4f7f\u7528\u8be5\u53cc\u6ee4\u8fc7\u751f\u6210\u7684\u62d3\u6251\u7279\u5f81\u8bad\u7ec3\u7684\u7b80\u5355MLP\u6a21\u578b\uff0c\u6027\u80fd\u53ef\u4e0e\u5728\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Google AutoML Vision\u3001ResNet\u3001AutoKeras\u3001auto-sklearn\uff09\u76f8\u5ab2\u7f8e\u3002", "conclusion": "G-LoG\u53cc\u6ee4\u8fc7\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u53c2\u6570\u6301\u4e45\u6027\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2602.18394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18394", "abs": "https://arxiv.org/abs/2602.18394", "authors": ["Stefan Becker", "Simon Weiss", "Wolfgang H\u00fcbner", "Michael Arens"], "title": "Self-Aware Object Detection via Degradation Manifolds", "comment": null, "summary": "Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.\n  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.\n  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.\n  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9000\u5316\u6d41\u5f62\u7684\u9000\u5316\u611f\u77e5\u81ea\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7ed3\u6784\u5316\u56fe\u50cf\u9000\u5316\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u9000\u5316\u6807\u7b7e\u7684\u68c0\u6d4b\u5668\u81ea\u611f\u77e5\u80fd\u529b", "motivation": "\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u6807\u51c6\u6210\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6a21\u7cca\u3001\u566a\u58f0\u3001\u538b\u7f29\u3001\u6076\u52a3\u5929\u6c14\u6216\u5206\u8fa8\u7387\u53d8\u5316\u7b49\u9000\u5316\u6761\u4ef6\u4e0b\u53ef\u80fd\u65e0\u58f0\u5931\u8d25\u3002\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u4ec5\u4ea7\u751f\u9884\u6d4b\u800c\u4e0d\u8bc4\u4f30\u8f93\u5165\u662f\u5426\u5728\u68c0\u6d4b\u5668\u6807\u79f0\u5de5\u4f5c\u8303\u56f4\u5185\u662f\u4e0d\u591f\u7684\uff0c\u9700\u8981\u68c0\u6d4b\u5668\u5177\u5907\u81ea\u611f\u77e5\u80fd\u529b", "method": "\u57fa\u4e8e\u9000\u5316\u6d41\u5f62\u7684\u9000\u5316\u611f\u77e5\u81ea\u611f\u77e5\u6846\u67b6\uff0c\u5728\u6807\u51c6\u68c0\u6d4b\u9aa8\u5e72\u7f51\u7edc\u4e0a\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u5d4c\u5165\u5934\uff0c\u901a\u8fc7\u591a\u5c42\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u3002\u76f8\u540c\u9000\u5316\u7ec4\u6210\u7684\u56fe\u50cf\u88ab\u62c9\u8fd1\uff0c\u4e0d\u540c\u9000\u5316\u914d\u7f6e\u7684\u56fe\u50cf\u88ab\u63a8\u8fdc\uff0c\u5f62\u6210\u51e0\u4f55\u7ec4\u7ec7\u7684\u8868\u793a\u7a7a\u95f4\u3002\u901a\u8fc7\u5e72\u51c0\u8bad\u7ec3\u5d4c\u5165\u4f30\u8ba1\u539f\u59cb\u539f\u578b\u4f5c\u4e3a\u6807\u79f0\u5de5\u4f5c\u70b9\uff0c\u81ea\u611f\u77e5\u8868\u73b0\u4e3a\u4e0e\u53c2\u8003\u70b9\u7684\u51e0\u4f55\u504f\u5dee", "result": "\u5728\u5408\u6210\u635f\u574f\u57fa\u51c6\u6d4b\u8bd5\u3001\u8de8\u6570\u636e\u96c6\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u81ea\u7136\u5929\u6c14\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u5b9e\u9a8c\u4e2d\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u539f\u59cb-\u9000\u5316\u53ef\u5206\u79bb\u6027\uff0c\u5728\u591a\u79cd\u68c0\u6d4b\u5668\u67b6\u6784\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u5728\u8bed\u4e49\u504f\u79fb\u4e0b\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b", "conclusion": "\u9000\u5316\u611f\u77e5\u8868\u793a\u51e0\u4f55\u4e3a\u68c0\u6d4b\u5668\u81ea\u611f\u77e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u4e0e\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u57fa\u7840\uff0c\u80fd\u591f\u72ec\u7acb\u4e8e\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u63d0\u4f9b\u56fe\u50cf\u7ea7\u9000\u5316\u504f\u79fb\u4fe1\u53f7"}}
{"id": "2602.18406", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18406", "abs": "https://arxiv.org/abs/2602.18406", "authors": ["Minh Dinh", "St\u00e9phane Deny"], "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges", "comment": null, "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u7b49\u53d8\u7b97\u5b50\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u8bad\u7ec3\u4e2d\u7f55\u89c1\u7684\u7fa4\u5bf9\u79f0\u53d8\u6362\uff08\u5982\u65cb\u8f6c\u3001\u5e73\u79fb\uff09\uff0c\u5728\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u566a\u58f0MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u51fa\u5206\u5e03\u7684\u5206\u7c7b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7f51\u7edc\u548c\u7b49\u53d8\u7f51\u7edc\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u8bc6\u522b\u8bad\u7ec3\u4e2d\u7f55\u89c1\u7684\u7fa4\u5bf9\u79f0\u53d8\u6362\uff08\u5982\u4e0d\u5e38\u89c1\u59ff\u6001\u3001\u5c3a\u5ea6\u3001\u4f4d\u7f6e\u6216\u5176\u7ec4\u5408\uff09\u7684\u5bf9\u8c61\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u80fd\u89e3\u51b3\u5bf9\u79f0\u53d8\u6362\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4f46\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u4ece\u5bf9\u79f0\u53d8\u6362\u793a\u4f8b\u4e2d\u5b66\u4e60\u7b49\u53d8\u7b97\u5b50\u7684\u66ff\u4ee3\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u67b6\u6784\uff0c\u4ece\u5bf9\u79f0\u53d8\u6362\u7684\u793a\u4f8b\u4e2d\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u7684\u7b49\u53d8\u7b97\u5b50\u3002\u4f7f\u7528\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u566a\u58f0MNIST\u7b80\u5355\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8fd9\u79cd\u67b6\u6784\u5b9e\u73b0\u8d85\u51fa\u5206\u5e03\u7684\u5206\u7c7b\u3002", "result": "\u5728\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u566a\u58f0MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u67b6\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u8d85\u51fa\u5206\u5e03\u7684\u5206\u7c7b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7f51\u7edc\u548c\u7b49\u53d8\u7f51\u7edc\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5904\u7406\u8bad\u7ec3\u4e2d\u7f55\u89c1\u7684\u5bf9\u79f0\u53d8\u6362\u3002", "conclusion": "\u867d\u7136\u6982\u5ff5\u4e0a\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5c06\u8fd9\u79cd\u67b6\u6784\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u5bf9\u79f0\u53d8\u6362\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2602.18422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18422", "abs": "https://arxiv.org/abs/2602.18422", "authors": ["Linxi Xie", "Lisong C. Sun", "Ashley Neall", "Tong Wu", "Shengqu Cai", "Gordon Wetzstein"], "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "comment": "Project page here: https://codeysun.github.io/generated-reality", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u4eba\u4f53\u4e2d\u5fc3\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5934\u90e8\u59ff\u6001\u548c\u624b\u90e8\u5173\u8282\u59ff\u6001\u63a7\u5236\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u865a\u62df\u73af\u5883\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u4e16\u754c\u6a21\u578b\u53ea\u80fd\u63a5\u53d7\u6587\u672c\u6216\u952e\u76d8\u7b49\u7c97\u7565\u63a7\u5236\u4fe1\u53f7\uff0c\u65e0\u6cd5\u54cd\u5e94\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7528\u6237\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u9650\u5236\u4e86\u5728\u6269\u5c55\u73b0\u5b9e(XR)\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "1) \u8bc4\u4f30\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\u6761\u4ef6\u7b56\u7565\uff0c\u63d0\u51fa\u6709\u6548\u76843D\u5934\u90e8\u548c\u624b\u90e8\u63a7\u5236\u673a\u5236\uff1b2) \u8bad\u7ec3\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff1b3) \u5c06\u5176\u84b8\u998f\u4e3a\u56e0\u679c\u4ea4\u4e92\u7cfb\u7edf\uff0c\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u865a\u62df\u73af\u5883\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u53d7\u8bd5\u8005\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u8868\u73b0\uff0c\u7528\u6237\u611f\u77e5\u5230\u7684\u52a8\u4f5c\u63a7\u5236\u7a0b\u5ea6\u4e5f\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4eba\u4f53\u59ff\u6001\u63a7\u5236\u7684\u89c6\u9891\u4e16\u754c\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\uff0c\u4e3a\u6269\u5c55\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u751f\u6210\u5f0f\u73af\u5883\u3002"}}
{"id": "2602.18424", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18424", "abs": "https://arxiv.org/abs/2602.18424", "authors": ["Xia Su", "Ruiqi Chen", "Benlin Liu", "Jingwei Ma", "Zonglin Di", "Ranjay Krishna", "Jon Froehlich"], "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav", "AI": {"tldr": "CapNav\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8003\u8651\u667a\u80fd\u4f53\u7269\u7406\u80fd\u529b\u7ea6\u675f\u4e0b\u8fdb\u884c\u5ba4\u5185\u5bfc\u822a\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b5\u79cd\u4ee3\u8868\u6027\u667a\u80fd\u4f53\u300145\u4e2a\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u3001473\u4e2a\u5bfc\u822a\u4efb\u52a1\u548c2365\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5f53\u524dVLM\u5728\u80fd\u529b\u7ea6\u675f\u4e25\u683c\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5bfc\u822a\u672c\u8d28\u4e0a\u53d7\u667a\u80fd\u4f53\u79fb\u52a8\u80fd\u529b\u7ea6\u675f\uff08\u5982\u626b\u5730\u673a\u5668\u4eba\u4e0d\u80fd\u722c\u697c\u68af\uff0c\u56db\u8db3\u673a\u5668\u4eba\u53ef\u4ee5\uff09\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u8fd9\u4e9b\u7269\u7406\u9650\u5236\u3002\u9700\u8981\u8bc4\u4f30VLM\u5728\u8003\u8651\u5177\u4f53\u667a\u80fd\u4f53\u80fd\u529b\u6761\u4ef6\u4e0b\u7684\u5bfc\u822a\u8868\u73b0\u3002", "method": "\u521b\u5efaCapNav\u57fa\u51c6\uff0c\u5b9a\u4e495\u79cd\u4ee3\u8868\u6027\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u667a\u80fd\u4f53\uff0c\u63cf\u8ff0\u5176\u7269\u7406\u5c3a\u5bf8\u3001\u79fb\u52a8\u80fd\u529b\u548c\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002\u63d0\u4f9b45\u4e2a\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u3001473\u4e2a\u5bfc\u822a\u4efb\u52a1\u548c2365\u4e2aQA\u5bf9\uff0c\u6d4b\u8bd5VLM\u80fd\u5426\u57fa\u4e8e\u667a\u80fd\u4f53\u80fd\u529b\u7a7f\u8d8a\u5ba4\u5185\u73af\u5883\u3002", "result": "\u8bc4\u4f3013\u4e2a\u73b0\u4ee3VLM\u53d1\u73b0\uff1a1\uff09\u5f53\u524dVLM\u5bfc\u822a\u6027\u80fd\u968f\u79fb\u52a8\u7ea6\u675f\u6536\u7d27\u800c\u6025\u5267\u4e0b\u964d\uff1b2\uff09\u5373\u4f7f\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u96be\u4ee5\u5904\u7406\u9700\u8981\u7a7a\u95f4\u7ef4\u5ea6\u63a8\u7406\u7684\u969c\u788d\u7c7b\u578b\uff1b3\uff09\u6a21\u578b\u5728\u80fd\u529b\u611f\u77e5\u5bfc\u822a\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u80fd\u529b\u611f\u77e5\u5bfc\u822a\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765VLM\u5728\u5177\u8eab\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002CapNav\u57fa\u51c6\u6709\u52a9\u4e8e\u63a8\u52a8VLM\u5728\u73b0\u5b9e\u4e16\u754c\u5bfc\u822a\u4e2d\u66f4\u597d\u5730\u8003\u8651\u667a\u80fd\u4f53\u7269\u7406\u7ea6\u675f\u3002"}}
{"id": "2602.18432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18432", "abs": "https://arxiv.org/abs/2602.18432", "authors": ["Evonne Ng", "Siwei Zhang", "Zhang Chen", "Michael Zollhoefer", "Alexander Richard"], "title": "SARAH: Spatially Aware Real-time Agentic Humans", "comment": "Project page: https://evonneng.github.io/sarah/", "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5b9e\u65f6\u3001\u5b8c\u5168\u56e0\u679c\u7684\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u5728VR\u5934\u663e\u4e0a\u90e8\u7f72\uff0c\u7ed3\u5408\u7528\u6237\u4f4d\u7f6e\u548c\u97f3\u9891\u751f\u6210\u5168\u8eab\u52a8\u4f5c\uff0c\u540c\u65f6\u6839\u636e\u7528\u6237\u65b9\u5411\u8c03\u6574\u4ee3\u7406\u671d\u5411\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u610f\u8bc6\uff0c\u65e0\u6cd5\u8ba9\u4ee3\u7406\u8f6c\u5411\u7528\u6237\u3001\u54cd\u5e94\u7528\u6237\u52a8\u4f5c\u5e76\u4fdd\u6301\u81ea\u7136\u6ce8\u89c6\u3002\u968f\u7740\u5177\u8eab\u4ee3\u7406\u5728VR\u3001\u8fdc\u7a0b\u5448\u73b0\u548c\u6570\u5b57\u4eba\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u9700\u8981\u8d85\u8d8a\u8bed\u97f3\u5bf9\u9f50\u624b\u52bf\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u56e0\u679cTransformer-based VAE\u4e0e\u4ea4\u9519\u6f5c\u5728token\u8fdb\u884c\u6d41\u5f0f\u63a8\u7406\uff0c\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u4ee5\u7528\u6237\u8f68\u8ff9\u548c\u97f3\u9891\u4e3a\u6761\u4ef6\u3002\u5f15\u5165\u6ce8\u89c6\u8bc4\u5206\u673a\u5236\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u5c06\u5b66\u4e60\u4e0e\u63a7\u5236\u89e3\u8026\u3002", "result": "\u5728Embody 3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u8d28\u91cf\uff0c\u8d85\u8fc7300 FPS\uff08\u6bd4\u975e\u56e0\u679c\u57fa\u7ebf\u5feb3\u500d\uff09\uff0c\u540c\u65f6\u6355\u6349\u81ea\u7136\u5bf9\u8bdd\u7684\u5fae\u5999\u7a7a\u95f4\u52a8\u6001\u3002\u5728\u5b9e\u65f6VR\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7b2c\u4e00\u4e2a\u5b9e\u65f6\u3001\u5b8c\u5168\u56e0\u679c\u7684\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u52a8\u4f5c\u751f\u6210\uff0c\u4f7f\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u4ee3\u7406\u80fd\u591f\u5b9e\u65f6\u90e8\u7f72\uff0c\u4e3aVR\u3001\u8fdc\u7a0b\u5448\u73b0\u548c\u6570\u5b57\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2602.18434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18434", "abs": "https://arxiv.org/abs/2602.18434", "authors": ["Vatsal Agarwal", "Saksham Suri", "Matthew Gwilliam", "Pulkit Kumar", "Abhinav Shrivastava"], "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory", "comment": "Project page: see https://vatsalag99.github.io/memstream/", "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "AI": {"tldr": "MemStream\u901a\u8fc7\u589e\u52a0token\u9884\u7b97\u3001\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u548c\u65e0\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u6027\u80fd", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u4f7f\u7528\u6709\u9650\u7684\u6bcf\u5e27token\u6570\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\uff0c\u4e14\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\u65f6\u5b58\u5728\u67e5\u8be2-\u5e27\u76f8\u4f3c\u5ea6\u968f\u65f6\u95f4\u589e\u52a0\u7684\u95ee\u9898\uff0c\u504f\u5411\u68c0\u7d22\u540e\u671f\u5e27", "method": "1) \u589e\u52a0token\u9884\u7b97\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u7406\u89e3\uff1b2) \u5f15\u5165\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u51cf\u5c11token\u5197\u4f59\u540c\u65f6\u4fdd\u7559\u5c40\u90e8\u65f6\u7a7a\u4fe1\u606f\uff1b3) \u63d0\u51fa\u65e0\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u5229\u7528\u5916\u90e8\u6a21\u578b\u66f4\u597d\u5730\u8bc6\u522b\u76f8\u5173\u5e27", "result": "\u5728CG-Bench\u4e0a\u63d0\u5347+8.0%\uff0cLVBench\u4e0a\u63d0\u5347+8.5%\uff0cVideoMME(Long)\u4e0a\u63d0\u5347+2.4%\uff08\u76f8\u6bd4ReKV with Qwen2.5-VL-7B\uff09", "conclusion": "MemStream\u901a\u8fc7\u6269\u5c55token\u9884\u7b97\u3001\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u548c\u65e0\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\u65f6\u7684\u5c40\u9650\u6027"}}
